{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning an sBERT model for semantic search\n",
    "\n",
    "**Henry Zelenak | Last updated: 05/12/2025**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 77374,
     "status": "ok",
     "timestamp": 1746995005209,
     "user": {
      "displayName": "Henry Zelenak",
      "userId": "01809909909045225068"
     },
     "user_tz": 240
    },
    "id": "UquVAzdtZOYH",
    "outputId": "d70436a9-004f-458d-ad91-79566623fd77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-3.6.0 dill-0.3.8 fsspec-2025.3.0 multiprocess-0.70.16 xxhash-3.5.0\n",
      "Mounted at /content/drive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install datasets\n",
    "import pandas as pd\n",
    "import ast  # To safely evaluate the string representation of lists\n",
    "from sentence_transformers import util\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses, LoggingHandler, models, SimilarityFunction\n",
    "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
    "from torch.utils.data import DataLoader\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import nltk\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import argparse\n",
    "import json\n",
    "from datasets import Dataset # <--- ADDED IMPORT\n",
    "\n",
    "\n",
    "from google.colab import drive, userdata\n",
    "drive.mount('/content/drive')\n",
    "# Adjust path as needed\n",
    "BASE_DIR = '/content/drive/My Drive/SUNY_Poly_DSA598/'\n",
    "\n",
    "\n",
    "# --- Setup Logging ---\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Download NLTK sentence tokenizer if needed ---\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "hf_key = userdata.get('hf_key')\n",
    "os.environ['HUGGINGFACE_TOKEN'] = hf_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "executionInfo": {
     "elapsed": 600,
     "status": "ok",
     "timestamp": 1746996729962,
     "user": {
      "displayName": "Henry Zelenak",
      "userId": "01809909909045225068"
     },
     "user_tz": 240
    },
    "id": "KFYZPiGXZOYJ",
    "outputId": "c19e79a3-4729-4939-e409-dc19b034d7ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of each label in the dataset:\n",
      "label\n",
      "REFUTES     516\n",
      "SUPPORTS    508\n",
      "Name: count, dtype: int64\n",
      "Count of each label in the dataset:\n",
      "label\n",
      "REFUTES     20\n",
      "SUPPORTS    16\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\nCount of each label in the dataset:\\nlabel\\nREFUTES     516\\nSUPPORTS    508\\nName: count, dtype: int64\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_time_obj = datetime.datetime.now()\n",
    "date_str = date_time_obj.strftime(\"%m-%d_%H%M\")\n",
    "\n",
    "# --- Configuration ---\n",
    "# SentenceTransformer\n",
    "MODEL_NAME = 'sentence-transformers/all-mpnet-base-v2'\n",
    "\n",
    "# Training Parameters\n",
    "NUM_EPOCHS = 2         # Number of training epochs (1-4 is often sufficient)\n",
    "TRAIN_BATCH_SIZE = 32  # Adjust based on GPU memory (larger often better for MNRL)\n",
    "LEARNING_RATE = 2e-5   # Standard learning rate for fine-tuning transformers\n",
    "WARMUP_STEPS = 100     # Number of warmup steps for the learning rate scheduler\n",
    "EVAL_STEPS = 64       # Evaluate performance every N steps (if dev set provided)\n",
    "MAX_SAMPLES = 1024      # Limit the number of training samples (for testing)\n",
    "MAX_VALID_SAMPLES = 36\n",
    "\n",
    "TRAIN_CSV_PATH = os.path.join(BASE_DIR, 'datasets/FEVER/tabular_sets/tabular_sentEx_paper_dev_train/v1_segmented_sentIDs_n3461_04-04_002.csv')\n",
    "DEV_CSV_PATH = os.path.join(BASE_DIR, 'datasets/FEVER/tabular_sets/tabular_sentEx_paper_dev_valid/v1_segmented_sentIDs_n1482_04-04_002.csv' ) # Use same for dev set for testing\n",
    "OUTPUT_PATH = os.path.join(BASE_DIR, f'models/sBERT/{MODEL_NAME.replace(\"/\", \"-\")}_n{MAX_SAMPLES}_{date_str}')\n",
    "\n",
    "verbose = True\n",
    "\n",
    "# --- Load and Prepare Data ---\n",
    "\n",
    "def load_and_prepare_data(csv_path, is_eval_set=False, max_samples=None, verbose=False):\n",
    "    \"\"\"Loads data, filters, parses evidence, and creates InputExamples or evaluation queries/corpus.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        logger.info(f\"Loaded {len(df)} rows from {csv_path}\")\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"Error: File not found at {csv_path}\")\n",
    "        return None if not is_eval_set else ({}, {}, {})\n",
    "\n",
    "    # Filter for relevant labels\n",
    "    df_filtered = df[df['label'].isin(['SUPPORTS', 'REFUTES'])].copy()\n",
    "    logger.info(f\"Filtered to {len(df_filtered)} SUPPORTS/REFUTES claims.\")\n",
    "\n",
    "    # Limit samples if specified\n",
    "    if max_samples and len(df_filtered) > max_samples:\n",
    "        df_filtered = df_filtered.sample(n=max_samples, random_state=42)\n",
    "        logger.info(f\"Sampled down to {len(df_filtered)} examples.\")\n",
    "\n",
    "    examples = []\n",
    "    queries = {}  # query_id -> query_text (for evaluation)\n",
    "    corpus = {}   # doc_id -> doc_text (for evaluation)\n",
    "    relevant_docs = {} # query_id -> set(doc_ids) (for evaluation)\n",
    "    doc_id_counter = 0\n",
    "\n",
    "    print(f\"Count of each label in the dataset:\")\n",
    "    print(df_filtered['label'].value_counts())\n",
    "        # Process each row\n",
    "    for index, row in df_filtered.iterrows():\n",
    "        claim = str(row['claim']).strip()\n",
    "        label = row['label']\n",
    "        claim_id = f\"claim_{row.get('id', index)}\" # Use provided ID or index\n",
    "\n",
    "        try:\n",
    "            # Safely parse the 'evidence_items' string\n",
    "            evidence_list = ast.literal_eval(str(row['evidence_items']))\n",
    "            if not isinstance(evidence_list, list):\n",
    "                raise ValueError(\"Parsed evidence_items is not a list\")\n",
    "\n",
    "            positive_found = False\n",
    "            for evidence_item in evidence_list:\n",
    "                 # Ensure evidence_item is a list/tuple with at least 2 elements\n",
    "                if isinstance(evidence_item, (list, tuple)) and len(evidence_item) >= 2:\n",
    "                    if verbose:\n",
    "                        print(f\"Processing evidence item: {evidence_item}\")\n",
    "                    sentence_text = str(evidence_item[0]).strip()\n",
    "                    page_title = str(evidence_item[1]).strip()\n",
    "\n",
    "                    if sentence_text and page_title:\n",
    "                        positive_text = f\"{sentence_text} {page_title}\"\n",
    "                        if verbose:\n",
    "                            print(f\"Positive text: {positive_text}\")\n",
    "\n",
    "                        if not is_eval_set:\n",
    "                            # Create training examples (pairs for MNRL)\n",
    "                            examples.append(InputExample(texts=[claim, positive_text]))\n",
    "                            positive_found = True\n",
    "                        else:\n",
    "                            # Create evaluation data structure\n",
    "                            queries[claim_id] = claim # Store claim as query\n",
    "\n",
    "                            # Unique ID for this evidence sentence + title combination\n",
    "                            doc_content = positive_text\n",
    "                            # Simple way to avoid duplicates in corpus for the *same evidence*\n",
    "                            # This assigns a NEW doc_id for each occurrence, which is needed for IR evaluator\n",
    "                            # if claim text is unique per evaluation row.\n",
    "                            current_doc_id = f\"doc_{doc_id_counter}\"\n",
    "                            corpus[current_doc_id] = doc_content\n",
    "                            doc_id_counter += 1\n",
    "\n",
    "                            if claim_id not in relevant_docs:\n",
    "                                relevant_docs[claim_id] = set()\n",
    "                            relevant_docs[claim_id].add(current_doc_id) # Mark this doc as relevant for this claim\n",
    "                            positive_found = True\n",
    "                else:\n",
    "                    logger.warning(f\"Skipping malformed evidence item in row {index}: {evidence_item}\")\n",
    "                    pass # Keep processing other items\n",
    "\n",
    "            # Optional: Add hard negative. I do not use this for MNRL, but it's here for reference.\n",
    "            \"\"\"\n",
    "            if not is_eval_set and positive_found and 'full_text' in row:\n",
    "                try:\n",
    "                    full_text = str(row['full_text'])\n",
    "                    page_sentences = nltk.sent_tokenize(full_text)\n",
    "                    positive_sentences = {str(item[0]).strip() for item in evidence_list if isinstance(item, (list, tuple)) and len(item) > 0}\n",
    "\n",
    "                    potential_negatives = [s.strip() for s in page_sentences if s.strip() and s.strip() not in positive_sentences]\n",
    "                    if potential_negatives:\n",
    "                      hard_negative_sentence = random.choice(potential_negatives)\n",
    "                      # Assume negative uses the *same* page title as one of the positives\n",
    "                      # This requires selecting one specific positive's title if multiple exist.\n",
    "                      # Simplification: Use the title from the first valid positive item\n",
    "                      first_valid_title = next((str(item[1]).strip() for item in evidence_list if isinstance(item, (list, tuple)) and len(item) >= 2 and str(item[0]).strip() and str(item[1]).strip()), None)\n",
    "                      if first_valid_title:\n",
    "                            hard_negative_text = f\"{hard_negative_sentence} {first_valid_title}\"\n",
    "                            # Triplet format: examples.append(InputExample(texts=[claim, positive_text, hard_negative_text]))\n",
    "                            # For MNRL, we can add as a negative pair (not directly supported in this format)\n",
    "                            examples.append(InputExample(texts=[claim, hard_negative_text])) # Add as a negative pair\n",
    "                except Exception as e_neg:\n",
    "                    logger.warning(f\"Could not generate hard negative for row {index}: {e_neg}\")\n",
    "            \"\"\"\n",
    "        except (ValueError, SyntaxError, TypeError) as e:\n",
    "            logger.warning(f\"Skipping row {index} due to error parsing evidence_items: {e}. Content: {row.get('evidence_items', 'N/A')}\")\n",
    "        except Exception as e:\n",
    "             logger.error(f\"Unexpected error processing row {index}: {e}\")\n",
    "\n",
    "    if is_eval_set:\n",
    "        logger.info(f\"Prepared evaluation data: {len(queries)} queries, {len(corpus)} corpus docs, {sum(len(v) for v in relevant_docs.values())} relevant pairs.\")\n",
    "        # Basic check for empty structures\n",
    "        if not queries or not corpus or not relevant_docs:\n",
    "             logger.warning(\"Evaluation data structures are empty or incomplete.\")\n",
    "        return queries, corpus, relevant_docs\n",
    "    else:\n",
    "        logger.info(f\"Created {len(examples)} training pairs.\")\n",
    "        if not examples:\n",
    "             logger.warning(\"No training examples were created. Check data and filtering.\")\n",
    "        return examples\n",
    "\n",
    "# --- Load Data ---\n",
    "logger.info(\"Loading training data...\")\n",
    "train_samples = load_and_prepare_data(TRAIN_CSV_PATH, is_eval_set=False, max_samples=MAX_SAMPLES, verbose=False) # Limit training set size if large\n",
    "\n",
    "\n",
    "evaluator = None\n",
    "if os.path.exists(DEV_CSV_PATH):\n",
    "    logger.info(\"Loading development (evaluation) data...\")\n",
    "    dev_queries, dev_corpus, dev_relevant_docs = load_and_prepare_data(DEV_CSV_PATH, is_eval_set=True, max_samples=MAX_VALID_SAMPLES) # Limit dev set size if large\n",
    "    if dev_queries and dev_corpus and dev_relevant_docs:\n",
    "        evaluator = InformationRetrievalEvaluator(dev_queries, dev_corpus, dev_relevant_docs,\n",
    "                                                name='fever-dev',\n",
    "                                                show_progress_bar=True,\n",
    "                                                write_csv=True\n",
    "        )\n",
    "        logger.info(\"InformationRetrievalEvaluator created for development set.\")\n",
    "    else:\n",
    "         logger.warning(\"Could not create evaluator due to missing/empty dev data structures.\")\n",
    "else:\n",
    "    logger.info(\"No development set specified or found. Skipping evaluation during training.\")\n",
    "\n",
    "\n",
    "'''\n",
    "Count of each label in the dataset:\n",
    "label\n",
    "REFUTES     516\n",
    "SUPPORTS    508\n",
    "Name: count, dtype: int64\n",
    "Count of each label in the dataset:\n",
    "label\n",
    "REFUTES     20\n",
    "SUPPORTS    16\n",
    "Name: count, dtype: int64\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gw_i1meIG8Eb"
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- Model & Training Setup ---\n",
    "if train_samples: # Proceed only if training data was loaded successfully\n",
    "    logger.info(f\"Loading pre-trained model: {MODEL_NAME}\")\n",
    "    # Use models.Transformer to ensure we can add pooling layer if needed, though MPNet usually has one.\n",
    "    word_embedding_model = models.Transformer(MODEL_NAME)\n",
    "    pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "    # No dense layer needed unless changing output dimensions\n",
    "    model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "    logger.info(f\"Using MultipleNegativesRankingLoss\")\n",
    "    loss = losses.MultipleNegativesRankingLoss(model=model)\n",
    "\n",
    "    # DataLoader\n",
    "    # Ensure shuffling for training data\n",
    "    train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=TRAIN_BATCH_SIZE)\n",
    "    train_loss = losses.MultipleNegativesRankingLoss(model=model)\n",
    "    logger.info(f\"Training batch size: {TRAIN_BATCH_SIZE}\")\n",
    "\n",
    "\n",
    "    # Calculate total steps and warmup steps if not fixed\n",
    "    num_training_steps = int(len(train_dataloader) * NUM_EPOCHS)\n",
    "    warmup_steps = math.ceil(num_training_steps * 0.1) # Alternative: 10% of total steps\n",
    "    #warmup_steps = WARMUP_STEPS\n",
    "\n",
    "    logger.info(\"Starting model training...\")\n",
    "    model.fit(train_objectives=[(train_dataloader, loss)],\n",
    "              epochs=NUM_EPOCHS,\n",
    "              optimizer_params={'lr': LEARNING_RATE},\n",
    "              warmup_steps=warmup_steps,\n",
    "              evaluator=evaluator,\n",
    "              evaluation_steps=EVAL_STEPS if evaluator else 0, # Only evaluate if evaluator exists\n",
    "              output_path=OUTPUT_PATH,\n",
    "              checkpoint_path=os.path.join(OUTPUT_PATH, 'checkpoints'),\n",
    "              checkpoint_save_steps=EVAL_STEPS * 2 if evaluator else 1000, # Save checkpoints periodically\n",
    "              checkpoint_save_total_limit=3, # Keep only the last few checkpoints\n",
    "              show_progress_bar=True)\n",
    "\n",
    "    logger.info(f\"Training complete. Model saved to: {OUTPUT_PATH}\")\n",
    "\n",
    "    # Optional: Save the final model in a new directory\n",
    "    os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "    model.save(OUTPUT_PATH)\n",
    "    if verbose:\n",
    "        print(f\"Final model saved to: {OUTPUT_PATH}\")\n",
    "    logger.info(f\"Final model explicitly saved to: {OUTPUT_PATH}\")\n",
    "\n",
    "else:\n",
    "    logger.error(\"Cannot start training because no training samples were loaded/prepared.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
