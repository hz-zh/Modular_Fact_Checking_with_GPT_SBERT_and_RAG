{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"05EjrBx7ENj6"},"outputs":[],"source":["import openai\n","from openai import OpenAI\n","\n","from google.colab import drive\n","import gc\n","\n","import json\n","import os\n","import io\n","import pandas as pd\n","import numpy as np\n","import scipy.stats as stats\n","import re\n","import time\n","import datetime\n","from random import random"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kv0BFFXK59Aq"},"outputs":[],"source":["from sklearn.metrics import confusion_matrix, classification_report\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15965,"status":"ok","timestamp":1743211344228,"user":{"displayName":"Henry Zelenak","userId":"01809909909045225068"},"user_tz":240},"id":"PZjBEw72qdiq","outputId":"20a33094-0d3b-402b-d76d-33306218e7a1"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n","[nltk_data] Downloading package maxent_ne_chunker to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n","[nltk_data] Downloading package maxent_ne_chunker_tab to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/words.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package treebank to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/treebank.zip.\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]},{"data":{"text/plain":["True"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk import pos_tag\n","from nltk import ne_chunk\n","from nltk.tree import Tree\n","from nltk import RegexpParser\n","from nltk import word_tokenize\n","from nltk import pos_tag\n","\n","# Download the necessary NLTK data files\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger_eng')\n","nltk.download('maxent_ne_chunker')\n","nltk.download('maxent_ne_chunker_tab')\n","nltk.download('words')\n","nltk.download('stopwords')\n","nltk.download('treebank')\n","nltk.download('punkt_tab')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":107},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1743211344236,"user":{"displayName":"Henry Zelenak","userId":"01809909909045225068"},"user_tz":240},"id":"V7IiY8MjTgyZ","outputId":"52072229-107e-4ff4-95d3-ae826d6c5679"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nimport torch\\nfrom sentence_transformers import SentenceTransformer, util\\nfrom sklearn.decomposition import PCA\\n\\nfrom wordcloud import WordCloud\\n\\nimport plotly.express as px\\nimport plotly.graph_objects as go\\nfrom plotly.subplots import make_subplots\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.metrics.pairwise import cosine_similarity\\nfrom sklearn.metrics import cohen_kappa_score\\nfrom sklearn.metrics import confusion_matrix, classification_report\\nfrom sklearn.metrics import accuracy_score\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import OrdinalEncoder\\n'"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","import torch\n","from sentence_transformers import SentenceTransformer, util\n","from sklearn.decomposition import PCA\n","\n","from wordcloud import WordCloud\n","\n","import plotly.express as px\n","import plotly.graph_objects as go\n","from plotly.subplots import make_subplots\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.metrics import cohen_kappa_score\n","from sklearn.metrics import confusion_matrix, classification_report\n","from sklearn.metrics import accuracy_score\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import OrdinalEncoder\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v7wHT41emmSX"},"outputs":[],"source":["from google.colab import userdata\n","api_key = userdata.get('openaikey')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7N1vsuEFnvta"},"outputs":[],"source":["client = OpenAI(api_key=api_key)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16399,"status":"ok","timestamp":1743211362655,"user":{"displayName":"Henry Zelenak","userId":"01809909909045225068"},"user_tz":240},"id":"yi4Bf4jl3X7L","outputId":"3f18a991-253c-4288-8145-5e85a8b394c3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# Mount google drive\n","\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1743211362691,"user":{"displayName":"Henry Zelenak","userId":"01809909909045225068"},"user_tz":240},"id":"OjYHaKKQ6dqR","outputId":"c3a61bcb-859e-487e-8692-c56b941cf541"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/My Drive/SUNY_Poly_DSA598\n"]}],"source":["\n","%cd ./drive/My Drive/SUNY_Poly_DSA598/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":135,"status":"ok","timestamp":1743211362836,"user":{"displayName":"Henry Zelenak","userId":"01809909909045225068"},"user_tz":240},"id":"p5P6aCEs6oL0","outputId":"1b17ff25-c08a-4973-e46b-abb4904e4bd5"},"outputs":[{"name":"stdout","output_type":"stream","text":["archive\t\t\t    .git\t\t\t    transcribe_voice_notes.ipynb\n","datasets\t\t    .gitignore\t\t\t    work_documents\n","FEVER_set_creation.ipynb    liar_gpt4omini_base_eval.ipynb\n","FEVER_tuning_archive.ipynb  presentation\n"]}],"source":["!ls -a"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YjqMXr8E5Oze"},"outputs":[],"source":["\n","def load_jsonl(file_path, encoding='utf-8'):\n","    \"\"\"Loads a JSON Lines file into a list of Python objects.\"\"\"\n","    data = []\n","    with open(file_path, 'r', encoding=encoding) as f:  # Specify encoding for safety\n","        for line in f:\n","            data.append(json.loads(line))  # Parse each line individually\n","    return data"]},{"cell_type":"markdown","metadata":{"id":"bYJip1V2L_UA"},"source":["## FEVER Fine-tuning Dataset Creation"]},{"cell_type":"markdown","metadata":{"id":"huzxXq7GMDBp"},"source":["### Sentence extraction from Wikipedia text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V6FT63bDSwmz"},"outputs":[],"source":["fever_path = './datasets/FEVER/'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":168,"status":"ok","timestamp":1743211363073,"user":{"displayName":"Henry Zelenak","userId":"01809909909045225068"},"user_tz":240},"id":"ydXkEBN5ZLLn","outputId":"0dc42240-060c-4626-c3a5-d7e4d79b5189"},"outputs":[{"data":{"text/plain":["0"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":127,"status":"ok","timestamp":1743211363222,"user":{"displayName":"Henry Zelenak","userId":"01809909909045225068"},"user_tz":240},"id":"hkeMRidFKro_","outputId":"2295603d-3a8e-45d0-c71b-a9ce7df34363"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/My Drive/SUNY_Poly_DSA598\n"]}],"source":["!pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":213442,"status":"ok","timestamp":1743211576665,"user":{"displayName":"Henry Zelenak","userId":"01809909909045225068"},"user_tz":240},"id":"UNjlqYKmCfy7","outputId":"2a38b390-25c5-4418-ee9a-46021b83ece7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Attempting to load 109 Wikipedia pages from ./datasets/FEVER/wiki-pages...\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-001.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-002.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-003.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-004.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-005.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-006.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-007.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-008.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-009.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-010.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-011.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-012.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-013.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-014.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-015.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-016.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-017.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-018.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-019.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-020.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-021.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-022.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-023.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-024.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-025.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-026.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-027.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-028.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-029.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-030.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-031.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-032.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-033.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-034.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-035.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-036.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-037.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-038.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-039.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-040.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-041.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-042.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-043.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-044.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-045.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-046.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-047.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-048.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-049.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-050.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-051.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-052.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-053.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-054.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-055.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-056.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-057.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-058.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-059.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-060.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-061.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-062.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-063.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-064.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-065.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-066.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-067.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-068.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-069.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-070.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-071.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-072.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-073.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-074.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-075.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-076.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-077.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-078.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-079.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-080.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-081.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-082.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-083.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-084.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-085.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-086.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-087.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-088.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-089.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-090.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-091.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-092.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-093.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-094.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-095.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-096.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-097.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-098.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-099.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-100.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-101.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-102.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-103.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-104.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-105.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-106.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-107.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-108.jsonl\n","Loading file: ./datasets/FEVER/wiki-pages/wiki-109.jsonl\n","Loaded texts from 5416537 Wikipedia pages (minimal memory).\n"]}],"source":["\n","\n","def load_wiki_pages_minimal_memory(wiki_dir, num_files_to_load=50):\n","  \"\"\"\n","  Loads Wikipedia page text from JSONL files into a dictionary (minimal memory usage).\n","  Loads only the 'text' field and only the first 'num_files_to_load' files.\n","\n","  Args:\n","      wiki_dir (str): Path to the directory containing wiki-*.jsonl files.\n","      num_files_to_load (int): Number of wiki files to load (default: 3).\n","\n","  Returns:\n","      dict: A dictionary where keys are page titles and values are the page text.\n","  \"\"\"\n","  print(f\"Attempting to load {num_files_to_load} Wikipedia pages from {wiki_dir}...\")\n","  wiki_pages = {}\n","  loaded_files_count = 0\n","  for filename in sorted(os.listdir(wiki_dir)):\n","      if filename.startswith('wiki-') and filename.endswith('.jsonl'):\n","        filepath = os.path.join(wiki_dir, filename)\n","        print(f\"Loading file: {filepath}\")\n","        with open(filepath, 'r', encoding='utf-8') as f:  # Specify encoding for safety\n","          for line in f:\n","            data = json.loads(line)\n","            # Create a object in the dictionary for the data\n","            wiki_pages[data['id']] = {\n","              'text': data['text'],\n","              'lines': data['lines']\n","            }\n","\n","        loaded_files_count += 1\n","      if loaded_files_count >= num_files_to_load:\n","            break\n","  return wiki_pages\n","\n","\n","wiki_pages_dir = './datasets/FEVER/wiki-pages'\n","num_to_load = len(os.listdir(wiki_pages_dir))\n","wiki_page_list_dicts = load_wiki_pages_minimal_memory(wiki_pages_dir, num_to_load)\n","\n","print(f\"Loaded texts from {len(wiki_page_list_dicts)} Wikipedia pages (minimal memory).\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1743183815966,"user":{"displayName":"Henry Zelenak","userId":"01809909909045225068"},"user_tz":240},"id":"MXhyBLp0TyC7","outputId":"74167bab-83b3-4b53-c436-e09cb82523bf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Page 1: \n","Page 2: 1928_in_association_football\n","Page 3: 1986_NBA_Finals\n","Page 4: 1901_Villanova_Wildcats_football_team\n","Page 5: 1992_Northwestern_Wildcats_football_team\n","Page 6: 1897_Princeton_Tigers_football_team\n","Page 7: 1536_in_philosophy\n","Page 8: ...Di_terra\n","Page 9: 1967–68_MJHL_season\n"]}],"source":["for page, i in zip(wiki_page_list_dicts, range(1, 10)):\n","  print(f\"Page {i}: {page}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2thzifpXZh1R"},"outputs":[],"source":["fever_train = load_jsonl(fever_path + 'paper_dev.jsonl')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":14,"status":"ok","timestamp":1743211838778,"user":{"displayName":"Henry Zelenak","userId":"01809909909045225068"},"user_tz":240},"id":"oNTvIJKRbH5R","outputId":"fff0be23-c4cc-48cc-eabf-5ccfbb8f99fd"},"outputs":[{"name":"stdout","output_type":"stream","text":["id: 91198\n","verifiable: NOT VERIFIABLE\n","label: NOT ENOUGH INFO\n","claim: Colin Kaepernick became a starting quarterback during the 49ers 63rd season in the National Football League.\n","evidence: [[[108548, None, None, None]]]\n","id: 194462\n","verifiable: NOT VERIFIABLE\n","label: NOT ENOUGH INFO\n","claim: Tilda Swinton is a vegan.\n","evidence: [[[227768, None, None, None]]]\n","id: 137334\n","verifiable: VERIFIABLE\n","label: SUPPORTS\n","claim: Fox 2000 Pictures released the film Soul Food.\n","evidence: [[[289914, 283015, 'Soul_Food_-LRB-film-RRB-', 0]], [[291259, 284217, 'Soul_Food_-LRB-film-RRB-', 0]], [[293412, 285960, 'Soul_Food_-LRB-film-RRB-', 0]], [[337212, 322620, 'Soul_Food_-LRB-film-RRB-', 0]], [[337214, 322622, 'Soul_Food_-LRB-film-RRB-', 0]]]\n","id: 166626\n","verifiable: NOT VERIFIABLE\n","label: NOT ENOUGH INFO\n","claim: Anne Rice was born in New Jersey.\n","evidence: [[[191656, None, None, None], [191657, None, None, None]]]\n","id: 111897\n","verifiable: VERIFIABLE\n","label: REFUTES\n","claim: Telemundo is a English-language television network.\n","evidence: [[[131371, 146144, 'Telemundo', 0]], [[131371, 146148, 'Telemundo', 1]], [[131371, 146150, 'Telemundo', 4], [131371, 146150, 'Hispanic_and_Latino_Americans', 0]], [[131371, 146151, 'Telemundo', 5]]]\n"]}],"source":["# Print the first few  nested objects\n","for item, i in zip(fever_train, range(5)):\n","  for key in (item.keys()):\n","    print(key + ': ' + str(fever_train[i][key]))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":50,"status":"ok","timestamp":1743183835394,"user":{"displayName":"Henry Zelenak","userId":"01809909909045225068"},"user_tz":240},"id":"k4wS0wmsdvKG","outputId":"a95f47f8-b61e-4dda-8d25-08c6840eb3cb"},"outputs":[{"data":{"text/plain":["9999"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["len(fever_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1743211873446,"user":{"displayName":"Henry Zelenak","userId":"01809909909045225068"},"user_tz":240},"id":"1tgTOIxSPbvf","outputId":"aee9c6d7-c6cd-4528-b00c-cb133daea7f8"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Page Title: 1993_European_Cup_-LRB-athletics-RRB-\n","Keys in page_data: dict_keys(['text', 'lines'])\n","Type of page_data['lines']: <class 'str'>\n","Number of sentences: 5\n","0\tThe 1993 European Cup was the 14th edition of the European Cup of athletics .\tEuropean Cup\tEuropean Cup (athletics)\tathletics\tAthletics (sport)\n","1\t\n","2\t\n","3\tThe Super League Finals were held in Rome , Italy .\tRome\tRome\tItaly\tItaly\n","4\n","\n","Page Title: 1905_Tempe_Normal_Owls_football_team\n","Keys in page_data: dict_keys(['text', 'lines'])\n","Type of page_data['lines']: <class 'str'>\n","Number of sentences: 4\n","0\tThe 1905 Tempe Normal Owls football team was an American football team that represented Tempe Normal School -LRB- later renamed Arizona State University -RRB- as an independent during the 1905 college football season .\tAmerican football\tAmerican football\tArizona State University\tArizona State University\t1905 college football season\t1905 college football season\n","1\tIn their seventh season under head coach Frederick M. Irish , the Owls compiled a 0 -- 3 record and were outscored by their opponents by a combined total of 28 to 8 .\tFrederick M. Irish\tFrederick M. Irish\n","2\tThe team lost games to the Phoenix Indian School -LRB- 8 -- 17 -RRB- , Phoenix High School -LRB- 0 -- 5 -RRB- , and Tempe High School -LRB- 0 -- 6 -RRB- .\n","3\n","\n","Page Title: 1976_Winter_Paralympics\n","Keys in page_data: dict_keys(['text', 'lines'])\n","Type of page_data['lines']: <class 'str'>\n","Number of sentences: 9\n","0\tThe 1976 Winter Paralympic Games -LRB- Swedish : Paralympiska vinterspelen 1976 -RRB- were the first Winter Paralympics .\tWinter Paralympics\tWinter Paralympics\n","1\tThey were held in Örnsköldsvik , Sweden , from 21 to 28 February 1976 .\tÖrnsköldsvik\tÖrnsköldsvik\tSweden\tSweden\n","2\tThe disabilities included in this Paralympics were blindness and amputees .\tdisabilities\tdisability\tblindness\tblindness\n","3\tSixteen countries took part with 53 athletes .\tcountries\tcountries\n","4\tThere were competitions in Alpine and Nordic skiing for amputee and visually impaired athletes , and a demonstration event in ice sledge racing .\tamputee\tamputee\tAlpine\tParalympic alpine skiing\tNordic skiing\tParalympic cross-country skiing\n","5\t\n","6\t\n","7\tOriginally known as the 1st Winter Olympic Games for the Disabled .\n","8\n"]}],"source":["for page_title in list(wiki_page_list_dicts.keys())[500:503]: # Inspect 3 pages\n","    page_data = wiki_page_list_dicts[page_title]\n","    print(f\"\\nPage Title: {page_title}\")\n","    print(f\"Keys in page_data: {page_data.keys()}\")\n","    if 'lines' in page_data:\n","        print(f\"Type of page_data['lines']: {type(page_data['lines'])}\")\n","        # If it's a string, print the first few lines (sentences)\n","        if isinstance(page_data['lines'], str):\n","            sentences = page_data['lines'].strip().split('\\n')\n","            print(f\"Number of sentences: {len(sentences)}\")\n","            if sentences:\n","                for line in sentences[:len(sentences)]:  # Print the first 5 sentences\n","                    print(line)\n","            else:\n","                print(\"No sentences found in the 'lines' field.\")\n","        else:\n","            print(\"The 'lines' field is not a string.\")\n","    else:\n","        print(\"No 'lines' key found in page_data\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lANVF7e4gjEv"},"outputs":[],"source":["fever_train = load_jsonl(fever_path + 'paper_dev.jsonl')"]},{"cell_type":"code","source":["# Print the labels in fever_train\n","fever_train_labels = [item['label'] for item in fever_train]\n","print(f\"Unique labels in fever_train: {set(fever_train_labels)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Caats6mmv3tJ","executionInfo":{"status":"ok","timestamp":1743215649412,"user_tz":240,"elapsed":10,"user":{"displayName":"Henry Zelenak","userId":"01809909909045225068"}},"outputId":"9a423ca3-039f-4ab2-9b4d-c3ebc6e6d623"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["Unique labels in fever_train: {'REFUTES', 'NOT ENOUGH INFO', 'SUPPORTS'}\n"]}]},{"cell_type":"code","execution_count":48,"metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1743215401606,"user":{"displayName":"Henry Zelenak","userId":"01809909909045225068"},"user_tz":240},"id":"LTJeOVlTbJLh"},"outputs":[],"source":["import random\n","import requests\n","import re\n","import nltk\n","\n","\n","def mediawiki_random_text(verbose=False):\n","    \"\"\"\n","    Returns 5 random sentences from the introduction sections of 5 random Wikipedia articles, and the introduction text from one of the articles.\n","    \"\"\"\n","    if verbose:\n","        print(\"Generating random text from Wikipedia articles...\")\n","    random_sentences = []\n","    intro_text = ''\n","\n","    # Call the English Wikipedia API to get a random article (Category: Wikipedia articles)\n","    url = f\"https://en.wikipedia.org/w/api.php?action=query&list=random&rnlimit=1&format=json&rnnamespace=0\"\n","    response = requests.get(url)\n","    data = response.json()\n","    page_id = data['query']['random'][0]['id']\n","    page_title = data['query']['random'][0]['title']\n","    if verbose:\n","        print(f\"Random article: {page_title} (ID: {page_id})\")\n","    # Call the English Wikipedia API to get the content of the article\n","    url = f\"https://en.wikipedia.org/w/api.php?action=parse&pageid={page_id}&prop=text&format=json\"\n","    response = requests.get(url)\n","    data = response.json()\n","    if 'parse' in data:\n","        page_content = data['parse']['text']['*']\n","        # Extract the introduction section (the first paragraph element)\n","        intro_section = re.search(r'<p>(.*?)</p>', page_content, re.DOTALL)\n","        if intro_section:\n","            intro_text = intro_section.group(1) # Get the text inside the first <p> tag\n","            # Remove HTML tags\n","            intro_text = re.sub(r'<.*?>', '', intro_text)\n","            # Remove references\n","            intro_text = re.sub(r'\\[.*?\\]', '', intro_text)\n","            # Remove digits that are preceded by any letter, period, colon, semicolon, endash (–), and emdash(—) and followed by a space\n","            intro_text = re.sub(r'(?<=[a-zA-Z0-9\\.\\:\\;\\–\\—])\\d+(?=\\s)', '', intro_text)\n","            # Convert encoded html entities to unicode (e.g., &amp; to &)\n","            intro_text = re.sub(r'&[a-zA-Z0-9#]+;', '', intro_text)\n","            # Remove extra whitespace\n","            intro_text = re.sub(r'\\s+', ' ', intro_text).strip()\n","            # Split into sentences\n","            sentences = nltk.sent_tokenize(intro_text)\n","            # If there are fewer than 5 sentences, recursively call the function\n","            if len(sentences) < 5:\n","                if verbose:\n","                    print(f\"Less than 5 sentences found in the introduction section of {page_title}. Calling recursively...\")\n","                    print('_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_')\n","                return mediawiki_random_text(verbose=verbose)\n","            random_sentences = random.sample(sentences, min(5, len(sentences)))\n","            if verbose:\n","                print(f\"Random sentences from {page_title}:\")\n","                for sentence in random_sentences:\n","                    print(f\"- {sentence}\")\n","        else:\n","            if verbose:\n","                print(f\"No introduction section found for {page_title}.\")\n","    else:\n","        if verbose:\n","            print(f\"Failed to retrieve content for {page_title}.\")\n","    if verbose:\n","        print(\"Random text generation completed.\")\n","        print('---------------------------------------------')\n","    return random_sentences, intro_text\n","\n","\n","def extract_evidence_text_debug(fever_item, wiki_page_dict, verbose=False, debug=False):\n","    \"\"\"\n","    Extracts evidence sentences with debugging prints.\n","    \"\"\"\n","\n","    if verbose:\n","      print(f\"Starting evidence extraction for claim: {fever_item['claim']}\")\n","    if fever_item['label'] == 'NOT ENOUGH INFO':\n","      if verbose:\n","        print(f\"NOT ENOUGH INFO found for claim: {fever_item['claim']}, getting random text...\")\n","      # Call mediawiki_random_text function to get random sentences and the introduction text\n","      rand_sents, intro = mediawiki_random_text(verbose=verbose)\n","\n","      return rand_sents, intro\n","\n","    evidence_sentences = []\n","    text_str = ''\n","    entities = []\n","    for evidence_set in fever_item['evidence']: # Looping over evidence sets (there should be only one)\n","        for evidence_piece in evidence_set: # Looping over evidence pieces (there will be 1 or more, these are the evidence sentences from different pages, for the same claim)\n","            if debug:\n","              print(f\"DEBUG 0: Evidence piece: {evidence_piece}\") # DEBUG 0\n","            if len(evidence_piece) == 4:  # Check if the evidence piece has 4 elements (annotation_id, evidence_id, page_title, sentence_id)\n","                annotation_id, evidence_id, page_title, sentence_id = evidence_piece # Unpack the evidence piece\n","                if debug:\n","                  print(f\"DEBUG 1: Processing evidence: page_title={page_title}, sentence_id={sentence_id}\") # DEBUG 1\n","                if page_title is not None and sentence_id is not None: # Check if page_title and sentence_id are not None\n","                    wiki_page = wiki_page_dict.get(page_title) # Retrieve the wiki page from the dictionary\n","                    if debug:\n","                      print(f\"DEBUG 2: Wiki page retrieved: {wiki_page is not None}\") # DEBUG 2\n","                    if wiki_page: # Check if the wiki page for these sentences\n","                        if debug:\n","                          print(f\"DEBUG 3: Wiki page keys: {wiki_page.keys()}\") # DEBUG 3\n","                        if 'lines' in wiki_page and isinstance(wiki_page['lines'], str): # Check if 'lines' key exists and is a string\n","                            lines_str = wiki_page['lines']\n","                            text_str = wiki_page['text']\n","                            sentences = lines_str.strip().split('\\n')\n","                            # Drop blank lines\n","                            #sentences = [sentence for sentence in sentences if sentence.strip()]\n","\n","                            if debug:\n","                              print(f\"DEBUG 4: Number of sentences found: {len(sentences)}\") # DEBUG 4\n","                            if sentence_id < len(sentences):  # Check if sentence_id is within the range of sentences\n","                              for sentence, _ in zip(sentences, range(len(sentences))): # Loop over the sentences and their indices\n","                                if len(sentence.split('\\t')) < 2 or sentence.split('\\t')[1].strip() == '': #\n","                                  if debug:\n","                                    print(f\"DEBUG 4.1: Skipping blank line at index {_}\")\n","                                  continue\n","                                if debug:\n","                                  print(f\"DEBUG 5: Line retrieved: {sentence}\") # DEBUG 5\n","                                sentence_text = sentence.split('\\t')[1].strip() # Extract the text after the tab character (To skip the index at the beginning and the entities after: \"0\tThe 1905 Tempe Normal Owls football team was an American football team that represented Tempe Normal School -LRB- later renamed Arizona State University -RRB- as an independent during the 1905 college football season .\tAmerican football\tAmerican football\tArizona State University\tArizona State University\t1905 college football season\t1905 college football season\")\n","                                if debug:\n","                                  print(f\"DEBUG 6: Sentence text extracted: {sentence_text}\") # DEBUG 6\n","                                evidence_sentences.append(sentence_text) # Append the sentence text to the evidence_sentences list\n","                                if debug:\n","                                  print(f\"DEBUG 7: Full text: {text_str}\") # DEBUG 7\n","                            else:\n","                              if verbose:\n","                                print(f\"Warning: Sentence index out of range for page: {page_title}, sentence_id: {sentence_id} (Number of sentences: {len(sentences)})\")\n","                        else:\n","                          if verbose:\n","                            print(f\"Warning: Could not retrieve page or lines for title: {page_title}\")\n","                    else:\n","                      if verbose:\n","                        print(f\"Warning: Could not retrieve wiki page for title: {page_title}\")\n","            else:\n","              if verbose:\n","                print(f\"Warning: Unexpected evidence format: {evidence_piece}\")\n","    if verbose:\n","      print(f\"Evidence extraction completed for claim: {fever_item['claim']}\")\n","      print(f\"Number of evidence sentences found: {len(evidence_sentences)}\")\n","      print('--------------------------------------------------------------')\n","    return evidence_sentences, text_str\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o8hJ2T4GsSls"},"outputs":[],"source":["# Test the mediawiki_random_text function\n","random_text, intro_text = mediawiki_random_text(verbose=True)\n","print(\"Random text from Wikipedia:\")\n","print(random_text)\n","print(\"Introduction text:\")\n","print(intro_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E7OpFaA2LRzW"},"outputs":[],"source":["# Test run evidence extraction\n","claim_item = fever_train[10]\n","evidence, full_text = extract_evidence_text_debug(claim_item, wiki_page_list_dicts, verbose=True, debug=True)\n","\n","print(f\"\\nClaim: {claim_item['claim']}\")\n","print(\"\\nEvidence Sentences:\")\n","for sentence in evidence:\n","    print(f\"- {sentence}\")\n","print(\"\\nFull Text:\")\n","print(full_text)"]},{"cell_type":"markdown","metadata":{"id":"mc-HCMvNAwBm"},"source":["### Create training files for Various Models"]},{"cell_type":"code","execution_count":55,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1743215777210,"user":{"displayName":"Henry Zelenak","userId":"01809909909045225068"},"user_tz":240},"id":"d_yrgf2DGB3U","outputId":"aec1fddd-bebf-4d48-b060-82a527300478"},"outputs":[{"output_type":"stream","name":"stdout","text":["Today's date: 03-28\n"]}],"source":["# Set the date of the data creation (today)\n","now = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=-5)))\n","date_fmt = now.strftime(\"%m-%d\")\n","print(f\"Today's date: {date_fmt}\")\n","\n","fever_label_mapping = {\n","    \"SUPPORTS\": \"1\",\n","    \"REFUTES\": \"0\",\n","    \"NOT ENOUGH INFO\": \"-1\"\n","}\n","\n","# Helper function to allow balanced index-based selection (which is required to segment the data between different NLI models, ensuring maximum variability)\n","def intersperse_labels(data):\n","  \"\"\"Intersperses SUPPORTS and REFUTES examples for guaranteed balance.\"\"\"\n","  print('>>>>>> STARTING INTERSPERSING SUPPORTS AND REFUTES <<<<<<<<<<')\n","  supports = [item for item in data if item.get('label') == 'SUPPORTS']\n","  refutes = [item for item in data if item.get('label') == 'REFUTES']\n","  nei = [item for item in data if item.get('label') == 'NOT ENOUGH INFO']\n","\n","  interspersed_data = []\n","  min_len = min(len(supports), len(refutes)) # Handle potential slight imbalances\n","  for i in range(min_len):\n","      interspersed_data.append(supports[i])\n","      interspersed_data.append(refutes[i])\n","      interspersed_data.append(nei[i]) # Add the NOT ENOUGH INFO item\n","\n","  # print(f\"Number of SUPPORTS in resulting data: {len([item for item in interspersed_data if item['label'] == 'SUPPORTS'])}\")\n","  # print(f\"Number of REFUTES in resulting data: {len([item for item in interspersed_data if item['label'] == 'REFUTES'])}\")\n","  print(f\"Length of fever_data after interspersing: {len(interspersed_data)}\")\n","  print('>>>>>>>>>>> INTERSPERSING SUPPORTS AND REFUTES COMPLETED <<<<<<<<<<<')\n","  return interspersed_data\n","\n","# Helper function to dry run the evidence extraction and return jsonl without the items that fail to associate with the wiki pages\n","def remove_items_with_no_evidence(fever_data, verbose=False):\n","    \"\"\"\n","    Removes items from fever_data that have no evidence associated with them before we attempt to retrieve it during the conversion.\n","    \"\"\"\n","    print(f\">>>>>>>>>>> STARTING REMOVAL OF ITEMS WITH NO EVIDENCE <<<<<<<<<<<\")\n","    # Call the the extraction function in a loop and only add the items that have evidence to the new list\n","    filtered_data = []\n","    dropped_true_count = 0\n","    dropped_false_count = 0\n","    for fever_item in fever_data:\n","        claim = fever_item['claim']\n","\n","        if claim == 'NOT ENOUGH INFO':\n","          filtered_data.append(fever_item)\n","          if verbose:\n","            print(f\"\\tItem is NEI, skipping evidence extraction.\")\n","          continue\n","        else:\n","          evidence_sentences, _ = extract_evidence_text_debug(fever_item, wiki_page_list_dicts)\n","          if len(evidence_sentences) > 0:\n","            filtered_data.append(fever_item)\n","          else:\n","            if claim == 'SUPPORTS':\n","                dropped_true_count += 1\n","            elif claim == 'REFUTES':\n","                dropped_false_count += 1\n","            if verbose:\n","              print(f\"\\tWarning: Dropping item with claim: {claim} due to no evidence found.\")\n","              continue\n","\n","    print(f\"Dropped {dropped_true_count} SUPPORTS and {dropped_false_count} REFUTES items due to no evidence found.\")\n","    print(f\"Total items dropped: {dropped_true_count + dropped_false_count}\")\n","    print(f\"Total items remaining: {len(filtered_data)}\")\n","    print('>>>>>>>>>>> REMOVAL OF ITEMS WITH NO EVIDENCE COMPLETED <<<<<<<<<<<')\n","    print('---------------------------------------------')\n","    return filtered_data\n","\n","def convert_fever_to_fine_tuning(fever_data, output_filepath, wiki_page_dict, user_prompt, sys_prompt, mode='fact_checking', data_type='tabular', train_size=200, valid_size=60, data_segmentation=False, debug=False):\n","\n","    # Initialize jsonl object lists for 'JSONL' data type\n","    train_jsonl = []\n","    valid_jsonl = []\n","    # Initialize dataframes for 'tabular' data type\n","    if mode == 'evidence_extraction':\n","        train_df = pd.DataFrame(columns=['claim', 'evidence_sentences', 'full_text', 'label'])\n","        valid_df = pd.DataFrame(columns=['claim', 'evidence_sentences', 'full_text', 'label'])\n","    elif mode == 'fact_checking':\n","        train_df = pd.DataFrame(columns=['claim', 'evidence_sentences', 'label'])\n","        valid_df = pd.DataFrame(columns=['claim', 'evidence_sentences', 'label'])\n","\n","\n","    print(f\"########################## PREPROCESSING STARTED ##########################\")\n","\n","    ####### THE NEXT TWO STEPS ARE FOR CREATING A SET WITH ONLY SUPPORTS AND REFUTES #######\n","    '''\n","    # Remove items where there is not enough info\n","    print(f\"Length of fever_data before filtering out NOT ENOUGH INFO: {len(fever_data)}\")\n","    fever_data = [item for item in fever_data if item['label'] != 'NOT ENOUGH INFO']\n","    print(f\"Length of fever_data after filtering out NOT ENOUGH INFO: {len(fever_data)}\")\n","\n","    # Remove items where there is no evidence\n","    fever_data = [item for item in fever_data if len(item['evidence']) > 0]\n","    print(f\"Length of fever_data after filtering out items with no evidence: {len(fever_data)}\")\n","    print(f\"Number of SUPPORTS in fever_data: {len([item for item in fever_data if item['label'] == 'SUPPORTS'])}\")\n","    print(f\"Number of REFUTES in fever_data: {len([item for item in fever_data if item['label'] == 'REFUTES'])}\")\n","    print('---------------------------------------------')\n","    '''\n","\n","    # Remove items where there is no wiki page associated with the claim\n","    print(f\"Length of fever_data before filtering out items with no wiki page: {len(fever_data)}\")\n","    fever_data = remove_items_with_no_evidence(fever_data)\n","    print(f\"Length of fever_data after filtering out items with no wiki page: {len(fever_data)}\")\n","\n","    # Reorder the data by interspersing SUPPORTS and REFUTES (one item and then the other), and balance the data\n","    fever_data = intersperse_labels(fever_data)\n","\n","    # Check the number of SUPPORTS and REFUTES in the data and reduce the number of NOT ENOUGH INFO items to the same number\n","    max_label_count = min(len([item for item in fever_data if item['label'] == 'SUPPORTS']), len([item for item in fever_data if item['label'] == 'REFUTES']))\n","    # Reduce the number of NOT ENOUGH INFO items to the same number as the max label count\n","    fever_data['NOT ENOUGH INFO'] = fever_data['NOT ENOUGH INFO'][:max_label_count]\n","    print(f\"Number of SUPPORTS in fever_data: {len([item for item in fever_data if item['label'] == 'SUPPORTS'])}\")\n","    print(f\"Number of REFUTES in fever_data: {len([item for item in fever_data if item['label'] == 'REFUTES'])}\")\n","    print(f\"Number of NOT ENOUGH INFO in fever_data: {len([item for item in fever_data if item['label'] == 'NOT ENOUGH INFO'])}\")\n","\n","    print(f\"########################## PREPROCESSING COMPLETE ##########################\")\n","    print('---------------------------------------------')\n","\n","\n","    if data_segmentation and (train_size + valid_size) > (len(fever_data) // 2):\n","        print(f\"Warning: Data segmentation is selected but the sum of train_size and valid_size exceeds half of the fever data size. Adjusting to max size with a 80-20 split.\")\n","        train_size = (len(fever_data) // 2) * 0.8\n","        valid_size = (len(fever_data) // 2) * 0.2\n","    elif not data_segmentation and (train_size + valid_size) > len(fever_data):\n","        print(f\"Warning: Data segmentation is not selected but the sum of train_size and valid_size exceeds the fever data size. Adjusting to max size with a 80-20 split.\")\n","        train_size = len(fever_data) * 0.8\n","        valid_size = len(fever_data) * 0.2\n","\n","    if mode == 'evidence_extraction':\n","        print('############################# START EVIDENCE EXTRACTION ###################')\n","        print(f\"Running conversion for evidence extraction mode...\")\n","        print('---------------------------------------------')\n","\n","        train_run_count = len(os.listdir(f'{fever_path}/GPT_sentEx_paper_dev_train'))\n","        valid_run_count = len(os.listdir(f'{fever_path}/GPT_sentEx_paper_dev_valid'))\n","        print(f\"Number of files in GPT_sentEx_paper_dev_train: {train_run_count}\")\n","        print(f\"Number of files in GPT_sentEx_paper_dev_valid: {valid_run_count}\")\n","        train_run_count = str(train_run_count).zfill(3)\n","        valid_run_count = str(valid_run_count).zfill(3)\n","\n","        # If data segmentation is enabled, use only the top half of the fever data, because the bottom half is used for the fact checking mode (max train + valid size = len(fever_data) // 2)\n","        if data_segmentation:\n","            print(f\"Data segmentation enabled. Using top half of the dataset for evidence extraction.\")\n","            train_data = fever_data[:train_size]\n","            print(f\"Getting data for training from 0 to {train_size}\")\n","            valid_data = fever_data[train_size:train_size + valid_size]\n","            print(f\"Getting data for validation from {train_size} to {train_size + valid_size}\")\n","        # If data segmentation is not enabled, use the whole fever data\n","        else:\n","          # Subset and balance the training data\n","          train_data = fever_data[:train_size]\n","          valid_data = fever_data[train_size:train_size + valid_size]\n","\n","        # Print the number of claims in each subset\n","        print(f\"Number of claims in training data before processing: {len(train_data)}\")\n","        print(f\"Number of claims in validation data before processing: {len(valid_data)}\")\n","\n","        # Process Training Data\n","        skipped_count = 0\n","        for fever_item in train_data:\n","          claim = fever_item['claim']\n","          if debug:\n","            print(f\"Processing claim: {claim}\")\n","          evidence_sentences, full_text = extract_evidence_text_debug(fever_item, wiki_page_dict)\n","\n","          if len(evidence_sentences) > 0:\n","            evidence_sentences = \"\\n\".join(evidence_sentences)\n","            if data_type == 'tabular':\n","              train_df = train_df.append({'claim': claim, 'evidence_sentences': evidence_sentences, 'full_text': full_text, 'label': fever_item['label']}, ignore_index=True)\n","              if debug:\n","                print(f\"DEBUG (TABULAR): Evidence sentences for training data: {evidence_sentences}\")\n","            elif data_type == 'jsonl':\n","              usr_prompt = user_prompt + f\"Claim:\\n{claim}\\n\\nSource Text:\\n{full_text}\"\n","              if debug:\n","                print(f\"DEBUG (JSONL): Evidence sentences for training data: {evidence_sentences}\")\n","                print(f\"DEBUG (JSONL): User prompt for training data: {usr_prompt}\")\n","              completion = evidence_sentences\n","              train_jsonl.append({\"messages\": [{\"role\": \"system\", \"content\": sys_prompt}, {\"role\": \"user\", \"content\": usr_prompt}, {\"role\": \"assistant\", \"content\": completion}]})\n","          else:\n","            skipped_count += 1\n","            print(f\"WARNING: In assembling the training data, no evidence found for claim: {claim}\")\n","            continue\n","        print(f\"Skipped claims in training data due to no evidence: {skipped_count}\")\n","        print('---------------------------------------------')\n","\n","        # Process Validation Data\n","        skipped_count = 0\n","        for fever_item in valid_data:\n","          claim = fever_item['claim']\n","          evidence_sentences, full_text = extract_evidence_text_debug(fever_item, wiki_page_dict)\n","\n","          if len(evidence_sentences) > 0:\n","            evidence_sentences = \"\\n\".join(evidence_sentences)\n","            if data_type == 'tabular':\n","              valid_df = valid_df.append({'claim': claim, 'evidence_sentences': evidence_sentences, 'full_text': full_text, 'label': fever_item['label']}, ignore_index=True)\n","            elif data_type == 'jsonl':\n","              usr_prompt = user_prompt + f\"Claim:\\n{claim}\\n\\nSource Text:\\n{full_text}\"\n","              completion = evidence_sentences\n","              valid_jsonl.append({\"messages\": [{\"role\": \"system\", \"content\": sys_prompt}, {\"role\": \"user\", \"content\": usr_prompt}, {\"role\": \"assistant\", \"content\": completion}]})\n","          else:\n","            skipped_count += 1\n","            print(f\"WARNING: In assembling the validation data, no evidence found for claim: {claim}\")\n","            continue\n","        print(f\"Skipped claims in validation data due to no evidence: {skipped_count}\")\n","\n","        print('---------------------------------------------')\n","        print(f\"Length of the evidence extraction training data: {len(train_jsonl)}\")\n","        print(f\"Number of SUPPORTS in the training data: {len([item for item in train_data if item['label'] == 'SUPPORTS'])}\")\n","        print(f\"Number of REFUTES in the training data: {len([item for item in train_data if item['label'] == 'REFUTES'])}\")\n","        print('---------------------------------------------')\n","        print(f\"Length of the evidence extraction validation data: {len(valid_jsonl)}\")\n","        print(f\"Number of SUPPORTS in the validation data: {len([item for item in valid_data if item['label'] == 'SUPPORTS'])}\")\n","        print(f\"Number of REFUTES in the validation data: {len([item for item in valid_data if item['label'] == 'REFUTES'])}\")\n","        print('---------------------------------------------')\n","\n","        # Save the jsonl object as jsonlines files\n","        with open(f\"{fever_path}tabular_sentEx_paper_dev_train/{output_filepath}{train_size}_{date_fmt}_{train_run_count}.jsonl\", 'w') as f:\n","           for data_item in train_jsonl:\n","                 json.dump(data_item, f)\n","                 f.write('\\n')\n","        with open(f\"{fever_path}tabular_sentEx_paper_dev_valid/{output_filepath}{valid_size}_{date_fmt}_{valid_run_count}.jsonl\", 'w') as f:\n","           for data_item in valid_jsonl:\n","                 json.dump(data_item, f)\n","                 f.write('\\n')\n","        print(f\"FEVER data conversion to OpenAI JSONL completed. Output files:\\n - {fever_path}GPT_sentEx_paper_dev_valid/{output_filepath}{train_size}_{date_fmt}_{train_run_count}.jsonl\\n - {fever_path}GPT_sentEx_paper_dev_valid/{output_filepath}{valid_size}_{date_fmt}_{valid_run_count}.jsonl\")\n","        print('################### END OF CONVERSION FOR EVIDENCE EXTRACTION ###################\\n')\n","\n","    ############### END OF EVIDENCE EXTRACTION MODE ################\n","\n","    ############### START OF FACT CHECKING MODE ###################\n","    elif mode == 'fact_checking':\n","        print('######################### START FACT CHECKING ###########################')\n","        print(f\"Running conversion for fact checking mode...\")\n","        print('---------------------------------------------')\n","\n","        train_run_count = len(os.listdir(f'{fever_path}/GPT_clf_paper_dev_train'))\n","        valid_run_count = len(os.listdir(f'{fever_path}/GPT_clf_paper_dev_valid'))\n","        print(f\"Number of files in GPT_clf_paper_dev_train: {train_run_count}\")\n","        print(f\"Number of files in GPT_clf_paper_dev_valid: {valid_run_count}\")\n","        train_run_count = str(train_run_count).zfill(3)\n","        valid_run_count = str(valid_run_count).zfill(3)\n","\n","        # If data segmentation is enabled, use only the bottom half of the fever data, because the top half is used for the evidence extraction mode (max train + valid size = len(fever_data) // 2)\n","        if data_segmentation:\n","            print(f\"Data segmentation enabled. Using bottom half of the dataset for fact checking.\")\n","            train_data = fever_data[len(fever_data) // 2:(len(fever_data) // 2) + train_size]\n","            print(f\"Getting data for training from {len(fever_data) // 2} to {len(fever_data) // 2 + train_size}\")\n","            valid_data = fever_data[(len(fever_data) // 2) + train_size:len(fever_data) // 2 + train_size + valid_size]\n","            print(f\"Getting data for validation from {len(fever_data) // 2 + train_size} to {len(fever_data) // 2 + train_size + valid_size}\")\n","        # If data segmentation is not enabled, use the whole fever data\n","        else:\n","          # Subset and balance the training data\n","          train_data = fever_data[:train_size]\n","          valid_data = fever_data[train_size:train_size + valid_size]\n","\n","        # Print the number of claims in each subset\n","        print(f\"Number of claims in training data before processing: {len(train_data)}\")\n","        print(f\"Number of claims in validation data before processing: {len(valid_data)}\")\n","\n","        # Process Training data\n","        skipped_count = 0\n","        for fever_item in train_data:\n","          claim = fever_item['claim']\n","          if debug:\n","            print(f\"Processing claim: {claim}\")\n","          fever_label = fever_item['label']\n","          mapped_label = fever_label_mapping.get(fever_label)\n","          evidence_sentences, full_text = extract_evidence_text_debug(fever_item, wiki_page_dict)\n","\n","          if mapped_label:\n","            if len(evidence_sentences) > 0:\n","              evidence_sentences = \"\\n\".join(evidence_sentences) # Join the sentences\n","              if data_type == 'tabular':\n","                train_df = train_df.append({'claim': claim, 'evidence_sentences': evidence_sentences, 'label': fever_item['label']}, ignore_index=True)\n","                if debug:\n","                  print(f\"DEBUG (TABULAR): Evidence sentences for training data: {evidence_sentences}\")\n","              elif data_type == 'jsonl':\n","                usr_prompt = user_prompt + f\"Claim:\\n{claim}\\n\\nEvidence sentences:\\n{evidence_sentences}\"\n","                if debug:\n","                  print(f\"Evidence sentences: {evidence_sentences}\")\n","                  print(f\"User prompt: {usr_prompt}\")\n","                completion = mapped_label\n","                train_jsonl.append({\"messages\": [{\"role\": \"system\", \"content\": sys_prompt}, {\"role\": \"user\", \"content\": usr_prompt}, {\"role\": \"assistant\", \"content\": completion}]})\n","            else:\n","              skipped_count += 1\n","              continue\n","        print(f\"Skipped claims in training data due to no evidence: {skipped_count}\")\n","        print('----------------------------------------------')\n","\n","        # Process Validation Data\n","        skipped_count = 0\n","        for fever_item in valid_data:\n","          claim = fever_item['claim']\n","          fever_label = fever_item['label']\n","          mapped_label = fever_label_mapping.get(fever_label)\n","          evidence_sentences, full_text = extract_evidence_text_debug(fever_item, wiki_page_dict)\n","\n","          if mapped_label:\n","            if len(evidence_sentences) > 0:\n","              evidence_sentences = \"\\n\".join(evidence_sentences) # Join the sentences\n","              if data_type == 'tabular':\n","                valid_df = valid_df.append({'claim': claim, 'evidence_sentences': evidence_sentences, 'label': fever_item['label']}, ignore_index=True)\n","              elif data_type == 'jsonl':\n","                usr_prompt = user_prompt + f\"Claim:\\n{claim}\\n\\nEvidence:\\n{evidence_sentences}\"\n","                completion = mapped_label\n","                valid_jsonl.append({\"messages\": [{\"role\": \"system\", \"content\": sys_prompt}, {\"role\": \"user\", \"content\": usr_prompt}, {\"role\": \"assistant\", \"content\": completion}]})\n","            else:\n","                skipped_count += 1\n","                continue\n","        print(f\"Skipped claims in validation data due to no evidence: {skipped_count}\")\n","\n","        print('----------------------------------------------')\n","        print(f\"Length of the fact checking training data: {len(train_jsonl)}\")\n","        print(f\"Number of SUPPORTS in the training data: {len([item for item in train_data if item['label'] == 'SUPPORTS'])}\")\n","        print(f\"Number of REFUTES in the training data: {len([item for item in train_data if item['label'] == 'REFUTES'])}\")\n","        print('----------------------------------------------')\n","        print(f\"Length of the fact checking validation data: {len(valid_jsonl)}\")\n","        print(f\"Number of SUPPORTS in the validation data: {len([item for item in valid_data if item['label'] == 'SUPPORTS'])}\")\n","        print(f\"Number of REFUTES in the validation data: {len([item for item in valid_data if item['label'] == 'REFUTES'])}\")\n","        print('----------------------------------------------')\n","\n","        with open(f\"{fever_path}tabular_clf_paper_dev_train/{output_filepath}{train_size}_{date_fmt}_{train_run_count}.jsonl\", 'w') as f:\n","           for data_item in train_jsonl:\n","                 json.dump(data_item, f)\n","                 f.write('\\n')\n","        with open(f\"{fever_path}tabular_clf_paper_dev_valid/{output_filepath}{valid_size}_{date_fmt}_{valid_run_count}.jsonl\", 'w') as f:\n","           for data_item in valid_jsonl:\n","                 json.dump(data_item, f)\n","                 f.write('\\n')\n","        print(f\"FEVER data conversion to OpenAI JSONL completed. Output files:\\n - {fever_path}GPT_clf_paper_dev_train/{output_filepath}{train_size}_{date_fmt}_{train_run_count}.jsonl\\n - {fever_path}GPT_clf_paper_dev_valid/{output_filepath}{valid_size}_{date_fmt}_{valid_run_count}.jsonl\")\n","        print('################### END OF CONVERSION FOR FACT CHECKING ###################\\n')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SmDC9BFfce63","outputId":"aaaa8deb-c5cb-4b6f-a924-2a1cbcb95929"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["########################## PREPROCESSING STARTED ##########################\n","Length of fever_data before filtering out items with no wiki page: 9999\n",">>>>>>>>>>> STARTING REMOVAL OF ITEMS WITH NO EVIDENCE <<<<<<<<<<<\n"]}],"source":["fever_train_filepath = fever_path + 'paper_dev.jsonl'\n","fever_data = load_jsonl(fever_train_filepath, 'utf-8')\n","\n","########## STUDY VARIABLES – SYSTEM AND USER PROMPTS AND TRAINING AND VALIDATION SET SIZES #########\n","sentEx_sys_prompt = \"\"\n","sentEx_user_prompt = f\"Extract sentences from the source text that are relevant (either supporting or refuting) to the preceding claim. Return a comma separated list of sentences.\\n\\n\"\n","clf_sys_prompt = \"\"\n","clf_user_prompt = \"Classify the truthfulness of the text out of the following categories: '1' (if the claim is true), '0' (if the claim is false), given the evidence. Do not use any other labels.\\n\\n\"\n","train_n = 100\n","valid_n = 30\n","####################################################################################################\n","\n","output_filepath = 'v1_segmented_n' # Subfolder + filename prefix (e.g., 'GPT_clf_paper_dev_train/prompt_v1_segmented_n'). If no subfolder is specified files will be saved to FEVER parent folder.\n","\n","convert_fever_to_fine_tuning(fever_data, output_filepath, wiki_page_list_dicts, sentEx_user_prompt, sentEx_sys_prompt, 'evidence_extraction', train_size=train_n, valid_size=valid_n, data_segmentation=True)\n","convert_fever_to_fine_tuning(fever_data, output_filepath, wiki_page_list_dicts, clf_user_prompt, clf_sys_prompt, 'fact_checking', train_size=train_n, valid_size=valid_n, data_segmentation=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1442,"status":"ok","timestamp":1743029634174,"user":{"displayName":"Henry Zelenak","userId":"01809909909045225068"},"user_tz":240},"id":"3HlGQjcH6Guz","outputId":"6ff82851-86ef-440d-bc11-1ec09418dd62"},"outputs":[{"name":"stdout","output_type":"stream","text":["Classifiction training data:\n","messages: [{'role': 'system', 'content': ''}, {'role': 'user', 'content': \"Classify the truthfulness of the text out of the following categories: '1' (if the claim is true), '0' (if the claim is false), given the evidence. Do not use any other labels.\\n\\nClaim: Riddick is in a science fiction film.\\n\\nEvidence:\\nRiddick is a Furyan , a member of a warrior race obliterated by a military campaign that left Furya desolate , and is one of the last of his kind .\\nRiddick was once a mercenary , then part of a security force , and later a soldier .\\nActor Vin Diesel has played the title role in all of the Riddick-based films and video games so far .\\nRichard B. Riddick , more commonly known as Riddick , is a fictional character and the antihero of four films in the Riddick series -LRB- Pitch Black , The Chronicles of Riddick , the animated movie The Chronicles of Riddick : Dark Fury , and Riddick -RRB- , as well as the two video games The Chronicles of Riddick : Escape from Butcher Bay and The Chronicles of Riddick : Assault on Dark Athena .\\nWithin the canon of the series , Riddick is shown to be a highly skilled predator -- he is extremely mobile and stealthy - especially for someone of his size , has a vast knowledge of how to kill almost any humanoid in a variety of ways , is an extreme survivalist , and is notoriously hard to contain .\\nOne of his most defining features are his eyes , a characteristic inherent in a certain caste of his species -LRB- the Alpha-Furyans -RRB- , although he implies in Pitch Black that they were `` shined '' by a back-alley surgical operation .\\nPitch Black -LRB- titled The Chronicles of Riddick : Pitch Black on its DVD re-release -RRB- is a 2000 American science fiction action horror film co-written and directed by David Twohy .\\nRiddick is a 2013 American science fiction thriller film , the third installment in the Riddick film series .\"}, {'role': 'assistant', 'content': '1'}]\n","messages: [{'role': 'system', 'content': ''}, {'role': 'user', 'content': \"Classify the truthfulness of the text out of the following categories: '1' (if the claim is true), '0' (if the claim is false), given the evidence. Do not use any other labels.\\n\\nClaim: Season 2 of Fargo takes place in 1982.\\n\\nEvidence:\\nA prequel to the events in its first season , season two of Fargo takes place in the Midwestern United States in March 1979 .\"}, {'role': 'assistant', 'content': '0'}]\n","messages: [{'role': 'system', 'content': ''}, {'role': 'user', 'content': \"Classify the truthfulness of the text out of the following categories: '1' (if the claim is true), '0' (if the claim is false), given the evidence. Do not use any other labels.\\n\\nClaim: Tremont Street Subway is a tunnel.\\n\\nEvidence:\\nThe Tremont Street Subway in Boston 's MBTA Subway system is the oldest subway tunnel in North America and the third oldest worldwide to exclusively use electric traction -LRB- after the City and South London Railway in 1890 , and the Budapest Metro 's Line 1 in 1896 -RRB- , opening on September 1 , 1897 .\\nThe tunnel originally served five closely spaced stations : Boylston , Park Street , Scollay Square , Adams Square , and Haymarket , with branches to the Public Garden Portal and Pleasant Street Incline south of Boylston .\"}, {'role': 'assistant', 'content': '1'}]\n","messages: [{'role': 'system', 'content': ''}, {'role': 'user', 'content': \"Classify the truthfulness of the text out of the following categories: '1' (if the claim is true), '0' (if the claim is false), given the evidence. Do not use any other labels.\\n\\nClaim: Albert S. Ruddy is born on March 29, 1930.\\n\\nEvidence:\\nAlbert S. Ruddy -LRB- born March 28 , 1930 -RRB- is a Canadian-born film and television producer .\"}, {'role': 'assistant', 'content': '0'}]\n","messages: [{'role': 'system', 'content': ''}, {'role': 'user', 'content': \"Classify the truthfulness of the text out of the following categories: '1' (if the claim is true), '0' (if the claim is false), given the evidence. Do not use any other labels.\\n\\nClaim: Eddie Guerrero had problems with substance abuse.\\n\\nEvidence:\\nHe experienced various substance abuse problems , including alcoholism and an addiction to painkillers ; these real-life issues were sometimes incorporated into his storylines .\"}, {'role': 'assistant', 'content': '1'}]\n","-------------------------------------------------------------------------------------------\n","Classifiction validation data:\n","messages: [{'role': 'system', 'content': ''}, {'role': 'user', 'content': \"Classify the truthfulness of the text out of the following categories: '1' (if the claim is true), '0' (if the claim is false), given the evidence. Do not use any other labels.\\n\\nClaim: Baz Luhrmann's film Australia stars at least one person.\\n\\nEvidence:\\nHugh Michael Jackman -LRB- born 12 October 1968 -RRB- is an Australian actor , singer , and producer .\\nNicole Mary Kidman , AC -LRB- born June 20 , 1967 -RRB- is an Australian actress and film producer .\\nAustralia is a 2008 Australian-American-British romantic historical adventure drama film directed by Baz Luhrmann and starring Nicole Kidman and Hugh Jackman .\\nHis 2008 film Australia is an epic historical romantic drama film starring Hugh Jackman and Nicole Kidman .\"}, {'role': 'assistant', 'content': '1'}]\n","messages: [{'role': 'system', 'content': ''}, {'role': 'user', 'content': \"Classify the truthfulness of the text out of the following categories: '1' (if the claim is true), '0' (if the claim is false), given the evidence. Do not use any other labels.\\n\\nClaim: Brubaker is a 1950 film.\\n\\nEvidence:\\nBrubaker is a 1980 American prison drama film directed by Stuart Rosenberg .\"}, {'role': 'assistant', 'content': '0'}]\n","messages: [{'role': 'system', 'content': ''}, {'role': 'user', 'content': \"Classify the truthfulness of the text out of the following categories: '1' (if the claim is true), '0' (if the claim is false), given the evidence. Do not use any other labels.\\n\\nClaim: Margaret Thatcher implemented policies.\\n\\nEvidence:\\nAs Prime Minister , she implemented policies that have come to be known as Thatcherism .\"}, {'role': 'assistant', 'content': '1'}]\n","messages: [{'role': 'system', 'content': ''}, {'role': 'user', 'content': \"Classify the truthfulness of the text out of the following categories: '1' (if the claim is true), '0' (if the claim is false), given the evidence. Do not use any other labels.\\n\\nClaim: Glee.com was first launched by Walmart in the spring of 2002.\\n\\nEvidence:\\nThe site was launched in February 2007 by Community Connect Inc. .\"}, {'role': 'assistant', 'content': '0'}]\n","messages: [{'role': 'system', 'content': ''}, {'role': 'user', 'content': \"Classify the truthfulness of the text out of the following categories: '1' (if the claim is true), '0' (if the claim is false), given the evidence. Do not use any other labels.\\n\\nClaim: Aristotle tutored Alexander the Great.\\n\\nEvidence:\\nShortly after Plato died , Aristotle left Athens and , at the request of Philip II of Macedon , tutored Alexander the Great beginning in 343 BC .\"}, {'role': 'assistant', 'content': '1'}]\n","-------------------------------------------------------------------------------------------\n","Evidence extraction training data:\n","messages: [{'role': 'system', 'content': ''}, {'role': 'user', 'content': \"Extract sentences from the source text that are relevant (either supporting or refuting) to the preceding claim. Return a comma separated list of sentences.\\n\\nClaim: Fox 2000 Pictures released the film Soul Food.\\n\\nSource Text: Soul Food is a 1997 American comedy-drama film produced by Kenneth `` Babyface '' Edmonds , Tracey Edmonds and Robert Teitel and released by Fox 2000 Pictures . Featuring an ensemble cast , the film stars Vanessa L. Williams , Vivica A. Fox , Nia Long , Michael Beach , Mekhi Phifer , Jeffrey D. Sams , Irma P. Hall , Gina Ravera and Brandon Hammond . Written and directed by George Tillman , Jr. -- in his major studio debut -- the film centers on the trials of an extended African-American family , held together by longstanding family traditions which begin to fade as serious problems take center stage .   Tillman based the family in the film on his own and Soul Food was widely acclaimed for presenting a more positive image of African-Americans than is typically seen in Hollywood films . In 2000 , Showtime premiered a one-hour television series based upon the film . In 2015 , it was announced that 20th Century Fox is planning a sequel for film called More Soul Food , written by Tillman , Jr. . \"}, {'role': 'assistant', 'content': \"Soul Food is a 1997 American comedy-drama film produced by Kenneth `` Babyface '' Edmonds , Tracey Edmonds and Robert Teitel and released by Fox 2000 Pictures .\"}]\n","messages: [{'role': 'system', 'content': ''}, {'role': 'user', 'content': \"Extract sentences from the source text that are relevant (either supporting or refuting) to the preceding claim. Return a comma separated list of sentences.\\n\\nClaim: Telemundo is a English-language television network.\\n\\nSource Text: Telemundo -LRB- -LSB- teleˈmundo -RSB- -RRB- is an American Spanish-language terrestrial television network owned by Comcast through the NBCUniversal division NBCUniversal Telemundo Enterprises . It is the second largest provider of Spanish content nationwide behind American competitor Univision , with programming syndicated worldwide to more than 100 countries in over 35 languages .   The channel broadcasts programs and original content aimed at Hispanic and Latino American audiences in the United States and worldwide , consisting of telenovelas , sports , reality television , news programming , and films -- either imported or Spanish-dubbed . In addition , Telemundo operates NBC Universo , a separate channel directed towards young Hispanic audiences ; Telemundo Digital Media , which distributes original programming content across mass media , the Telemundo and NBC Universo websites ; Puerto Rico telestation WKAQ-TV ; and international distribution arm Telemundo Internacional .   Telemundo is headquartered in the Miami suburb of Hialeah , Florida , and has 1,900 employees worldwide . The majority of Telemundo 's programs are filmed at an operated studio facility in Miami , where 85 % of the network 's telenovelas were filmed during 2011 . The average hourly primetime drama costs $ 70K to produce . \"}, {'role': 'assistant', 'content': 'It is the second largest provider of Spanish content nationwide behind American competitor Univision , with programming syndicated worldwide to more than 100 countries in over 35 languages .\\nHispanic Americans and Latino Americans -LRB- hispanos -LSB- isˈpanos -RSB- -RRB- are American descendants from Spain and the Spanish speaking countries of Latin America .\\nIn addition , Telemundo operates NBC Universo , a separate channel directed towards young Hispanic audiences ; Telemundo Digital Media , which distributes original programming content across mass media , the Telemundo and NBC Universo websites ; Puerto Rico telestation WKAQ-TV ; and international distribution arm Telemundo Internacional .\\nTelemundo -LRB- -LSB- teleˈmundo -RSB- -RRB- is an American Spanish-language terrestrial television network owned by Comcast through the NBCUniversal division NBCUniversal Telemundo Enterprises .\\nThe channel broadcasts programs and original content aimed at Hispanic and Latino American audiences in the United States and worldwide , consisting of telenovelas , sports , reality television , news programming , and films -- either imported or Spanish-dubbed .'}]\n","messages: [{'role': 'system', 'content': ''}, {'role': 'user', 'content': \"Extract sentences from the source text that are relevant (either supporting or refuting) to the preceding claim. Return a comma separated list of sentences.\\n\\nClaim: There is a capital called Mogadishu.\\n\\nSource Text: Mogadishu -LRB- -LSB- ˌmɔːɡəˈdiːʃuː -RSB- Muqdisho -LSB- mʉqdɪʃɔ -RSB- ; مقديشو -LSB- maqadiːʃuː -RSB- -RRB- , known locally as Hamar , is the capital and most populous city of Somalia . Located in the coastal Banaadir region on the Indian Ocean , the city has served as an important port for millennia . , it has a population of 2,425,000 residents .   Tradition and old records assert that southern Somalia , including the Mogadishu area , was historically inhabited by hunter-gatherers . These were later joined by Cushitic-speaking agro-pastoralists , who would go on to establish local aristocracies . During its medieval Golden Age , Mogadishu was ruled by the Muzaffar dynasty , a vassal of the Ajuran Sultanate . It subsequently fell under the control of an assortment of local Sultanates and polities , most notably the Sultanate of the Geledi . The city later became the capital of Italian Somaliland -LRB- 1889 -- 1936 -RRB- in the colonial period . After the Somali Republic became independent in 1960 , Mogadishu became known and promoted as the White Pearl of the Indian Ocean . After the ousting of the Siad Barre regime in 1991 and the ensuing Somali Civil War , various militias fought for control of the city , later to be replaced by the Islamic Courts Union in the mid-2000s . The ICU thereafter splintered into more radical groups , notably al-Shabaab , which fought the Transitional Federal Government -LRB- 2004 -- 2012 -RRB- and its African Union Mission to Somalia allies . With a change in administration in late 2010 , government troops and their military partners had succeeded in forcing out Al-Shabaab by August 2011 . Mogadishu has subsequently experienced a period of intense reconstruction .   As Somalia 's capital city , many important national institutions are based in Mogadishu . It is the seat of the Federal Government of Somalia established in August 2012 , with the Somalia Federal Parliament serving as the government 's legislative branch . Thabit Abdi Mohammed has been the Mayor of Mogadishu since April 2017 . Villa Somalia is the official residential palace and principal workplace of the President of Somalia , Mohamed Abdullahi Mohamed . In May 2012 , the First Somali Bank was established in the capital , which organized Mogadishu 's first ever Technology , Entertainment , Design -LRB- TEDx -RRB- conference . The establishment of a local construction yard has also galvanized the city 's real-estate sector . Arba'a Rukun Mosque is one of the oldest Islamic places of worship in the capital , built circa 667 -LRB- 1268/9 AD -RRB- . The Mosque of Islamic Solidarity in Mogadishu is the largest masjid in the Horn region . Mogadishu Cathedral was built in 1928 by the colonial authorities in Italian Somaliland in a Norman Gothic style , and served as the traditional seat of the Roman Catholic Diocese of Mogadiscio . The National Museum of Somalia is based in Mogadishu and holds many culturally important artefacts . The National Library of Somalia is undergoing a $ 1 million Somali federal government funded renovation , including a new library complex .   Mogadishu is home to a number of scholastic and media institutions . As part of the municipality 's urban renewal program , 100 schools across the capital are scheduled to be refurbished and reopened . The Somali National University -LRB- SNU -RRB- was established in the 1950s , and professors from the university later founded the non-governmental Mogadishu University -LRB- MU -RRB- . Benadir University -LRB- BU -RRB- was established in 2002 with the intention of training doctors . Various national sporting bodies have their headquarters in Mogadishu , including the Somali Football Federation and the Somali Olympic Committee . Mogadishu Stadium was constructed in 1978 during the Siad Barre administration , with the assistance of Chinese engineers . It hosts football matches with teams from the Somali First Division and the Somalia Cup . Additionally , the Port of Mogadishu serves as a major national seaport and is the largest harbour in Somalia . Mogadishu International Airport , the capital 's main airport , is the hub of the national carrier Somali Airlines . \"}, {'role': 'assistant', 'content': 'Mogadishu -LRB- -LSB- ˌmɔːɡəˈdiːʃuː -RSB- Muqdisho -LSB- mʉqdɪʃɔ -RSB- ; مقديشو -LSB- maqadiːʃuː -RSB- -RRB- , known locally as Hamar , is the capital and most populous city of Somalia .'}]\n","messages: [{'role': 'system', 'content': ''}, {'role': 'user', 'content': \"Extract sentences from the source text that are relevant (either supporting or refuting) to the preceding claim. Return a comma separated list of sentences.\\n\\nClaim: Damon Albarn's debut album was released in 2011.\\n\\nSource Text: Damon Albarn , OBE -LRB- -LSB- ˈdeɪmən_ˈælbɑrn -RSB- born 23 March 1968 -RRB- is an English musician , singer , songwriter , multi-instrumentalist and record producer . He is the lead singer of the British rock band Blur and co-founder , vocalist , instrumentalist and principal songwriter of the virtual band Gorillaz .   Raised in Leytonstone , East London and around Colchester , Essex , Albarn attended the Stanway School , where he met Graham Coxon and eventually formed Blur , whose debut album Leisure was released in 1991 to mixed reviews . After spending long periods of time touring the US , Albarn 's songwriting became increasingly influenced by British bands from the 1960s . The result of these influences came in the form of Modern Life Is Rubbish -LRB- 1993 -RRB- , Parklife -LRB- 1994 -RRB- and The Great Escape -LRB- 1995 -RRB- . All three albums received critical acclaim while Blur gained mass popularity in the UK , aided by a Britpop rivalry with Oasis . Subsequent albums such as Blur -LRB- 1997 -RRB- , 13 -LRB- 1999 -RRB- , Think Tank -LRB- 2003 -RRB- and The Magic Whip -LRB- 2015 -RRB- contained influences from lo-fi , electronic and hip hop music .   Along with Tank Girl creator Jamie Hewlett , Albarn formed the `` virtual band '' Gorillaz in 1998 . Drawing influences from alternative rock , trip hop , hip hop , electronica , dub , reggae and pop music , the band released their self-titled debut album in 2001 to worldwide success . Although Albarn is the only permanent musical contributor , the albums feature collaborations from a wide range of artists . Gorillaz are cited by the Guinness Book of World Records as the `` Most Successful Virtual Band '' . Other projects include working with African musicians in aid of the charity Oxfam , writing and performing lead vocals on The Good , the Bad & the Queen as part of an unnamed supergroup and composing film soundtracks . He has also ventured into the world of opera with Dr Dee and Monkey : Journey to the West . His debut solo studio album Everyday Robots -- co-produced by XL Recordings CEO Richard Russell -- was released on 28 April 2014 and featured collaborations with Brian Eno , Natasha Khan and the Leytonstone City Pentecostal Mission Church Choir as well as sampling several rants by Lord Buckley .   In 2008 , The Daily Telegraph ranked Albarn number 18 in their list of the `` 100 most powerful people in British culture '' . In a 2010 UK poll for Q magazine Albarn was voted the fourth-greatest frontman of all time . He was appointed Officer of the Order of the British Empire -LRB- OBE -RRB- in the 2016 New Year Honours for services to music . \"}, {'role': 'assistant', 'content': 'His debut solo studio album Everyday Robots -- co-produced by XL Recordings CEO Richard Russell -- was released on 28 April 2014 and featured collaborations with Brian Eno , Natasha Khan and the Leytonstone City Pentecostal Mission Church Choir as well as sampling several rants by Lord Buckley .'}]\n","messages: [{'role': 'system', 'content': ''}, {'role': 'user', 'content': 'Extract sentences from the source text that are relevant (either supporting or refuting) to the preceding claim. Return a comma separated list of sentences.\\n\\nClaim: The Cretaceous ended.\\n\\nSource Text: The Cretaceous -LRB- -LSB- pronkrᵻˈteɪʃəs -RSB- , -RRB- is a geologic period and system that spans 79 million years from the end of the Jurassic Period million years ago -LRB- Mya -RRB- to the beginning of the Paleogene Period Mya . It is the last period of the Mesozoic Era . The Cretaceous Period is usually abbreviated K , for its German translation Kreide -LRB- chalk -RRB- .   The Cretaceous was a period with a relatively warm climate , resulting in high eustatic sea levels that created numerous shallow inland seas . These oceans and seas were populated with now-extinct marine reptiles , ammonites and rudists , while dinosaurs continued to dominate on land . During this time , new groups of mammals and birds , as well as flowering plants , appeared . The Cretaceous ended with a large mass extinction , the Cretaceous -- Paleogene extinction event , in which many groups , including non-avian dinosaurs , pterosaurs and large marine reptiles died out . The end of the Cretaceous is defined by the abrupt Cretaceous -- Paleogene boundary -LRB- K -- Pg boundary -RRB- , a geologic signature associated with the mass extinction which lies between the Mesozoic and Cenozoic eras . '}, {'role': 'assistant', 'content': 'The end of the Cretaceous is defined by the abrupt Cretaceous -- Paleogene boundary -LRB- K -- Pg boundary -RRB- , a geologic signature associated with the mass extinction which lies between the Mesozoic and Cenozoic eras .\\nThe Cretaceous ended with a large mass extinction , the Cretaceous -- Paleogene extinction event , in which many groups , including non-avian dinosaurs , pterosaurs and large marine reptiles died out .'}]\n","-------------------------------------------------------------------------------------------\n","Evidence extraction validation data:\n","messages: [{'role': 'system', 'content': ''}, {'role': 'user', 'content': \"Extract sentences from the source text that are relevant (either supporting or refuting) to the preceding claim. Return a comma separated list of sentences.\\n\\nClaim: Roar (song) is on the 2013 Katy Perry album Prism.\\n\\nSource Text: `` Roar '' is a song by American singer Katy Perry for her fourth studio album , Prism -LRB- 2013 -RRB- . It was released as the lead single from the record on August 10 , 2013 . Perry co-wrote the song with Bonnie McKee and its producers Dr. Luke , Max Martin , and Cirkut . It is a pop song containing elements of arena rock and lyrics centering on standing up for oneself and self-empowerment .   Some critics praised the track 's production while others felt that its lyrics contained `` clichés '' . The song was a commercial success , becoming Perry 's eighth non-consecutive number one song on the U.S. Billboard Hot 100 , and also topping charts in Australia , Canada , Ireland , New Zealand , and the United Kingdom . It also reached the top five in most international charts , including France , Germany , Italy , Japan , and Switzerland .   To promote the song , Perry performed under the Brooklyn Bridge at the closing ceremony of the 2013 MTV Video Music Awards , on The X Factor Australia , at the Sydney Opera House in late October 2013 , and on the German TV show Schlag den Raab . Grady Hall and Mark Kudsi directed the song 's music video , which features Perry trying to adapt to the jungle where she survived a plane crash , and taming a tiger . It garnered generally mixed reviews from music critics . `` Roar '' was nominated for Song of the Year and Best Pop Solo Performance at the 56th Annual Grammy Awards . The song topped the charts in 14 countries and , by the end of 2013 , sold 9.9 million units -LRB- combined sales and track-equivalent streams -RRB- globally according to the IFPI . `` Roar '' has sold 6.4 million copies in the US , over 1 million in the UK . \"}, {'role': 'assistant', 'content': \"`` Roar '' is a song by American singer Katy Perry for her fourth studio album , Prism -LRB- 2013 -RRB- .\"}]\n","messages: [{'role': 'system', 'content': ''}, {'role': 'user', 'content': \"Extract sentences from the source text that are relevant (either supporting or refuting) to the preceding claim. Return a comma separated list of sentences.\\n\\nClaim: The Others (2001 film) lost Best Film.\\n\\nSource Text: The Others -LRB- Los Otros -RRB- is a 2001 Spanish-American supernatural gothic horror film with elements of psychological horror . It was written , directed , and scored by Alejandro Amenábar . It stars Nicole Kidman and Fionnula Flanagan .   The film won eight Goya Awards , including awards for Best Film and Best Director . This was the first English-language film ever to receive the Best Film Award at the Goyas -LRB- Spain 's national film awards -RRB- , without a single word of Spanish spoken in it . The Others was nominated for six Saturn Awards including Best Director and Best Writing for Amenábar and Best Performance by a Younger Actor for Alakina Mann , and won three : Best Horror Film , Best Actress for Kidman and Best Supporting Actress for Fionnula Flanagan . Kidman was also nominated for a Golden Globe Award for Best Actress in Drama and a BAFTA Award for Best Actress in a Leading Role , with Amenábar being nominated for a BAFTA Award for Best Original Screenplay , a rare occurrence for a horror film . \"}, {'role': 'assistant', 'content': 'The film won eight Goya Awards , including awards for Best Film and Best Director .'}]\n","messages: [{'role': 'system', 'content': ''}, {'role': 'user', 'content': \"Extract sentences from the source text that are relevant (either supporting or refuting) to the preceding claim. Return a comma separated list of sentences.\\n\\nClaim: The human brain contains a hypothalamus.\\n\\nSource Text: The human brain is the central organ of the human nervous system . The human brain , with the spinal cord , makes up the central nervous system . The brain consists of the cerebrum , the brainstem and the cerebellum . The brain is the organ that controls most of the activities of the body . The brain processes , integrates , and coordinates all of the information it receives from the sense organs . Sensory information is interpreted and analysed , and decisions are made as to the instructions transmitted to the rest of the body . The brain is contained in , and protected by , the skull bones of the head .   The cerebrum is the largest part of the human brain . It is divided into two cerebral hemispheres which are covered by the cerebral cortex . The cortex is an outer layer of grey matter , that covers the core of white matter . The cortex is split into the neocortex and the much smaller allocortex . The neocortex is made up of six neuronal layers , and the allocortex has three or four such layers . Each hemisphere is conventionally divided into four lobes -- the frontal , temporal , parietal , and occipital lobes . The frontal lobe is associated with executive functions including self-control , planning , reasoning , and abstract thought , while the occipital lobe is dedicated to vision . Within each lobe , there are also cortical areas associated with specific functions , such as the sensory , a motor and association regions . Although the left and right hemispheres are broadly similar in shape and function , some functions are associated with a particular side of the brain , such as language in the left and visual-spatial ability in the right . The hemispheres are connected by nerve tracts known as commisures , the largest being the corpus callosum .   The cerebrum is connected by the brainstem to the spinal cord . The brainstem consists of the midbrain , the pons , and the medulla oblongata . The cerebellum is connected to the brainstem by pairs of tracts known as peduncles . Within the cerebrum is the ventricular system of the brain , which consists of four interconnected ventricles in which cerebrospinal fluid is produced and circulated . Underneath the cerebral cortex , several important structures are located , including the thalamus , the epithalamus , the pineal gland , the hypothalamus , the pituitary gland , and the subthalamus ; the limbic structures , including the amygdala and the hippocampus ; the claustrum , the various nuclei of the basal ganglia ; the basal forebrain structures , and the three circumventricular organs .   The cells of the brain include neurons and supportive glial cells . There are more than 86 billion neurons in the brain and a more or less equal number of other cells . Brain activity is made possible by the interconnections of neurons that are linked together to reach their targets . These connections form various neural networks of neural pathways and circuits . The whole circuitry of the brain is driven by the process of neurotransmission .   The brain is protected by the skull , suspended in cerebrospinal fluid , and isolated from the bloodstream by the blood -- brain barrier . However , the brain is still susceptible to damage , disease , and infection . Damage can be caused by trauma , or a loss of blood supply known as a stroke . The brain is also susceptible to degenerative disorders , such as Parkinson 's disease , forms of dementia including Alzheimer 's disease , and multiple sclerosis . A number of psychiatric conditions , including schizophrenia and clinical depression , are thought to be associated with brain dysfunctions , although the nature of these is not well understood . The brain can also be the site of tumours , both benign and malignant . Malignant tumours mostly originate from sites outside the brain .   The study of the anatomy of the brain is neuroanatomy , while the study of its function is neuroscience . A number of different techniques are used to study the brain . Brain specimens from other animals , which may be examined microscopically , have been a traditional source of much information . Medical imaging technologies such as functional neuroimaging and electroencephalography -LRB- EEG -RRB- recordings are important techniques in studying the brain . Examining the medical history of people with brain injury has also provided great insight into the function of each part of the brain . \"}, {'role': 'assistant', 'content': 'Underneath the cerebral cortex , several important structures are located , including the thalamus , the epithalamus , the pineal gland , the hypothalamus , the pituitary gland , and the subthalamus ; the limbic structures , including the amygdala and the hippocampus ; the claustrum , the various nuclei of the basal ganglia ; the basal forebrain structures , and the three circumventricular organs .'}]\n","messages: [{'role': 'system', 'content': ''}, {'role': 'user', 'content': \"Extract sentences from the source text that are relevant (either supporting or refuting) to the preceding claim. Return a comma separated list of sentences.\\n\\nClaim: The 14th Dalai Lama lives in Japan exclusively.\\n\\nSource Text: The 14th Dalai Lama -LRB- -LSB- ˈdɑːlaɪ_ˈlɑːmə -RSB- -LRB- US -RRB- , -LSB- ˌdælaɪ_ˈlɑːmə -RSB- -LRB- UK -RRB- -LRB- religious name : Tenzin Gyatso , shortened from Jetsun Jamphel Ngawang Lobsang Yeshe Tenzin Gyatso , born Lhamo Thondup , 6 July 1935 -RRB- is the current Dalai Lama . Dalai Lamas are important monks of the Gelug school , the newest school of Tibetan Buddhism which is nominally headed by the Ganden Tripas . From the time of the 5th Dalai Lama to 1959 , the central government of Tibet , the Ganden Phodrang , invested the position of Dalai Lama with temporal duties .   The 14th Dalai Lama was born in Taktser village , Amdo , Tibet -LRB- was administratively in Qinghai province , Republic of China -RRB- , and was selected as the tulku of the 13th Dalai Lama in 1937 and formally recognized as the 14th Dalai Lama at a public declaration near the town of Bumchen in 1939 . His enthronement ceremony as the Dalai Lama was held in Lhasa on 22 February 1940 , and he eventually assumed full temporal -LRB- political -RRB- duties on 17 November 1950 , at the age of 15 , after the People 's Republic of China 's invasion of Tibet . The Gelug school 's government administered an area roughly corresponding to the Tibet Autonomous Region just as the nascent PRC wished to assert central control over it .   During the 1959 Tibetan uprising , the Dalai Lama fled to India , where he currently lives as a refugee . The 14th Dalai Lama received the Nobel Peace Prize in 1989 . He has traveled the world and has spoken about the welfare of Tibetans , environment , economics , women 's rights , non-violence , interfaith dialogue , physics , astronomy , Buddhism and science , cognitive neuroscience , reproductive health , and sexuality , along with various Mahayana and Vajrayana topics . \"}, {'role': 'assistant', 'content': 'During the 1959 Tibetan uprising , the Dalai Lama fled to India , where he currently lives as a refugee .'}]\n","messages: [{'role': 'system', 'content': ''}, {'role': 'user', 'content': \"Extract sentences from the source text that are relevant (either supporting or refuting) to the preceding claim. Return a comma separated list of sentences.\\n\\nClaim: Alice Cooper has been working in his field for over 50 years.\\n\\nSource Text: Alice Cooper -LRB- born Vincent Damon Furnier ; February 4 , 1948 -RRB- is an American singer , songwriter , and actor whose career spans over five decades . With his distinctive raspy voice and a stage show that features guillotines , electric chairs , fake blood , deadly snakes , baby dolls , and dueling swords , Cooper is considered by music journalists and peers alike to be `` The Godfather of Shock Rock '' . He has drawn equally from horror films , vaudeville , and garage rock to pioneer a macabre and theatrical brand of rock designed to shock people .   Originating in Phoenix , Arizona , in the late 1960s after he moved from Detroit , Michigan , `` Alice Cooper '' was originally a band consisting of Furnier on vocals and harmonica , lead guitarist Glen Buxton , Michael Bruce on rhythm guitar , Dennis Dunaway on bass guitar , and drummer Neal Smith . The original Alice Cooper band released its first album in 1969 but broke into the international music mainstream with the 1971 hit `` I 'm Eighteen '' from their third studio album Love It to Death , which was followed by the even bigger single `` School 's Out '' in 1972 . The band reached their commercial peak with the 1973 album Billion Dollar Babies .   Furnier adopted the band 's name as his own name in the 1970s and began a solo career with the 1975 concept album Welcome to My Nightmare . In 2011 , he released Welcome 2 My Nightmare , his 19th album as a solo artist and 26th album in total . In 2011 , the original Alice Cooper band was inducted into the Rock and Roll Hall of Fame . Expanding from his Detroit rock roots , Cooper has experimented with a number of musical styles , including art rock , hard rock , heavy metal , new wave , glam metal , pop rock , experimental rock , and industrial rock .   Cooper is known for his social and witty persona offstage , with The Rolling Stone Album Guide calling him the world 's most `` beloved heavy metal entertainer '' . He is credited with helping to shape the sound and look of heavy metal , and has been described as the artist who `` first introduced horror imagery to rock 'n' roll , and whose stagecraft and showmanship have permanently transformed the genre '' . Away from music , Cooper is a film actor , a golfing celebrity , a restaurateur , and , since 2004 , a popular radio DJ with his classic rock show Nights with Alice Cooper . \"}, {'role': 'assistant', 'content': 'Alice Cooper -LRB- born Vincent Damon Furnier ; February 4 , 1948 -RRB- is an American singer , songwriter , and actor whose career spans over five decades .'}]\n","-------------------------------------------------------------------------------------------\n"]}],"source":["# Print the first several rows of each of the two training files\n","sentEx_train = load_jsonl(f'{fever_path}GPT_sentEx_paper_dev_train/prompt_v1_segmented_n100_03-16_000.jsonl')\n","sentEx_valid = load_jsonl(f'{fever_path}GPT_sentEx_paper_dev_valid/prompt_v1_segmented_n30_03-16_000.jsonl')\n","clf_train = load_jsonl(f'{fever_path}GPT_clf_paper_dev_train/prompt_v1_segmented_n100_03-16_000.jsonl')\n","clf_valid = load_jsonl(f'{fever_path}GPT_clf_paper_dev_valid/prompt_v1_segmented_n30_03-16_000.jsonl')\n","\n","# Print the first few  nested objects\n","print(f\"Classifiction training data:\")\n","for item, i in zip(clf_train, range(5)):\n","  for key in (item.keys()):\n","    print(key + ': ' + str(clf_train[i][key]))\n","print('-------------------------------------------------------------------------------------------')\n","print(f\"Classifiction validation data:\")\n","for item, i in zip(clf_valid, range(5)):\n","  for key in (item.keys()):\n","    print(key + ': ' + str(clf_valid[i][key]))\n","print('-------------------------------------------------------------------------------------------')\n","print(f\"Evidence extraction training data:\")\n","for item, i in zip(sentEx_train, range(5)):\n","  for key in (item.keys()):\n","    print(key + ': ' + str(sentEx_train[i][key]))\n","print('-------------------------------------------------------------------------------------------')\n","print(f\"Evidence extraction validation data:\")\n","for item, i in zip(sentEx_valid, range(5)):\n","  for key in (item.keys()):\n","    print(key + ': ' + str(sentEx_valid[i][key]))\n","print('-------------------------------------------------------------------------------------------')\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":625,"status":"ok","timestamp":1742179737484,"user":{"displayName":"Henry Zelenak","userId":"01809909909045225068"},"user_tz":240},"id":"N99yAn0_kIdX","outputId":"62051e2e-2cc3-4f4f-9a2e-9632e58a93e3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Processing file: prompt_v1_segmented_n200_03-16_001.jsonl\n","Metadata saved to ./datasets/FEVER/GPT_meta/sentEx/train_001.jsonl_metadata.csv\n","Processing file: prompt_v1_segmented_n60_03-16_001.jsonl\n","Metadata saved to ./datasets/FEVER/GPT_meta/sentEx/valid_001.jsonl_metadata.csv\n","Processing file: prompt_v1_segmented_n200_03-16_001.jsonl\n","Metadata saved to ./datasets/FEVER/GPT_meta/clf/train_001.jsonl_metadata.csv\n","Processing file: prompt_v1_segmented_n60_03-16_001.jsonl\n","Metadata saved to ./datasets/FEVER/GPT_meta/clf/valid_001.jsonl_metadata.csv\n"]}],"source":["# Collect metadata about the files (dataset_name  avg_evdc_len  max_evdc_len  min_evdc_len  sys_msg user_msg  sys_msg_num_tokens\tsys_msg_avg_wrd_len\tusr_msg_num_tokens\tusr_msg_avg_wrd_len)\n","\n","# Helper function to tokenize text and return number of tokens and semantic complexity\n","def tokenize_text(text):\n","    \"\"\"\n","    Tokenizes the input text and returns the number of tokens and semantic complexity.\n","    \"\"\"\n","    # Tokenize the text\n","    tokens = nltk.word_tokenize(text)\n","\n","    # Calculate the number of tokens\n","    num_tokens = len(tokens)\n","\n","    # Calculate semantic complexity using a simple heuristic (e.g., average word length)\n","    avg_word_length = sum(len(word) for word in tokens) / num_tokens if num_tokens > 0 else 0\n","\n","    return num_tokens, avg_word_length\n","\n","\n","# Create a function to collect metadata about the files\n","def collect_metadata(in_path, out_path, mode):\n","    \"\"\"\n","    Collects metadata about the file in the input path and saves it to a CSV file in the output path.\n","    \"\"\"\n","    metadata = pd.DataFrame(columns=['dataset_name', 'size', 'avg_evdc_len', 'max_evdc_len', 'min_evdc_len', 'avg_evdc_tkns', 'avg_claim_tkns', 'max_claim_tkns', 'min_claim_tkns', 'sys_msg', 'user_msg', 'sys_msg_num_tokens', 'sys_msg_avg_wrd_len', 'usr_msg_num_tokens', 'usr_msg_avg_wrd_len'])\n","    filename = os.path.basename(in_path)\n","    print(f\"Processing file: {filename}\")\n","    set_id = filename.split('_')[-1]\n","    if filename.endswith('.jsonl'):\n","        with open(in_path, 'r', encoding='utf-8') as f:\n","            data = [json.loads(line) for line in f]\n","            ds_size = len(data)\n","            # Calculate average and max evidence sentence length\n","            evdc_counts = []\n","            evdc_tokens = []\n","            user_stub_tokens = []\n","            claim_tokens = []\n","            if mode == 'clf':\n","              for item in data:\n","                  evidence = item['messages'][1]['content'].split('\\n\\n')[2]\n","                  evdc_counts.append(len(evidence.split('\\n')))\n","                  evdc_tokens.append(len(nltk.word_tokenize(evidence)))\n","                  user_stub_tokens.append(len(nltk.word_tokenize(item['messages'][1]['content'].split('\\n\\n')[0])))\n","                  claim_tokens.append(len(nltk.word_tokenize(item['messages'][1]['content'].split('\\n\\n')[1])))\n","            elif mode == 'sentEx':\n","              for item in data:\n","                  evdc_counts.append(len(item['messages'][2]['content'].split('\\n')))\n","                  evdc_tokens.append(len(nltk.word_tokenize(item['messages'][2]['content'])))\n","                  user_stub_tokens.append(len(nltk.word_tokenize(item['messages'][1]['content'].split('\\n\\n')[0])))\n","                  claim_tokens.append(len(nltk.word_tokenize(item['messages'][1]['content'].split('\\n\\n')[1])))\n","\n","            # Calculate average, max, and min evidence sentence length\n","            avg_evdc_len = np.mean(evdc_counts)\n","            max_evdc_len = np.max(evdc_counts)\n","            min_evdc_len = np.min(evdc_counts)\n","            avg_evdc_tkns = np.mean(evdc_tokens)\n","\n","            # Calculate average, max, and min claim token count\n","            avg_claim_len = np.mean(claim_tokens)\n","            max_claim_len = np.max(claim_tokens)\n","            min_claim_len = np.min(claim_tokens)\n","\n","            item = data[0]\n","            sys_msg_tks, sys_msg_avg_wrd_len = tokenize_text(item['messages'][0]['content'])\n","            usr_msg_tks, usr_msg_avg_wrd_len = tokenize_text(item['messages'][1]['content'].split('\\n\\n')[0])\n","            # Append metadata to the DataFrame\n","            metadata.loc[len(metadata)] = [filename, ds_size, avg_evdc_len, max_evdc_len, min_evdc_len, avg_evdc_tkns, avg_claim_len, max_claim_len, min_claim_len, item['messages'][0]['content'], item['messages'][1]['content'].split('\\n\\n')[0], sys_msg_tks, sys_msg_avg_wrd_len, usr_msg_tks, usr_msg_avg_wrd_len]\n","    else:\n","        print(f\"Error: {filename} is not a JSONL file.\")\n","    # Save metadata to CSV\n","    save_path = out_path + f\"{set_id}_metadata.csv\"\n","    metadata.to_csv(save_path, index=False)\n","    print(f\"Metadata saved to {save_path}\")\n","\n","# Collect metadata for the evidence extraction training data\n","sentEx_meta_paths = {\n","    \"in_paths\": [f\"{fever_path}GPT_sentEx_paper_dev_train/prompt_v1_segmented_n200_03-16_001.jsonl\", f\"{fever_path}GPT_sentEx_paper_dev_valid/prompt_v1_segmented_n60_03-16_001.jsonl\"],\n","    \"out_paths\": [f\"{fever_path}GPT_meta/sentEx/train_\", f\"{fever_path}GPT_meta/sentEx/valid_\"]\n","}\n","clf_meta_paths = {\n","    \"in_paths\": [f\"{fever_path}GPT_clf_paper_dev_train/prompt_v1_segmented_n200_03-16_001.jsonl\", f\"{fever_path}GPT_clf_paper_dev_valid/prompt_v1_segmented_n60_03-16_001.jsonl\"],\n","    \"out_paths\": [f\"{fever_path}GPT_meta/clf/train_\", f\"{fever_path}GPT_meta/clf/valid_\"]\n","}\n","\n","for in_path, out_path in zip(sentEx_meta_paths['in_paths'], sentEx_meta_paths['out_paths']):\n","    collect_metadata(in_path, out_path, 'sentEx')\n","for in_path, out_path in zip(clf_meta_paths['in_paths'], clf_meta_paths['out_paths']):\n","    collect_metadata(in_path, out_path, 'clf')"]}],"metadata":{"colab":{"provenance":[{"file_id":"1BS8ppvnMAXYKJZjuR6u2OPh3aDBdA3Mb","timestamp":1741372446020}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":0}