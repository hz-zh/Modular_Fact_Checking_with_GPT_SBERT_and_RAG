{"cells":[{"cell_type":"code","source":["!rm -rf fever-scorer"],"metadata":{"id":"umwuOVbmuUG9","executionInfo":{"status":"ok","timestamp":1743601098815,"user_tz":240,"elapsed":111,"user":{"displayName":"Henry Zelenak","userId":"01809909909045225068"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["!git clone -b release-v2.0 https://github.com/sheffieldnlp/fever-scorer.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SWnbo9JNrNRl","executionInfo":{"status":"ok","timestamp":1743601104511,"user_tz":240,"elapsed":517,"user":{"displayName":"Henry Zelenak","userId":"01809909909045225068"}},"outputId":"252f3573-9877-4e78-f1de-aa07a6f0831a"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'fever-scorer'...\n","remote: Enumerating objects: 224, done.\u001b[K\n","remote: Counting objects:  20% (1/5)\u001b[K\rremote: Counting objects:  40% (2/5)\u001b[K\rremote: Counting objects:  60% (3/5)\u001b[K\rremote: Counting objects:  80% (4/5)\u001b[K\rremote: Counting objects: 100% (5/5)\u001b[K\rremote: Counting objects: 100% (5/5), done.\u001b[K\n","remote: Compressing objects: 100% (5/5), done.\u001b[K\n","remote: Total 224 (delta 0), reused 0 (delta 0), pack-reused 219 (from 1)\u001b[K\n","Receiving objects: 100% (224/224), 1.13 MiB | 9.40 MiB/s, done.\n","Resolving deltas: 100% (110/110), done.\n"]}]},{"cell_type":"code","source":["%cd fever-scorer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qhs44MknrYBW","executionInfo":{"status":"ok","timestamp":1743601106662,"user_tz":240,"elapsed":11,"user":{"displayName":"Henry Zelenak","userId":"01809909909045225068"}},"outputId":"927bcb9c-f6ea-4522-a839-68257431a6da"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/fever-scorer\n"]}]},{"cell_type":"code","source":["!pip install -r requirements.txt\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MJRmxui4s4_9","executionInfo":{"status":"ok","timestamp":1743601110569,"user_tz":240,"elapsed":2314,"user":{"displayName":"Henry Zelenak","userId":"01809909909045225068"}},"outputId":"fc68268a-4f94-429c-91fe-d10f864e6e65"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (1.17.0)\n"]}]},{"cell_type":"code","source":["!pip install .\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q14RLRmVs8hd","executionInfo":{"status":"ok","timestamp":1743601180442,"user_tz":240,"elapsed":5452,"user":{"displayName":"Henry Zelenak","userId":"01809909909045225068"}},"outputId":"aad3216a-9cca-4f6a-c4f0-076795273773"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Using pip 24.1.2 from /usr/local/lib/python3.11/dist-packages/pip (python 3.11)\n","Processing /content/fever-scorer\n","  Running command python setup.py egg_info\n","  running egg_info\n","  creating /tmp/pip-pip-egg-info-dvmtizb9/fever_scorer.egg-info\n","  writing /tmp/pip-pip-egg-info-dvmtizb9/fever_scorer.egg-info/PKG-INFO\n","  writing dependency_links to /tmp/pip-pip-egg-info-dvmtizb9/fever_scorer.egg-info/dependency_links.txt\n","  writing requirements to /tmp/pip-pip-egg-info-dvmtizb9/fever_scorer.egg-info/requires.txt\n","  writing top-level names to /tmp/pip-pip-egg-info-dvmtizb9/fever_scorer.egg-info/top_level.txt\n","  writing manifest file '/tmp/pip-pip-egg-info-dvmtizb9/fever_scorer.egg-info/SOURCES.txt'\n","  adding license file 'LICENSE' (matched pattern 'LICEN[CS]E*')\n","  reading manifest file '/tmp/pip-pip-egg-info-dvmtizb9/fever_scorer.egg-info/SOURCES.txt'\n","  reading manifest template 'MANIFEST.in'\n","  writing manifest file '/tmp/pip-pip-egg-info-dvmtizb9/fever_scorer.egg-info/SOURCES.txt'\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from fever-scorer==0.0.0) (1.17.0)\n","Building wheels for collected packages: fever-scorer\n","  Running command python setup.py bdist_wheel\n","  running bdist_wheel\n","  running build\n","  running build_py\n","  Generating grammar tables from /usr/lib/python3.11/lib2to3/Grammar.txt\n","  Generating grammar tables from /usr/lib/python3.11/lib2to3/PatternGrammar.txt\n","  creating build\n","  creating build/lib\n","  creating build/lib/fever\n","  copying src/fever/scorer.py -> build/lib/fever\n","  copying src/fever/__init__.py -> build/lib/fever\n","  creating build/lib/fever2\n","  copying src/fever2/scorer.py -> build/lib/fever2\n","  copying src/fever2/__init__.py -> build/lib/fever2\n","  installing to build/bdist.linux-x86_64/wheel\n","  running install\n","  running install_lib\n","  creating build/bdist.linux-x86_64\n","  creating build/bdist.linux-x86_64/wheel\n","  creating build/bdist.linux-x86_64/wheel/fever\n","  copying build/lib/fever/scorer.py -> build/bdist.linux-x86_64/wheel/fever\n","  copying build/lib/fever/__init__.py -> build/bdist.linux-x86_64/wheel/fever\n","  creating build/bdist.linux-x86_64/wheel/fever2\n","  copying build/lib/fever2/scorer.py -> build/bdist.linux-x86_64/wheel/fever2\n","  copying build/lib/fever2/__init__.py -> build/bdist.linux-x86_64/wheel/fever2\n","  running install_egg_info\n","  running egg_info\n","  creating fever_scorer.egg-info\n","  writing fever_scorer.egg-info/PKG-INFO\n","  writing dependency_links to fever_scorer.egg-info/dependency_links.txt\n","  writing requirements to fever_scorer.egg-info/requires.txt\n","  writing top-level names to fever_scorer.egg-info/top_level.txt\n","  writing manifest file 'fever_scorer.egg-info/SOURCES.txt'\n","  adding license file 'LICENSE' (matched pattern 'LICEN[CS]E*')\n","  reading manifest file 'fever_scorer.egg-info/SOURCES.txt'\n","  reading manifest template 'MANIFEST.in'\n","  writing manifest file 'fever_scorer.egg-info/SOURCES.txt'\n","  Copying fever_scorer.egg-info to build/bdist.linux-x86_64/wheel/fever_scorer-0.0.0-py3.11.egg-info\n","  running install_scripts\n","  adding license file \"LICENSE\" (matched pattern \"LICEN[CS]E*\")\n","  creating build/bdist.linux-x86_64/wheel/fever_scorer-0.0.0.dist-info/WHEEL\n","  creating '/tmp/pip-wheel-m3hqw7dl/fever_scorer-0.0.0-py3-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n","  adding 'fever/__init__.py'\n","  adding 'fever/scorer.py'\n","  adding 'fever2/__init__.py'\n","  adding 'fever2/scorer.py'\n","  adding 'fever_scorer-0.0.0.dist-info/LICENSE'\n","  adding 'fever_scorer-0.0.0.dist-info/METADATA'\n","  adding 'fever_scorer-0.0.0.dist-info/WHEEL'\n","  adding 'fever_scorer-0.0.0.dist-info/top_level.txt'\n","  adding 'fever_scorer-0.0.0.dist-info/RECORD'\n","  removing build/bdist.linux-x86_64/wheel\n","  Building wheel for fever-scorer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fever-scorer: filename=fever_scorer-0.0.0-py3-none-any.whl size=8301 sha256=f967644ef37fca606b27c72f61a2a5627f135b165e0a8146dbd145d8f725192d\n","  Stored in directory: /root/.cache/pip/wheels/f7/a5/f9/dffaef703ff054c8aa2ea4534130aae0e1ff9450753d0d7556\n","Successfully built fever-scorer\n","Installing collected packages: fever-scorer\n","Successfully installed fever-scorer-0.0.0\n"]}]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7563,"status":"ok","timestamp":1743601329677,"user":{"displayName":"Henry Zelenak","userId":"01809909909045225068"},"user_tz":240},"id":"4n9uJPKcF4hL","outputId":"36b0d18e-5918-46a3-8b29-9881384c35de"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting rouge-score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (2.0.2)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.1.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (4.67.1)\n","Building wheels for collected packages: rouge-score\n","  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24951 sha256=410254fc43d4bf702d4d898311a62c256d12aee325def93e3ef9c3f5a29474f4\n","  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n","Successfully built rouge-score\n","Installing collected packages: rouge-score\n","Successfully installed rouge-score-0.1.2\n"]}],"source":["!pip install rouge-score"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15938,"status":"ok","timestamp":1743601354366,"user":{"displayName":"Henry Zelenak","userId":"01809909909045225068"},"user_tz":240},"id":"tHI9GFAlDpDV","outputId":"dab254ed-ece4-4ec5-f927-d87819a6dc5c"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n","[nltk_data] Downloading package maxent_ne_chunker to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n","[nltk_data] Downloading package maxent_ne_chunker_tab to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/words.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package treebank to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/treebank.zip.\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":9}],"source":["import pandas as pd\n","import nltk  # Make sure NLTK is installed and data downloaded (e.g., nltk.download('punkt'))\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","from rouge_score import rouge_scorer\n","import openai  # For LLM interaction\n","from openai import OpenAI\n","import numpy as np\n","from nltk import Tree, pos_tag, word_tokenize, ne_chunk\n","from nltk.corpus import stopwords\n","import numpy as np\n","from fever.scorer import fever_score # Import the FEVER scorer\n","from nltk import RegexpParser\n","\n","\n","# Download the necessary NLTK data files\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger_eng')\n","nltk.download('maxent_ne_chunker')\n","nltk.download('maxent_ne_chunker_tab')\n","nltk.download('words')\n","nltk.download('stopwords')\n","nltk.download('treebank')\n","nltk.download('punkt_tab')"]},{"cell_type":"code","source":["\n","\n","# Sample instances\n","instances = [\n","    {\n","        \"label\": \"REFUTES\",\n","        \"predicted_label\": \"REFUTES\",\n","        \"predicted_evidence\": [[\"Page1\", 1]],\n","        \"evidence\": [\n","            [\n","                [None, None, \"Page1\", 1],\n","                [None, None, \"Page2\", 2],\n","            ]\n","        ],\n","    },\n","    {\n","        \"label\": \"SUPPORTS\",\n","        \"predicted_label\": \"SUPPORTS\",\n","        \"predicted_evidence\": [[\"Page3\", 3]],\n","        \"evidence\": [\n","            [\n","                [None, None, \"Page3\", 3]\n","            ]\n","        ],\n","    },\n","    {\n","        \"label\": \"NOT ENOUGH INFO\",\n","        \"predicted_label\": \"NOT ENOUGH INFO\",\n","        \"predicted_evidence\": [],\n","        \"evidence\": [],\n","    },\n","]\n","\n","# Calculate scores\n","strict_score, label_accuracy, precision, recall, f1 = fever_score(instances)\n","\n","# Display results\n","print(f\"Strict Score: {strict_score:.2f}\")\n","print(f\"Label Accuracy: {label_accuracy:.2f}\")\n","print(f\"Precision: {precision:.2f}\")\n","print(f\"Recall: {recall:.2f}\")\n","print(f\"F1 Score: {f1:.2f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hPMp1E7ivS9G","executionInfo":{"status":"ok","timestamp":1743601359701,"user_tz":240,"elapsed":19,"user":{"displayName":"Henry Zelenak","userId":"01809909909045225068"}},"outputId":"4d1391a7-d5cc-47c8-f209-ec39ffbf68f9"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Strict Score: 0.67\n","Label Accuracy: 1.00\n","Precision: 1.00\n","Recall: 0.50\n","F1 Score: 0.67\n"]}]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15405,"status":"ok","timestamp":1743601433389,"user":{"displayName":"Henry Zelenak","userId":"01809909909045225068"},"user_tz":240},"id":"T21Kds-sFkB5","outputId":"e1d6f136-ac8e-459c-d7d6-da599c992025"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Mount google drive\n","from google.colab import drive\n","import gc\n","\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":922,"status":"ok","timestamp":1743601434312,"user":{"displayName":"Henry Zelenak","userId":"01809909909045225068"},"user_tz":240},"id":"-Op1htlxFQZz"},"outputs":[],"source":["from google.colab import userdata\n","api_key = userdata.get('openaikey')\n","client = OpenAI(api_key=api_key)"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1743601449183,"user":{"displayName":"Henry Zelenak","userId":"01809909909045225068"},"user_tz":240},"id":"HrnHrCbCFXXs","outputId":"a150f0d0-7da5-485d-e385-0340fe3033a5"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/SUNY_Poly_DSA598\n"]}],"source":["%cd ../drive/My Drive/SUNY_Poly_DSA598/"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":128,"status":"ok","timestamp":1743601450888,"user":{"displayName":"Henry Zelenak","userId":"01809909909045225068"},"user_tz":240},"id":"MWAcKxBFFcfQ","outputId":"e774db4c-0f0e-414a-a2d9-0d2416b9ffc3"},"outputs":[{"output_type":"stream","name":"stdout","text":["archive\t\t\t    .git\t\t\t    presentation\n","datasets\t\t    .gitignore\t\t\t    transcribe_voice_notes.ipynb\n","FEVER_set_creation.ipynb    liar_gpt4omini_base_eval.ipynb  work_documents\n","FEVER_tuning_archive.ipynb  Module_2_dev.ipynb\n"]}],"source":["!ls -a"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1743601483425,"user":{"displayName":"Henry Zelenak","userId":"01809909909045225068"},"user_tz":240},"id":"oZ5Hhk7HUAld","outputId":"46a79b0b-693e-436d-da96-00cef5da1958"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/SUNY_Poly_DSA598/datasets/FEVER\n"]}],"source":["%cd ./datasets/FEVER/"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":112,"status":"ok","timestamp":1743601484358,"user":{"displayName":"Henry Zelenak","userId":"01809909909045225068"},"user_tz":240},"id":"TxfUDQuvUDew","outputId":"12a9e659-10a5-4557-9942-99cb61f59dd8"},"outputs":[{"output_type":"stream","name":"stdout","text":["AVeriTeC   fever2-adversarial.jsonl\t    fever-train.jsonl  paper_dev.jsonl\t tabular_sets\n",".DS_Store  feverous_train_challenges.jsonl  GPT_sets\t       paper_test.jsonl  wiki-pages\n"]}],"source":["!ls -a"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":1767,"status":"ok","timestamp":1743601492724,"user":{"displayName":"Henry Zelenak","userId":"01809909909045225068"},"user_tz":240},"id":"fxxK5zjBPXar"},"outputs":[],"source":["# Data paths (replace with your actual paths if different)\n","fever_path = './datasets/FEVER/'\n","#train_clf_path = f\"{fever_path}tabular_clf_paper_dev_train/v1_segmented_n3461_03-29_001.csv\"\n","#valid_clf_path = f\"{fever_path}tabular_clf_paper_dev_valid/v1_segmented_n1482_03-29_001.csv\"\n","train_sentEx_path = f\"/content/drive/MyDrive/SUNY_Poly_DSA598/datasets/FEVER/tabular_sets/tabular_sentEx_paper_dev_train/v1_segmented_n3461_03-29_001.csv\"\n","valid_sentEx_path = f\"/content/drive/MyDrive/SUNY_Poly_DSA598/datasets/FEVER/tabular_sets/tabular_sentEx_paper_dev_valid/v1_segmented_n1482_03-29_001.csv\"\n","\n","# Load datasets\n","#train_clf = pd.read_csv(train_clf_path)\n","#valid_clf = pd.read_csv(valid_clf_path)\n","train_sentEx = pd.read_csv(train_sentEx_path)\n","valid_sentEx = pd.read_csv(valid_sentEx_path)"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":78,"status":"ok","timestamp":1743601573456,"user":{"displayName":"Henry Zelenak","userId":"01809909909045225068"},"user_tz":240},"id":"FwlHIP2GEmz7","outputId":"36ef34be-7b9b-401c-c992-fb766c08f050"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train set label distribution:\n","label\n","SUPPORTS           1156\n","REFUTES            1156\n","NOT ENOUGH INFO    1149\n","Name: count, dtype: int64\n","Valid set label distribution:\n","label\n","SUPPORTS           495\n","REFUTES            495\n","NOT ENOUGH INFO    488\n","Name: count, dtype: int64\n","Train set label distribution after balancing:\n","label\n","NOT ENOUGH INFO    1149\n","REFUTES            1149\n","SUPPORTS           1149\n","Name: count, dtype: int64\n","Valid set label distribution after balancing:\n","label\n","NOT ENOUGH INFO    488\n","REFUTES            488\n","SUPPORTS           488\n","Name: count, dtype: int64\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-26-03091368d446>:9: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  train_sentEx = train_sentEx.groupby('label').apply(lambda x: x.sample(min_count)).reset_index(drop=True)\n","<ipython-input-26-03091368d446>:11: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  valid_sentEx = valid_sentEx.groupby('label').apply(lambda x: x.sample(min_count)).reset_index(drop=True)\n"]}],"source":["# Show the distribution of labels\n","print(f\"Train set label distribution:\")\n","print(train_sentEx['label'].value_counts())\n","print(f\"Valid set label distribution:\")\n","print(valid_sentEx['label'].value_counts())\n","\n","# Balance the labels by reducing each to the minimum count\n","min_count = min(train_sentEx['label'].value_counts())\n","train_sentEx = train_sentEx.groupby('label').apply(lambda x: x.sample(min_count)).reset_index(drop=True)\n","min_count = min(valid_sentEx['label'].value_counts())\n","valid_sentEx = valid_sentEx.groupby('label').apply(lambda x: x.sample(min_count)).reset_index(drop=True)\n","\n","print(f\"Train set label distribution after balancing:\")\n","print(train_sentEx['label'].value_counts())\n","print(f\"Valid set label distribution after balancing:\")\n","print(valid_sentEx['label'].value_counts())"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":55,"status":"ok","timestamp":1743601595538,"user":{"displayName":"Henry Zelenak","userId":"01809909909045225068"},"user_tz":240},"id":"TO5Dt_5vCGqG"},"outputs":[],"source":["\n","\n","\n","def module_2_semi_supervised_distillation(claim, documents, entities, keywords, verbose=0, debug=False):\n","    \"\"\"\n","    Module 2: Semi-supervised Distillation for sentence extraction.\n","\n","    Args:\n","        claim (str): The input claim.\n","        documents (list of str): List of retrieved documents (full text).\n","        entity (str):  The main entity in the claim (or None if not found).\n","        keywords (list of str):  Top keywords from the claim.\n","\n","    Returns:\n","        tuple: (list of str, str):  A list of extracted evidence sentences and the exit status (\"NOT ENOUGH INFO\" or \"OK\").\n","    \"\"\"\n","\n","\n","    extracted_sentences = []\n","    max_iterations = min(5, len(documents)) # This will be determined later with actual number of sentences across all docs\n","    if debug:\n","      print(f\"DEBUG 2.1.1:\")\n","      print(f\"\\tNumber of documents: {len(documents)}\")\n","      print(f\"\\tMax iterations: {max_iterations}\")\n","      print(\"-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\")\n","\n","\n","    if entities == []:\n","      if verbose == 1:\n","        print(\"No entities found, early exiting...\")\n","        return [], \"NOT ENOUGH INFO\" # Early exit if no entity\n","\n","    # convert entities to string\n","    entities = \", \".join(entities)\n","    if debug:\n","      print(f\"DEBUG 2.1.2:\")\n","      print(f\"\\tClaim: {claim}\")\n","      print(f\"\\tEntities: {entities}\")\n","      print(f\"\\tKeywords: {keywords}\")\n","      print(\"-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\")\n","\n","    for iteration in range(max_iterations):\n","        # 2.2 Prompt Selection (no agent - programmatic)\n","        if iteration == 0:\n","            prompt = f\"Retrieve sentences from the list that either support or refute the following claim. Specifically, focus on sentences mentioning {entities}. Order the sentences by relevance, highest first, and return a list separated by the return character. If there are no relevant sentences, respond with 'NOT ENOUGH INFO'. DO NOT CREATE ANY SENTENCES THAT ARE NOT IN THE PROVIDED LIST.\"\n","        else:  # Follow up prompts\n","            prompt = f\"You didn’t find enough sentences. Find additional (new) sentences that mention {entities} or other key points in the claim. Order the sentences by relevance, highest first, and return a list separated by the return character. If there are no relevant sentences, respond with 'NOT ENOUGH INFO'. DO NOT CREATE ANY SENTENCES THAT ARE NOT IN THE PROVIDED LIST.\"\n","\n","\n","        # 2.3 Sentence Extraction (with pre-filtering)\n","        #filtered_text = sliding_window_filter(documents, entities, keywords)\n","        # Simply form the list of document strings into one string joined by \\n for now\n","        filtered_text = \"\\n\".join(documents)\n","        if debug:\n","          print(f\"DEBUG 2.3.1:\")\n","          print(f\"\\tFiltered text: {filtered_text}\")\n","          print(\"-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\")\n","        new_sentences = extract_sentences_with_llm(claim, filtered_text, prompt, True)\n","\n","        if new_sentences == [\"NOT ENOUGH INFO\"]:  # LLM found no relevant sentences\n","\n","            if iteration == 0:  # No sentences found in the first iteration\n","                if verbose == 1:\n","                  print(f\"2.3: LLM found no relevant sentences, early exiting...\")\n","                return [], \"NOT ENOUGH INFO\"  # Early exit if no relevant sentences in first pass.\n","            else: # No sentences found in subsequent iteration\n","                if len(extracted_sentences) == 0:\n","                  if verbose == 1:\n","                    print(f\"2.3: LLM found no relevant sentences in second attempt, early exiting...\")\n","                  return [], \"NOT ENOUGH INFO\"  # Still not enough info\n","                break   #Stop iterating and use sentences we found\n","\n","\n","        # 2.4 Similarity Comparison and Thresholding\n","        new_sentences = similarity_thresholding(claim, new_sentences, True)\n","\n","        extracted_sentences.extend(new_sentences)  # Add new sentences to the list\n","        if debug:\n","          print(f\"DEBUG 2.4.2:\")\n","          print(f\"\\tAdded {len(new_sentences)} new sentences on iteration {iteration}.\")\n","          print(f\"\\tTotal extracted sentences: {len(extracted_sentences)}\")\n","          print(f\"\\tExtracted sentences: {extracted_sentences}\")\n","          print(\"-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\")\n","        if len(extracted_sentences) >= min(max_iterations, 5):\n","            if verbose == 1:\n","                print(f\"2.4: Found enough sentences, exiting...\")\n","            break  # Exit if enough sentences are found.\n","\n","    if verbose == 1:\n","      print(f\"\\tExtracted sentences: {extracted_sentences}\")\n","      print(f\"\\tNumber of extracted sentences: {len(extracted_sentences)}\")\n","      print(f\"\\tMax iterations: {max_iterations}\")\n","      print(\"-------------------------------------------------------------\")\n","\n","    #Check for final early exit\n","    if len(extracted_sentences) < 5 and max_iterations > 5: #Pilot study will determine best practices here\n","        return [], \"NOT ENOUGH INFO\"  # Not enough relevant sentences found after iterations\n","\n","    return extracted_sentences, \"OK\"\n","\n","\n","\n","def sliding_window_filter(documents, entity, keywords, debug=False):\n","    \"\"\"\n","    Performs sliding window filtering based on entity and keywords.\n","\n","    Args:\n","        documents (list of str):  List of documents.\n","        entity (str): The entity to match.\n","        keywords (list of str): The keywords to match.\n","\n","    Returns:\n","        str: Concatenated filtered text.\n","    \"\"\"\n","    # TODO: Implement sliding window filtering logic (using NLTK, spaCy, etc.).\n","    # This function should concatenate sentences from windows that contain entity/keywords.\n","    filtered_sentences = []\n","\n","    for document in documents:\n","        sentences = nltk.sent_tokenize(document)\n","        window_size = 3  # Experiment with different window sizes.\n","        for i in range(len(sentences) - window_size + 1):\n","            window = sentences[i : i + window_size]\n","            window_text = \" \".join(window)\n","            if entity in window_text and any(keyword in window_text for keyword in keywords):\n","                filtered_sentences.extend(window) # Add matching sentences from window to filtered sentences\n","\n","    filtered_text = \"\\n\".join(filtered_sentences)\n","    return filtered_text\n","\n","\n","\n","def extract_sentences_with_llm(claim, filtered_text, prompt, debug=False):\n","    \"\"\"\n","    Extracts sentences using an LLM.\n","\n","    Args:\n","        claim (str): The input claim.\n","        filtered_text (str): The pre-filtered text.\n","        prompt (str): The prompt for the LLM.\n","\n","    Returns:\n","        list of str: Extracted sentences.\n","    \"\"\"\n","    # TODO: Implement LLM interaction (using OpenAI API, etc.).\n","    # Use the provided prompt and filtered_text to extract sentences with the LLM.\n","\n","    if debug:\n","      print(f\"DEBUG 2.3.2:\")\n","      print(f\"\\tPrompt: {prompt}\")\n","      print(f\"\\tFiltered text: {filtered_text}\")\n","      print(\"-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\")\n","\n","    response = client.chat.completions.create(\n","        model=\"gpt-4o-mini-2024-07-18\",\n","        messages=[\n","            {\"role\": \"system\", \"content\": \"You are a helpful assistant that extracts evidence sentences.\"},\n","            {\"role\": \"user\", \"content\": f\"{prompt}\\nClaim: {claim}\\nText: {filtered_text}\"},\n","        ],\n","        max_tokens=512,  # Adjust as needed\n","        n=1,\n","        stop=None,  # Or a suitable stop sequence\n","        temperature=0.9,  # Adjust as needed\n","    )\n","    extracted_sentences_raw = response.choices[0].message.content\n","    extracted_sentences = extracted_sentences_raw.split('\\n')  # Assuming sentences are separated by newlines\n","\n","    return extracted_sentences\n","\n","\n","\n","\n","def similarity_thresholding(claim, sentences, debug=False):\n","    \"\"\"\n","    Thresholds sentences based on similarity to the claim.\n","\n","    Args:\n","        claim (str): The input claim.\n","        sentences (list of str): The sentences to threshold.\n","\n","    Returns:\n","        list of str:  The sentences that meet the similarity threshold.\n","    \"\"\"\n","    # TODO: Implement similarity calculations (ROUGE, TF-IDF, cosine similarity).\n","    # Return only the sentences that exceed the defined threshold(s).\n","\n","    threshold = 0.2 # Placeholder, determine empirically.\n","    filtered_sentences = []\n","\n","    vectorizer = TfidfVectorizer()  # Initialize TF-IDF vectorizer\n","    claim_tfidf = vectorizer.fit_transform([claim])\n","    scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n","\n","\n","    for sentence in sentences:\n","        sentence_tfidf = vectorizer.transform([sentence])\n","        cosine_sim = cosine_similarity(claim_tfidf, sentence_tfidf)[0][0]\n","        rouge_scores = scorer.score(claim, sentence)\n","        rouge1_score = rouge_scores['rouge1'].fmeasure  # Example: Use ROUGE-1 F1-score\n","\n","        if debug:\n","          print(f\"DEBUG 2.4.1:\")\n","          print(f\"\\tClaim: {claim}\")\n","          print(f\"\\tSentence: {sentence}\")\n","          print(f\"\\tCosine Similarity: {cosine_sim}\")\n","\n","        if cosine_sim >= threshold and rouge1_score >= threshold:  # Example: Combine cosine similarity and ROUGE\n","            filtered_sentences.append(sentence)\n","\n","    return filtered_sentences\n","\n","\n","# Entity and keywords extraction function\n","def extract_entities(text):\n","    \"\"\"\n","    Extracts entities from the text using NLTK's Named Entity Chunker.\n","\n","    Args:\n","        text (str): The input text.\n","\n","    Returns:\n","        list of str: List of extracted entities.\n","    \"\"\"\n","    stop_words = set(stopwords.words('english'))\n","    tokens = word_tokenize(text)\n","    tokens = [word for word in tokens if word.lower() not in stop_words]\n","    tagged_tokens = pos_tag(tokens)\n","    named_entities = ne_chunk(tagged_tokens)\n","\n","    entities = []\n","    for subtree in named_entities:\n","        if isinstance(subtree, Tree):\n","            entity = \" \".join([word for word, tag in subtree.leaves()])\n","            entities.append(entity)\n","\n","    return entities\n","\n","\n","# Keyword extraction function\n","def extract_keywords(text):\n","    \"\"\"\n","    Extracts keywords from the text using TF-IDF.\n","\n","    Args:\n","        text (str): The input text.\n","\n","    Returns:\n","        list of str: List of extracted keywords.\n","    \"\"\"\n","    vectorizer = TfidfVectorizer(stop_words='english', max_features=10)  # Adjust max_features as needed\n","    tfidf_matrix = vectorizer.fit_transform([text])\n","    feature_names = vectorizer.get_feature_names_out()\n","    dense = tfidf_matrix.todense()\n","    denselist = dense.tolist()\n","    # Convert the list to a NumPy array to use argsort()\n","\n","    dense_array = np.array(denselist[0])\n","    keywords = [feature_names[i] for i in dense_array.argsort()[-5:]]  # Get top 5 keywords\n","\n","    return keywords\n","\n","# Scorer function for evaluation\n","def scorer(gold_sentences, extracted_sentences):\n","    \"\"\"\n","    Evaluates the extracted sentences against gold standard sentences.\n","\n","    Args:\n","        gold_sentences (list of str): The gold standard sentences.\n","        extracted_sentences (list of str): The extracted sentences.\n","\n","    Returns:\n","        dict: Evaluation scores.\n","    \"\"\"\n","    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n","    scores = {}\n","\n","    for gold_sentence in gold_sentences:\n","        for extracted_sentence in extracted_sentences:\n","            score = scorer.score(gold_sentence, extracted_sentence)\n","            scores[extracted_sentence] = score\n","\n","    # Calculate the macro-F1 score\n","    total_score = 0\n","    for sentence, score in scores.items():\n","        total_score += score['rouge1'].fmeasure\n","    avg_score = total_score / len(scores) if scores else 0\n","    return scores, avg_score\n","\n","# Claim generator function using valid_sentEx\n","index = 4\n","def generate_claim(df):\n","    \"\"\"\n","    Generates a claim from the provided DataFrame.\n","    \"\"\"\n","    global index\n","    if index >= len(df):\n","        index = 0  # Reset index if it exceeds the DataFrame length\n","    claim = df.iloc[index]['claim']\n","    documents = df.iloc[index]['full_text']\n","    documents = documents.split('\\n')\n","    label = df.iloc[index]['label']\n","    index += 1\n","    return claim, documents, label\n"]},{"cell_type":"code","source":["claim, documents_text, label = generate_claim(valid_sentEx)  # Generate a claim and its associated documents\n","\n","\n","keywords = extract_keywords(claim)  # Extract keywords from the claim\n","entities = extract_entities(claim)  # Extract entities from the claim\n","\n","print(f\"DEBUG 1.1:\")\n","print(f\"\\tClaim: {claim}\")\n","print(\"Entities:\", entities)\n","print(\"Keywords:\", keywords)\n","\n","\n","# Convert documents to list of strings if it's a single string\n","if isinstance(documents_text, str):\n","    documents = documents_text.split('\\n')\n","elif isinstance(documents_text, list):\n","  documents = documents_text\n","else:\n","  raise Exception(f\"documents must be a list of strings, not {type(documents_text)}\")\n","\n","print(f\"Claim: {claim}\")\n","print(\"Entities:\", entities)\n","print(\"Keywords:\", keywords)\n","\n","extracted_evidence, status = module_2_semi_supervised_distillation(claim, documents, entities, keywords, verbose=1, debug=True)\n","\n","\n","\n","# Convert extracted evidence to FEVER format\n","predicted_evidence = []\n","for sentence in extracted_evidence:\n","    for doc in documents:\n","        if sentence in doc:  # or a more robust check if sentences are preprocessed\n","            line_num = doc.split('\\n').index(sentence)  # Assumes sentences are split by newlines in doc\n","            predicted_evidence.append([f\"page_{documents.index(doc)}\", line_num])\n","            break  # Stop searching once sentence is found in a document\n","\n","# Construct FEVER prediction instance\n","prediction_instance = {\n","    \"predicted_label\": \"SUPPORTS\" if status == \"OK\" else \"NOT ENOUGH INFO\",  # Or REFUTES based on your logic\n","    \"predicted_evidence\": predicted_evidence,\n","\n","}\n","\n","\n","# Get evidence for the claim (adapt this based on your gold standard data format)\n","actual_evidence = []\n","gold_sentences = valid_sentEx[valid_sentEx[\"claim\"] == claim][\"evidence_sentences\"].iloc[0].split('\\n')\n","for sentence in gold_sentences:\n","    for doc in documents:\n","        if sentence in doc:\n","            line_num = doc.split('\\n').index(sentence)\n","            actual_evidence.append([f\"page_{documents.index(doc)}\", line_num])\n","            break\n","\n","# Construct the actual evidence (gold standard) dictionary:\n","\n","actual_instance = {\"label\": label, \"evidence\": [actual_evidence]}\n","\n","# Calculate FEVER score\n","strict_score, label_accuracy, precision, recall, f1 = fever_score([prediction_instance], [actual_instance])\n","fever_results = {\n","    'strict_score': strict_score,\n","    'label_accuracy': label_accuracy,\n","    'precision': precision,\n","    'recall': recall,\n","    'f1': f1\n","}\n","\n","print(\"FEVER Scores:\", fever_results)\n","print(\"-------------------------------------------------------------\")\n","\n","\n","if status == \"OK\":\n","    print(\"Extracted Evidence Sentences:\")\n","    for sentence in extracted_evidence:\n","        print('\\t' + sentence)\n","else:\n","    print(f\"Claim classification: {status}\")\n","\n","\n","\n","\n","# Example evaluation using your existing scorer (optional, but good for comparison)\n","print(\"\\nAdditional Evaluation Scores (ROUGE, etc.):\")\n","scores, avg_score = scorer(gold_sentences, set(extracted_evidence)) # changed documents to gold_sentences for ROUGE evaluation, make sure to set to correct variable, most likely will be called evidence_sentences\n","for sentence, score in scores.items():\n","    print(f\"Sentence: {sentence}\")\n","    print(f\"Scores: {score}\")\n","    print(\"-------------------------------------------------------------\")\n","print(f\"Average Score: {avg_score}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":664},"id":"FJ6Azxe8MHBB","executionInfo":{"status":"error","timestamp":1743601669526,"user_tz":240,"elapsed":1793,"user":{"displayName":"Henry Zelenak","userId":"01809909909045225068"}},"outputId":"142e0d17-79c3-4632-a112-e3f47536fe98"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["DEBUG 1.1:\n","\tClaim: Goosebumps (film) was directed by a Baptist.\n","Entities: ['Goosebumps', 'Baptist']\n","Keywords: ['baptist', 'directed', 'film', 'goosebumps']\n","Claim: Goosebumps (film) was directed by a Baptist.\n","Entities: ['Goosebumps', 'Baptist']\n","Keywords: ['baptist', 'directed', 'film', 'goosebumps']\n","DEBUG 2.1.1:\n","\tNumber of documents: 1\n","\tMax iterations: 1\n","-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n","DEBUG 2.1.2:\n","\tClaim: Goosebumps (film) was directed by a Baptist.\n","\tEntities: Goosebumps, Baptist\n","\tKeywords: ['baptist', 'directed', 'film', 'goosebumps']\n","-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n","DEBUG 2.3.1:\n","\tFiltered text: The Tennis South Invitational, was a men's tennis tournament founded in 1 as the Mississippi International Indoor Tennis Championships. It was played at the Mississippi Coliseum in Jackson, Mississippi in the United States until 1977. The event was played as part of the USLTA Indoor Circuit from 1 through 19751 and became a World Championship Tennis event in 1976. In its final year, 1977, it was an independent event, i.e. not part of a tennis tour or circuit.2 The tournament was played on indoor carpet courts. Ken Rosewall was the only multiple singles champion, winning the title in 1 and 1976.\n","-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n","DEBUG 2.3.2:\n","\tPrompt: Retrieve sentences from the list that either support or refute the following claim. Specifically, focus on sentences mentioning Goosebumps, Baptist. Order the sentences by relevance, highest first, and return a list separated by the return character. If there are no relevant sentences, respond with 'NOT ENOUGH INFO'. DO NOT CREATE ANY SENTENCES THAT ARE NOT IN THE PROVIDED LIST.\n","\tFiltered text: The Tennis South Invitational, was a men's tennis tournament founded in 1 as the Mississippi International Indoor Tennis Championships. It was played at the Mississippi Coliseum in Jackson, Mississippi in the United States until 1977. The event was played as part of the USLTA Indoor Circuit from 1 through 19751 and became a World Championship Tennis event in 1976. In its final year, 1977, it was an independent event, i.e. not part of a tennis tour or circuit.2 The tournament was played on indoor carpet courts. Ken Rosewall was the only multiple singles champion, winning the title in 1 and 1976.\n","-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n","2.3: LLM found no relevant sentences, early exiting...\n"]},{"output_type":"error","ename":"ValueError","evalue":"'Ken Rosewall was the only multiple singles champion, winning the title in 1 and 1976.' is not in list","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-cfb122bcf288>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mline_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0mactual_evidence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"page_{documents.index(doc)}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: 'Ken Rosewall was the only multiple singles champion, winning the title in 1 and 1976.' is not in list"]}]},{"cell_type":"code","source":["\n","\n","\n","extracted_evidence, status = module_2_semi_supervised_distillation(claim, documents, entities, keywords, verbose=1, debug=True)\n","\n","if status == \"OK\":\n","    print(\"Extracted Evidence Sentences:\")\n","    for sentence in extracted_evidence:\n","        print('\\t' + sentence)\n","    print(f\"Claim classification: {status}\")\n","    print(f\"Label: {label}\")\n","else:\n","    print(f\"Claim classification: {status}\")\n","    print(f\"Label: {label}\")\n","\n","# Evaluation\n","# Assuming you have a gold standard set of sentences for evaluation\n","\n","print(\"Evaluation Scores:\")\n","scores, avg_score = scorer(documents, set(extracted_evidence))\n","for sentence, score in scores.items():\n","    print(f\"Sentence: {sentence}\")\n","    print(f\"Scores: {score}\")\n","    print(\"-------------------------------------------------------------\")\n","print(f\"Average Score: {avg_score}\")  # Adjust based on your scoring logic"],"metadata":{"id":"PERzNad2ISk2"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}