{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "umwuOVbmuUG9"
   },
   "outputs": [],
   "source": [
    "#%cd /content/\n",
    "#!rm -rf fever-scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1431,
     "status": "ok",
     "timestamp": 1743790148744,
     "user": {
      "displayName": "Henry Zelenak",
      "userId": "01809909909045225068"
     },
     "user_tz": 240
    },
    "id": "SWnbo9JNrNRl",
    "outputId": "a94f6371-83f1-459b-c903-08b232f9eeb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'fever-scorer'...\n",
      "remote: Enumerating objects: 224, done.\u001b[K\n",
      "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
      "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
      "remote: Total 224 (delta 0), reused 0 (delta 0), pack-reused 219 (from 1)\u001b[K\n",
      "Receiving objects: 100% (224/224), 1.13 MiB | 3.94 MiB/s, done.\n",
      "Resolving deltas: 100% (110/110), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone -b release-v2.0 https://github.com/sheffieldnlp/fever-scorer.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1743790148763,
     "user": {
      "displayName": "Henry Zelenak",
      "userId": "01809909909045225068"
     },
     "user_tz": 240
    },
    "id": "qhs44MknrYBW",
    "outputId": "18e72105-fd1d-415e-e714-b148298f3d6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/fever-scorer\n"
     ]
    }
   ],
   "source": [
    "%cd fever-scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9593,
     "status": "ok",
     "timestamp": 1743790158357,
     "user": {
      "displayName": "Henry Zelenak",
      "userId": "01809909909045225068"
     },
     "user_tz": 240
    },
    "id": "MJRmxui4s4_9",
    "outputId": "58be5dbc-ee76-439e-e180-3f4c4572bf6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 100,
     "status": "ok",
     "timestamp": 1743790158459,
     "user": {
      "displayName": "Henry Zelenak",
      "userId": "01809909909045225068"
     },
     "user_tz": 240
    },
    "id": "ImTsxLYqjDs2",
    "outputId": "c797a490-8fb0-4f1c-9b10-1f014d89e149"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup.py updated\n"
     ]
    }
   ],
   "source": [
    "# Open /setup.py and add 'license=\"MIT\"' on line 12, then overwrite the file\n",
    "import os\n",
    "with open('setup.py', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    lines[11] = 'license=\"MIT\"\\n'\n",
    "with open('setup.py', 'w') as f:\n",
    "    f.writelines(lines)\n",
    "    f.close()\n",
    "    print(\"setup.py updated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13921,
     "status": "ok",
     "timestamp": 1743790172379,
     "user": {
      "displayName": "Henry Zelenak",
      "userId": "01809909909045225068"
     },
     "user_tz": 240
    },
    "id": "Q14RLRmVs8hd",
    "outputId": "f53290e2-087b-4b68-bb05-b891ecdfd154"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /content/fever-scorer\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from fever-scorer==0.0.0) (1.17.0)\n",
      "Building wheels for collected packages: fever-scorer\n",
      "  Building wheel for fever-scorer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for fever-scorer: filename=fever_scorer-0.0.0-py3-none-any.whl size=8288 sha256=e18124507e73ad409b9381001026ec35f61fd0569cd634e7da8fb856d5339208\n",
      "  Stored in directory: /root/.cache/pip/wheels/f7/a5/f9/dffaef703ff054c8aa2ea4534130aae0e1ff9450753d0d7556\n",
      "Successfully built fever-scorer\n",
      "Installing collected packages: fever-scorer\n",
      "Successfully installed fever-scorer-0.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10995,
     "status": "ok",
     "timestamp": 1743790183375,
     "user": {
      "displayName": "Henry Zelenak",
      "userId": "01809909909045225068"
     },
     "user_tz": 240
    },
    "id": "4n9uJPKcF4hL",
    "outputId": "7ff92b5f-cec8-4eaf-fd88-fa73537dc302"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (2.0.2)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.1.8)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (4.67.1)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=47a366ec7e904c185d3dfa0110044d4ecb8b25ee77bbd104796aa67a1696b1f9\n",
      "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: rouge-score\n",
      "Successfully installed rouge-score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19755,
     "status": "ok",
     "timestamp": 1743790203146,
     "user": {
      "displayName": "Henry Zelenak",
      "userId": "01809909909045225068"
     },
     "user_tz": 240
    },
    "id": "tHI9GFAlDpDV",
    "outputId": "dff3d662-8a9e-4c0d-c438-4f3367604c47"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
      "[nltk_data] Downloading package words to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/treebank.zip.\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk  # Make sure NLTK is installed and data downloaded (e.g., nltk.download('punkt'))\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rouge_score import rouge_scorer\n",
    "import openai  # For LLM interaction\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "from nltk import Tree, pos_tag, word_tokenize, ne_chunk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from fever.scorer import fever_score # Import the FEVER scorer\n",
    "from nltk import RegexpParser\n",
    "import json\n",
    "\n",
    "# Download the necessary NLTK data files\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('maxent_ne_chunker_tab')\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('treebank')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1743790203187,
     "user": {
      "displayName": "Henry Zelenak",
      "userId": "01809909909045225068"
     },
     "user_tz": 240
    },
    "id": "hPMp1E7ivS9G",
    "outputId": "ea679bc8-d3df-4eb1-81a7-97bc6e14c222"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strict Score: 0.67\n",
      "Label Accuracy: 1.00\n",
      "Precision: 0.67\n",
      "Recall: 0.50\n",
      "F1 Score: 0.57\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Sample instances\n",
    "instances = [\n",
    "    {\n",
    "        \"label\": \"REFUTES\",\n",
    "        \"predicted_label\": \"REFUTES\",\n",
    "        \"predicted_evidence\": [[\"Page1\", 1], [\"Page3\", 2], [\"Page3\", 2]],\n",
    "        \"evidence\": [\n",
    "            [\n",
    "                [None, None, \"Page1\", 1],\n",
    "                [None, None, \"Page2\", 2],\n",
    "                [None, None, \"Page2\", 2],\n",
    "            ],\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"SUPPORTS\",\n",
    "        \"predicted_label\": \"SUPPORTS\",\n",
    "        \"predicted_evidence\": [[\"Page3\", 3]],\n",
    "        \"evidence\": [\n",
    "            [\n",
    "                [None, None, \"Page3\", 3]\n",
    "            ]\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"NOT ENOUGH INFO\",\n",
    "        \"predicted_label\": \"NOT ENOUGH INFO\",\n",
    "        \"predicted_evidence\": [],\n",
    "        \"evidence\": [],\n",
    "    },\n",
    "]\n",
    "\n",
    "# Calculate scores\n",
    "strict_score, label_accuracy, precision, recall, f1 = fever_score(instances)\n",
    "\n",
    "# Display results\n",
    "print(f\"Strict Score: {strict_score:.2f}\")\n",
    "print(f\"Label Accuracy: {label_accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 104164,
     "status": "ok",
     "timestamp": 1743790307351,
     "user": {
      "displayName": "Henry Zelenak",
      "userId": "01809909909045225068"
     },
     "user_tz": 240
    },
    "id": "T21Kds-sFkB5",
    "outputId": "4fc0c729-92dd-4d17-85c8-5ae0b7f39c3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Mount google drive\n",
    "from google.colab import drive\n",
    "import gc\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Op1htlxFQZz"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "api_key = userdata.get('openaikey')\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1743790308152,
     "user": {
      "displayName": "Henry Zelenak",
      "userId": "01809909909045225068"
     },
     "user_tz": 240
    },
    "id": "HrnHrCbCFXXs",
    "outputId": "4189f63d-bdc7-406f-9fee-6869e82767e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/SUNY_Poly_DSA598\n"
     ]
    }
   ],
   "source": [
    "%cd ../drive/My Drive/SUNY_Poly_DSA598/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 105,
     "status": "ok",
     "timestamp": 1743790308258,
     "user": {
      "displayName": "Henry Zelenak",
      "userId": "01809909909045225068"
     },
     "user_tz": 240
    },
    "id": "MWAcKxBFFcfQ",
    "outputId": "84d81132-a1a3-4934-c22a-db4e06271a48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "archive\t\t\t  .git\t\t\t\t  presentation\n",
      "datasets\t\t  .gitignore\t\t\t  transcribe_voice_notes.ipynb\n",
      "FEVER_set_creation.ipynb  liar_gpt4omini_base_eval.ipynb  work_documents\n",
      "FEVER_set_update.ipynb\t  Module_2_dev.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 78,
     "status": "ok",
     "timestamp": 1743790308364,
     "user": {
      "displayName": "Henry Zelenak",
      "userId": "01809909909045225068"
     },
     "user_tz": 240
    },
    "id": "oZ5Hhk7HUAld",
    "outputId": "d9459550-dc1e-42e9-8b02-6b00afbbb341"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/SUNY_Poly_DSA598/datasets/FEVER\n"
     ]
    }
   ],
   "source": [
    "%cd ./datasets/FEVER/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 359,
     "status": "ok",
     "timestamp": 1743790308723,
     "user": {
      "displayName": "Henry Zelenak",
      "userId": "01809909909045225068"
     },
     "user_tz": 240
    },
    "id": "TxfUDQuvUDew",
    "outputId": "85ea4e6c-fa77-49fa-9475-b6065e24a9c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVeriTeC   fever2-adversarial.jsonl\t    fever-train.jsonl  paper_dev.jsonl\t tabular_sets\n",
      ".DS_Store  feverous_train_challenges.jsonl  GPT_sets\t       paper_test.jsonl  wiki-pages\n"
     ]
    }
   ],
   "source": [
    "!ls -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IBL_wR71bbeh"
   },
   "outputs": [],
   "source": [
    "def load_jsonl(file_path, encoding='utf-8'):\n",
    "    \"\"\"Loads a JSON Lines file into a list of Python objects.\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding=encoding) as f:  # Specify encoding for safety\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))  # Parse each line individually\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fxxK5zjBPXar"
   },
   "outputs": [],
   "source": [
    "# Data paths (replace with your actual paths if different)\n",
    "fever_path = \"./datasets/FEVER/\"\n",
    "train_clf_path = f\"tabular_sets/tabular_clf_paper_dev_train/v1_segmented_sentIDs_n3461_04-04_002.csv\"\n",
    "valid_clf_path = f\"tabular_sets/tabular_clf_paper_dev_valid/v1_segmented_sentIDs_n1482_04-04_002.csv\"\n",
    "train_sentEx_path = f\"tabular_sets/tabular_sentEx_paper_dev_train/v1_segmented_sentIDs_n3461_04-04_002.csv\"\n",
    "valid_sentEx_path = f\"tabular_sets/tabular_sentEx_paper_dev_valid/v1_segmented_sentIDs_n1482_04-04_002.csv\"\n",
    "test_path = f\"paper_test.jsonl\"\n",
    "train_path = f\"paper_dev.jsonl\"\n",
    "\n",
    "# Load datasets\n",
    "#train_clf = pd.read_csv(train_clf_path)\n",
    "#valid_clf = pd.read_csv(valid_clf_path)\n",
    "train_sentEx = pd.read_csv(train_sentEx_path)\n",
    "valid_sentEx = pd.read_csv(valid_sentEx_path)\n",
    "test_jsonl = load_jsonl(test_path)\n",
    "train_jsonl = load_jsonl(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1743790696147,
     "user": {
      "displayName": "Henry Zelenak",
      "userId": "01809909909045225068"
     },
     "user_tz": 240
    },
    "id": "FwlHIP2GEmz7",
    "outputId": "cb2c1dab-1c65-4c25-d2aa-c17472426ceb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set label distribution:\n",
      "label\n",
      "SUPPORTS           1156\n",
      "REFUTES            1156\n",
      "NOT ENOUGH INFO    1149\n",
      "Name: count, dtype: int64\n",
      "Valid set label distribution:\n",
      "label\n",
      "SUPPORTS           495\n",
      "REFUTES            495\n",
      "NOT ENOUGH INFO    488\n",
      "Name: count, dtype: int64\n",
      "Train set label distribution after balancing:\n",
      "label\n",
      "NOT ENOUGH INFO    1149\n",
      "REFUTES            1149\n",
      "SUPPORTS           1149\n",
      "Name: count, dtype: int64\n",
      "Valid set label distribution after balancing:\n",
      "label\n",
      "NOT ENOUGH INFO    488\n",
      "REFUTES            488\n",
      "SUPPORTS           488\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-25-03091368d446>:9: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train_sentEx = train_sentEx.groupby('label').apply(lambda x: x.sample(min_count)).reset_index(drop=True)\n",
      "<ipython-input-25-03091368d446>:11: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  valid_sentEx = valid_sentEx.groupby('label').apply(lambda x: x.sample(min_count)).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "# Show the distribution of labels\n",
    "print(f\"Train set label distribution:\")\n",
    "print(train_sentEx['label'].value_counts())\n",
    "print(f\"Valid set label distribution:\")\n",
    "print(valid_sentEx['label'].value_counts())\n",
    "\n",
    "# Balance the labels by reducing each to the minimum count\n",
    "min_count = min(train_sentEx['label'].value_counts())\n",
    "train_sentEx = train_sentEx.groupby('label').apply(lambda x: x.sample(min_count)).reset_index(drop=True)\n",
    "min_count = min(valid_sentEx['label'].value_counts())\n",
    "valid_sentEx = valid_sentEx.groupby('label').apply(lambda x: x.sample(min_count)).reset_index(drop=True)\n",
    "\n",
    "print(f\"Train set label distribution after balancing:\")\n",
    "print(train_sentEx['label'].value_counts())\n",
    "print(f\"Valid set label distribution after balancing:\")\n",
    "print(valid_sentEx['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TO5Dt_5vCGqG"
   },
   "outputs": [],
   "source": [
    "def module_2_semi_supervised_distillation(claim, documents, entities, keywords, sim_thresh=0.2, verbose=0, debug=False):\n",
    "    \"\"\"\n",
    "    Module 2: Semi-supervised Distillation for sentence extraction.\n",
    "\n",
    "    Args:\n",
    "        claim (str): The input claim.\n",
    "        documents (list of str): List of retrieved documents (full text).\n",
    "        entity (str):  The main entity in the claim (or None if not found).\n",
    "        keywords (list of str):  Top keywords from the claim.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (list of str, str):  A list of extracted evidence sentences and the exit status (\"NOT ENOUGH INFO\" or \"OK\").\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    extracted_sentences = []\n",
    "    max_iterations = min(5, len(documents)) # This will be determined later with actual number of sentences across all docs\n",
    "    if debug:\n",
    "      print(f\"DEBUG 2.1.1:\")\n",
    "      print(f\"\\tNumber of documents: {len(documents)}\")\n",
    "      print(f\"\\tMax iterations: {max_iterations}\")\n",
    "      print(\"-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\")\n",
    "\n",
    "    \"\"\"\n",
    "    if entities == []:\n",
    "      if verbose == 1:\n",
    "        print(\"No entities found, early exiting...\")\n",
    "        return [], \"NOT ENOUGH INFO\" # Early exit if no entity\n",
    "    \"\"\"\n",
    "\n",
    "    # convert entities to string\n",
    "    if not entities:\n",
    "      entities = \"\"\n",
    "      # prompts\n",
    "      prompts = {\n",
    "            \"no_entities\": f\"Retrieve sentences from the list that either support or refute the following claim. Specifically, focus on sentences mentioning {keywords}. Order the sentences by relevance, highest first, and return a list separated by the return character. If there are no relevant sentences, respond with 'NOT ENOUGH INFO'. DO NOT CREATE ANY SENTENCES THAT ARE NOT IN THE PROVIDED LIST.\",\n",
    "            \"no_entities_followup\": f\"You didn’t find enough sentences. Find additional (new) sentences that are relevant to key points in the claim. Order the sentences by relevance, highest first, and return a list separated by the return character. If there are no relevant sentences, respond with 'NOT ENOUGH INFO'. DO NOT CREATE ANY SENTENCES THAT ARE NOT IN THE PROVIDED LIST.\",\n",
    "        }\n",
    "    else:\n",
    "      entities = \", \".join(entities)\n",
    "      # prompts\n",
    "      prompts = {\n",
    "            \"init\": f\"Retrieve sentences from the list that either support or refute the following claim. Specifically, focus on sentences mentioning {entities} or {keywords}. Order the sentences by relevance, highest first, and return a list separated by the return character. If there are no relevant sentences, respond with 'NOT ENOUGH INFO'. DO NOT CREATE ANY SENTENCES THAT ARE NOT IN THE PROVIDED LIST.\",\n",
    "            \"followup\": f\"You didn’t find enough sentences. Find additional (new) sentences that that are relevant to key points in the claim. Order the sentences by relevance, highest first, and return a list separated by the return character. If there are no relevant sentences, respond with 'NOT ENOUGH INFO'. DO NOT CREATE ANY SENTENCES THAT ARE NOT IN THE PROVIDED LIST.\",\n",
    "      }\n",
    "      if debug:\n",
    "        print(f\"DEBUG 2.1.2:\")\n",
    "        print(f\"\\tClaim: {claim}\")\n",
    "        print(f\"\\tEntities: {entities}\")\n",
    "        print(f\"\\tKeywords: {keywords}\")\n",
    "        print(\"-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\")\n",
    "\n",
    "    for iteration in range(max_iterations):\n",
    "            # 2.2 Prompt Selection (no agent - programmatic)\n",
    "        if iteration == 0:\n",
    "            prompt = prompts[\"init\"]\n",
    "        else:\n",
    "            prompt = prompts[\"followup\"]\n",
    "\n",
    "\n",
    "        # 2.3 Sentence Extraction (with pre-filtering)\n",
    "        #filtered_text = sliding_window_filter(documents, entities, keywords)\n",
    "        # Simply form the list of document strings into one string joined by \\n for now\n",
    "        filtered_text = \"\\n\".join(documents)\n",
    "        if debug:\n",
    "          print(f\"DEBUG 2.3.1:\")\n",
    "          print(f\"\\tFiltered text: {filtered_text}\")\n",
    "          print(\"-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\")\n",
    "        new_sentences = extract_sentences_with_llm(claim, filtered_text, prompt, debug)\n",
    "\n",
    "        if debug:\n",
    "          print(f\"DEBUG 2.3.3:\")\n",
    "          print(f\"\\tNumber of retrieved sentences on iteration {iteration}: {len(new_sentences)}\")\n",
    "          print(f\"\\tNew sentences: {new_sentences}\")\n",
    "          print(\"-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\")\n",
    "\n",
    "        if new_sentences == [\"NOT ENOUGH INFO\"]:  # LLM found no relevant sentences\n",
    "            if iteration == 1:  # No sentences found in the second iteration\n",
    "                if verbose == 1:\n",
    "                  print(f\"2.3: LLM found no relevant sentences in the second iteration, early exiting...\")\n",
    "                return [], \"NOT ENOUGH INFO\"  # Early exit if no relevant sentences in first pass.\n",
    "            elif iteration > 1:\n",
    "              if verbose:\n",
    "                print(f\"2.3: LLM claims to find NOT ENOUGH INFO, early exiting...\")\n",
    "                print(\"-------------------------------------------------------------\")\n",
    "              return [], \"NOT ENOUGH INFO\"  # Early exit if no relevant sentences in second pass.\n",
    "            else:\n",
    "              if verbose:\n",
    "                print(f\"2.3: LLM claims to find NOT ENOUGH INFO on first iteration (focusing on entities), continuing...\")\n",
    "                print(\"-------------------------------------------------------------\")\n",
    "                continue\n",
    "\n",
    "\n",
    "\n",
    "        # 2.4 Similarity Comparison and Thresholding\n",
    "        new_sentences = similarity_thresholding(claim, new_sentences, sim_thresh, True)\n",
    "\n",
    "        extracted_sentences.extend(new_sentences)  # Add new sentences to the list\n",
    "        if debug:\n",
    "          print(f\"DEBUG 2.4.2:\")\n",
    "          print(f\"\\tAdded {len(new_sentences)} new sentences on iteration {iteration}.\")\n",
    "          print(f\"\\tTotal extracted sentences: {len(extracted_sentences)}\")\n",
    "          print(f\"\\tExtracted sentences: {extracted_sentences}\")\n",
    "          print(\"-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\")\n",
    "        if len(extracted_sentences) >= min(max_iterations, 5):\n",
    "            if verbose == 1:\n",
    "                print(f\"2.4: Found enough sentences, exiting...\")\n",
    "                print(\"-------------------------------------------------------------\")\n",
    "            break  # Exit if enough sentences are found.\n",
    "\n",
    "    if verbose == 1:\n",
    "      print(f\"\\tExtracted sentences: {extracted_sentences}\")\n",
    "      print(f\"\\tNumber of extracted sentences: {len(extracted_sentences)}\")\n",
    "      print(f\"\\tMax iterations: {max_iterations}\")\n",
    "      print(\"-------------------------------------------------------------\")\n",
    "\n",
    "    #Check for final early exit\n",
    "    if len(extracted_sentences) < 5 and max_iterations > 5: #Pilot study will determine best practices here\n",
    "      if verbose == 1:\n",
    "        print(f\"2.4: REACHED MAX ITERATIONS, BUT FEWER THAN 5 SENTENCES FOUND, EARLY EXITING WITH 'NOT ENOUGH INFO'\")\n",
    "        print(\"-------------------------------------------------------------\")\n",
    "      return [], \"NOT ENOUGH INFO\"  # Not enough relevant sentences found after iterations\n",
    "\n",
    "    return extracted_sentences, \"OK\"\n",
    "\n",
    "\n",
    "\n",
    "def sliding_window_filter(documents, entity, keywords, debug=False):\n",
    "    \"\"\"\n",
    "    Performs sliding window filtering based on entity and keywords.\n",
    "\n",
    "    Args:\n",
    "        documents (list of str):  List of documents.\n",
    "        entity (str): The entity to match.\n",
    "        keywords (list of str): The keywords to match.\n",
    "\n",
    "    Returns:\n",
    "        str: Concatenated filtered text.\n",
    "    \"\"\"\n",
    "    # TODO: Implement sliding window filtering logic (using NLTK, spaCy, etc.).\n",
    "    # This function should concatenate sentences from windows that contain entity/keywords.\n",
    "    filtered_sentences = []\n",
    "\n",
    "    for document in documents:\n",
    "        sentences = nltk.sent_tokenize(document)\n",
    "        window_size = 3  # Experiment with different window sizes.\n",
    "        for i in range(len(sentences) - window_size + 1):\n",
    "            window = sentences[i : i + window_size]\n",
    "            window_text = \" \".join(window)\n",
    "            if entity in window_text and any(keyword in window_text for keyword in keywords):\n",
    "                filtered_sentences.extend(window) # Add matching sentences from window to filtered sentences\n",
    "\n",
    "    filtered_text = \"\\n\".join(filtered_sentences)\n",
    "    return filtered_text\n",
    "\n",
    "\n",
    "\n",
    "def extract_sentences_with_llm(claim, filtered_text, prompt, debug=False):\n",
    "    \"\"\"\n",
    "    Extracts sentences using an LLM.\n",
    "\n",
    "    Args:\n",
    "        claim (str): The input claim.\n",
    "        filtered_text (str): The pre-filtered text.\n",
    "        prompt (str): The prompt for the LLM.\n",
    "\n",
    "    Returns:\n",
    "        list of str: Extracted sentences.\n",
    "    \"\"\"\n",
    "    # TODO: Implement LLM interaction (using OpenAI API, etc.).\n",
    "    # Use the provided prompt and filtered_text to extract sentences with the LLM.\n",
    "\n",
    "    if debug:\n",
    "      print(f\"DEBUG 2.3.2:\")\n",
    "      print(f\"\\tPrompt: {prompt}\")\n",
    "      print(f\"\\tFiltered text: {filtered_text}\")\n",
    "      print(\"-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\")\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini-2024-07-18\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that extracts evidence sentences.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"{prompt}\\nClaim: {claim}\\nText: {filtered_text}\"},\n",
    "        ],\n",
    "        max_tokens=512,  # Adjust as needed\n",
    "        n=1,\n",
    "        stop=None,  # Or a suitable stop sequence\n",
    "        temperature=0.9,  # Adjust as needed\n",
    "    )\n",
    "    extracted_sentences_raw = response.choices[0].message.content\n",
    "    extracted_sentences = extracted_sentences_raw.split('\\n')  # Assuming sentences are separated by newlines\n",
    "\n",
    "    return extracted_sentences\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def similarity_thresholding(claim, sentences, threshold=0, debug=False):\n",
    "    \"\"\"\n",
    "    Thresholds sentences based on similarity to the claim.\n",
    "\n",
    "    Args:\n",
    "        claim (str): The input claim.\n",
    "        sentences (list of str): The sentences to threshold.\n",
    "\n",
    "    Returns:\n",
    "        list of str:  The sentences that meet the similarity threshold.\n",
    "    \"\"\"\n",
    "    # TODO: Implement similarity calculations (ROUGE, TF-IDF, cosine similarity).\n",
    "    # Return only the sentences that exceed the defined threshold(s).\n",
    "\n",
    "    filtered_sentences = []\n",
    "\n",
    "    vectorizer = TfidfVectorizer()  # Initialize TF-IDF vectorizer\n",
    "    claim_tfidf = vectorizer.fit_transform([claim])\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
    "\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_tfidf = vectorizer.transform([sentence])\n",
    "        cosine_sim = cosine_similarity(claim_tfidf, sentence_tfidf)[0][0]\n",
    "        rouge_scores = scorer.score(claim, sentence)\n",
    "        rouge1_score = rouge_scores['rouge1'].fmeasure  # Example: Use ROUGE-1 F1-score\n",
    "\n",
    "        if debug:\n",
    "          print(f\"DEBUG 2.4.1:\")\n",
    "          print(f\"\\tClaim: {claim}\")\n",
    "          print(f\"\\tSentence: {sentence}\")\n",
    "          print(f\"\\tCosine Similarity: {cosine_sim}\")\n",
    "\n",
    "        if cosine_sim >= threshold and rouge1_score >= threshold:  # Example: Combine cosine similarity and ROUGE\n",
    "            filtered_sentences.append(sentence)\n",
    "\n",
    "    return filtered_sentences\n",
    "\n",
    "\n",
    "# Entity and keywords extraction function\n",
    "def extract_entities(text):\n",
    "    \"\"\"\n",
    "    Extracts entities from the text using NLTK's Named Entity Chunker.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "\n",
    "    Returns:\n",
    "        list of str: List of extracted entities.\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    named_entities = ne_chunk(tagged_tokens)\n",
    "\n",
    "    entities = []\n",
    "    for subtree in named_entities:\n",
    "        if isinstance(subtree, Tree):\n",
    "            entity = \" \".join([word for word, tag in subtree.leaves()])\n",
    "            entities.append(entity)\n",
    "\n",
    "    return entities\n",
    "\n",
    "\n",
    "# Keyword extraction function\n",
    "def extract_keywords(text):\n",
    "    \"\"\"\n",
    "    Extracts keywords from the text using TF-IDF.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "\n",
    "    Returns:\n",
    "        list of str: List of extracted keywords.\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=10)  # Adjust max_features as needed\n",
    "    tfidf_matrix = vectorizer.fit_transform([text])\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    dense = tfidf_matrix.todense()\n",
    "    denselist = dense.tolist()\n",
    "    # Convert the list to a NumPy array to use argsort()\n",
    "\n",
    "    dense_array = np.array(denselist[0])\n",
    "    keywords = [feature_names[i] for i in dense_array.argsort()[-5:]]  # Get top 5 keywords\n",
    "\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qz3SVu5uhcxx"
   },
   "source": [
    "## Module 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1743791658581,
     "user": {
      "displayName": "Henry Zelenak",
      "userId": "01809909909045225068"
     },
     "user_tz": 240
    },
    "id": "PVjj_l5QDl67",
    "outputId": "31af5969-34b4-4e45-fa2d-9e25780d0909"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"[('Despite their San Francisco Bay Area origins , they played in a Southern rock style , with lyrics about bayous , catfish , the Mississippi River , and other popular elements of Southern United States iconography , as well as political and socially-conscious lyrics about topics including the Vietnam War .', 'Creedence_Clearwater_Revival', 3, ['Vietnam War', 'Southern rock', 'San Francisco Bay Area', 'Opposition to United States involvement in the Vietnam War', 'rock', 'Mississippi River', 'rock music']), ('Creedence Clearwater Revival , often informally abbreviated to Creedence or CCR , was an American rock band active in the late 1960s and early 1970s .', 'Creedence_Clearwater_Revival', 0, ['rock', 'rock music']), ('Their musical style encompassed the roots rock , swamp rock , and blues rock genres .', 'Creedence_Clearwater_Revival', 2, ['roots rock', 'rock', 'blues rock', 'rock music', 'swamp rock'])]\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the evidence_setences\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "train_sentEx['evidence_sentences'][1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bK5axsjBn_Fw"
   },
   "outputs": [],
   "source": [
    "# Module 1: Document retrieval from wikipedia based on keywords, entities, and claim\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "\n",
    "def query_generator(claim, keywords, entities, debug=False):\n",
    "    \"\"\"\n",
    "    Generates a query for Wikipedia based on the claim, keywords, and entities.\n",
    "\n",
    "    Args:\n",
    "        claim (str): The input claim.\n",
    "        keywords (list of str): The keywords to include in the query.\n",
    "        entities (list of str): The entities to include in the query.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated query.\n",
    "    \"\"\"\n",
    "    prompt = f\"Generate a Wikipedia query based on the claim: '{claim}'. Include keywords: {', '.join(keywords)} and entities: {', '.join(entities)}. Respond only with the query as a string on a single line.\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini-2024-07-18\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that generates Wikipedia queries.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        max_tokens=512,  # Adjust as needed\n",
    "        n=1,\n",
    "        stop=None,  # Or a suitable stop sequence\n",
    "        temperature=0.9,  # Adjust as needed\n",
    "    )\n",
    "    query = response.choices[0].message.content\n",
    "    return query\n",
    "\n",
    "def retrieve_documents_from_wikipedia(claim, keywords, entities, debug=False):\n",
    "    \"\"\"\n",
    "    Retrieves documents from Wikipedia based on the claim, keywords, and entities.\n",
    "\n",
    "    Args:\n",
    "        claim (str): The input claim.\n",
    "        keywords (list of str): The keywords to include in the query.\n",
    "        entities (list of str): The entities to include in the query.\n",
    "\n",
    "    Returns:\n",
    "        list of str: Retrieved documents.\n",
    "    \"\"\"\n",
    "    query = query_generator(claim, keywords, entities, debug=False)\n",
    "    if debug:\n",
    "      print(f\"DEBUG 1.2.1:\")\n",
    "      print(f\"\\tQuery: {query}\")\n",
    "      print(\"-_-_-_-_-_-_-_-_-_-_-_-_-_-\")\n",
    "    # Perform a Wikipedia search\n",
    "    search_url = f\"https://en.wikipedia.org/w/index.php?search={requests.utils.quote(query)}\"\n",
    "    response = requests.get(search_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    # Extract links to Wikipedia pages\n",
    "    links = soup.find_all('a', href=re.compile(r'^/wiki/'))\n",
    "    documents = []\n",
    "    # Get just the introduction of each page\n",
    "    for link in links:\n",
    "        page_url = f\"https://en.wikipedia.org{link['href']}\"\n",
    "        response = requests.get(page_url)\n",
    "        data = response.json\n",
    "        if 'parse' in data:\n",
    "          page_content = data['parse']['text']['*']\n",
    "          # Extract the introduction section (the first paragraph element)\n",
    "          intro_section = re.search(r'<p>(.*?)</p>', page_content, re.DOTALL)\n",
    "          if intro_section:\n",
    "            intro_text = intro_section.group(1) # Get the text inside the first <p> tag\n",
    "            # Remove HTML tags\n",
    "            intro_text = re.sub(r'<.*?>', '', intro_text)\n",
    "            # Remove references\n",
    "            intro_text = re.sub(r'\\[.*?\\]', '', intro_text)\n",
    "            # Remove digits that are preceded by any letter, period, colon, semicolon, endash (–), and emdash(—) and followed by a space\n",
    "            intro_text = re.sub(r'(?<=[a-zA-Z0-9\\.\\:\\;\\–\\—])\\d+(?=\\s)', '', intro_text)\n",
    "            # Convert encoded html entities to unicode (e.g., &amp; to &)\n",
    "            intro_text = re.sub(r'&[a-zA-Z0-9#]+;', '', intro_text)\n",
    "            # Remove extra whitespace\n",
    "            intro_text = re.sub(r'\\s+', ' ', intro_text).strip()\n",
    "            documents.append(intro_text)\n",
    "        else:\n",
    "            print(f\"Error: No parse data found for {page_url}\")\n",
    "    if debug:\n",
    "      print(f\"DEBUG 1.2.2:\")\n",
    "      print(f\"\\tNumber of documents retrieved: {len(documents)}\")\n",
    "      print(\"-_-_-_-_-_-_-_-_-_-_-_-_-\")\n",
    "    return documents\n",
    "\n",
    "index = 0\n",
    "def get_test_claim(df, mode, verbose=0, debug=False):\n",
    "    \"\"\"\n",
    "    In \"test\" mode, gets data for a claim by matching a row in the df (which contains the claim and the wikipedia text data) to the JSONL object\n",
    "    (which contains the claim and the evidence references) by the claim, returning the claim, label, evidence sentences, documents, and evidence references.\n",
    "\n",
    "    In 'live' mode, gets data for a claim by generating a query and retrieving documents from Wikipedia. THIS MODE IS NOT COMPATIBLE WITH FEVER SCORING.\n",
    "\n",
    "    EXAMPLE:\n",
    "    {\"id\": 113501, \"verifiable\": \"NOT VERIFIABLE\", \"label\": \"NOT ENOUGH INFO\", \"claim\": \"Grease had bad reviews.\", \"evidence\": [[[133128, null, null, null]]]}\n",
    "    {\"id\": 163803, \"verifiable\": \"VERIFIABLE\", \"label\": \"SUPPORTS\", \"claim\": \"Ukrainian Soviet Socialist Republic was a founding participant of the UN.\", \"evidence\": [[[296950, 288668, \"Ukrainian_Soviet_Socialist_Republic\", 7]], [[298602, 290067, \"Ukrainian_Soviet_Socialist_Republic\", 7], [298602, 290067, \"United_Nations\", 0]], [[300696, 291816, \"Ukrainian_Soviet_Socialist_Republic\", 7]], [[344347, 327887, \"Ukrainian_Soviet_Socialist_Republic\", 7]], [[344994, 328433, \"Ukrainian_Soviet_Socialist_Republic\", 7]], [[344997, 328435, \"Ukrainian_Soviet_Socialist_Republic\", 7]]]}\n",
    "    {\"id\": 70041, \"verifiable\": \"VERIFIABLE\", \"label\": \"SUPPORTS\", \"claim\": \"2 Hearts is a musical composition by Minogue.\", \"evidence\": [[[225394, 230056, \"2_Hearts_-LRB-Kylie_Minogue_song-RRB-\", 0]], [[317953, 306972, \"2_Hearts_-LRB-Kylie_Minogue_song-RRB-\", 0]], [[319638, 308345, \"2_Hearts_-LRB-Kylie_Minogue_song-RRB-\", 0]], [[319643, 308348, \"2_Hearts_-LRB-Kylie_Minogue_song-RRB-\", 0]]]}\n",
    "    {\"id\": 202314, \"verifiable\": \"VERIFIABLE\", \"label\": \"REFUTES\", \"claim\": \"The New Jersey Turnpike has zero shoulders.\", \"evidence\": [[[238335, 240393, \"New_Jersey_Turnpike\", 15]]]}\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    global index\n",
    "    claim = df.iloc[index]['claim']\n",
    "    documents = df.iloc[index]['full_text']\n",
    "    documents = documents.split('\\n')\n",
    "    label = df.iloc[index]['label']\n",
    "    keywords = extract_keywords(claim)  # Extract keywords from the claim\n",
    "    evidence_items = df.iloc[index]['evidence_sentences']\n",
    "    # Evidence items are in the format (sentence, page_title, sentence_id, entities[entity1, entity2, ...])\n",
    "    '''\n",
    "    [('Despite their San Francisco Bay Area origins , they played in a Southern rock style , with lyrics about bayous , catfish , the Mississippi River , and other popular elements of Southern United States iconography , as well as political and socially-conscious lyrics about topics including the Vietnam War .', 'Creedence_Clearwater_Revival', 3, ['Vietnam War', 'Southern rock', 'San Francisco Bay Area', 'Opposition to United States involvement in the Vietnam War', 'rock', 'Mississippi River', 'rock music']), ('Creedence Clearwater Revival , often informally abbreviated to Creedence or CCR , was an American rock band active in the late 1960s and early 1970s .', 'Creedence_Clearwater_Revival', 0, ['rock', 'rock music']), ('Their musical style encompassed the roots rock , swamp rock , and blues rock genres .', 'Creedence_Clearwater_Revival', 2, ['roots rock', 'rock', 'blues rock', 'rock music', 'swamp rock'])]\n",
    "    '''\n",
    "    \n",
    "    # Return essential information (we only need the documents, keywords, entities, claim, and label for the NOT ENOUGH INFO case, since we don't need to extract evidence sentences)\n",
    "    if label == \"NOT ENOUGH INFO\":\n",
    "        evidence_items = []\n",
    "        entities = extract_entities(claim)  # Extract entities from the claim\n",
    "        if verbose == 1:\n",
    "          print(f\"-------------------------------------------------------------\")\n",
    "          print(f\"In get_test_claim, running in {mode} mode...\")\n",
    "          print(f\"Claim: {claim}\")\n",
    "          print(f\"Label: {label}\")\n",
    "          print(f\"Evidence items: {evidence_items}\")\n",
    "          print(f\"Documents: {documents}\")\n",
    "          print(f\"Entities: {entities}\")\n",
    "          print(f\"Keywords: {keywords}\")\n",
    "          print(f\"-------------------------------------------------------------\")\n",
    "        return claim, label, evidence_items, documents, keywords, entities\n",
    "  \n",
    "    entities = []\n",
    "    for item in evidence_items:\n",
    "        entities.extend(item[3])\n",
    "    entities = list(set(entities))  # Remove duplicates\n",
    "\n",
    "    if verbose == 1:\n",
    "        print(f\"-------------------------------------------------------------\")\n",
    "        print(f\"In get_test_claim, running in {mode} mode...\")\n",
    "        print(f\"Claim: {claim}\")\n",
    "        print(f\"Label: {label}\")\n",
    "        print(f\"Evidence items: {evidence_items}\")\n",
    "        print(f\"Documents: {documents}\")\n",
    "        print(f\"Entities: {entities}\")\n",
    "        print(f\"Keywords: {keywords}\")\n",
    "        print(f\"-------------------------------------------------------------\")\n",
    "\n",
    "    if mode == 'live':\n",
    "        # Generate a query and retrieve documents from Wikipedia\n",
    "        documents = retrieve_documents_from_wikipedia(claim, keywords, entities, debug=debug)\n",
    "        if debug:\n",
    "          print(f\"DEBUG 1.2.3, Live mode:\")\n",
    "          print(f\"\\tDocuments: {documents}\")\n",
    "          print(\"-_-_-_-_-_-_-_-_-_-_-_-_-\")\n",
    "\n",
    "    elif mode == 'test':\n",
    "        # Use the documents from the df\n",
    "        if debug:\n",
    "          print(f\"DEBUG 1.2.4, Test mode:\")\n",
    "          print(f\"\\tDocuments: {documents}\")\n",
    "          print(\"-_-_-_-_-_-_-_-_-_-_-_-_-\")\n",
    "\n",
    "    return claim, label, evidence_items, documents, keywords, entities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C-2nuV15vpuQ"
   },
   "outputs": [],
   "source": [
    "# Shuffle the valid_sentEx df\n",
    "#valid_sentEx = valid_sentEx.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1864,
     "status": "ok",
     "timestamp": 1743635499265,
     "user": {
      "displayName": "Henry Zelenak",
      "userId": "01809909909045225068"
     },
     "user_tz": 240
    },
    "id": "FJ6Azxe8MHBB",
    "outputId": "5f84f2db-9714-4214-cb66-ab2ac112f07a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claim found in JSONL data: In the End was the only track on Hybrid Theory.\n",
      "-------------------------------------------------------------\n",
      "VERBOSE OUTPUT (2)\n",
      "Index: 1\n",
      "Claim: In the End was the only track on Hybrid Theory.\n",
      "Label: REFUTES\n",
      "Evidence sentences: It is the eighth track on their debut album Hybrid Theory ( 2000 ) and was released as the album 's fourth single October 9 , 2001 .\n",
      "Evidence references: [[[51567, 61334, 'In_the_End', 1]]]\n",
      "Document 0: \n",
      "Document 1: `` In the End '' is a song by American rock band Linkin Park . It is the eighth track on their debut album Hybrid Theory ( 2000 ) and was released as the album 's fourth single October 9 , 2001 . `` In the End '' is one of Linkin Park 's most recognizable and signature songs . It is the second most played song in all of the band 's live performances , after `` One Step Closer '' .   `` In the End '' received positive reviews by music critics , with most reviewers complimenting the song 's signature piano riff , as well as noting rapper Mike Shinoda 's vocal prominence in the song . `` In the End '' also achieved mainstream popularity , and was a commercial success upon release . The song reached the top ten of numerous worldwide music charts and reached number 2 on the Billboard Hot 100 , the band 's highest peak on the chart , as well as their first song that peaked within the Top 40 . It also reached number one on the Z100 Top 100 songs of 2002 countdown . This song also ranked at # 121 in Blender magazine 's The 500 Greatest Songs Since You Were Born . The song is Billboards second most played rock song of the decade . It was also remixed on Reanimation as `` Enth E ND '' . The music video of the song , directed by Nathan Cox and the band 's turntablist Joe Hahn , featured the band in a fantasy setting .   Chester Bennington , the band 's lead vocalist , initially disliked this song and did n't want it to be included on Hybrid Theory . \n",
      "Keywords: ['end', 'hybrid', 'theory', 'track']\n",
      "Entities: ['End', 'Hybrid Theory']\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "DEBUG 1.2.4:\n",
      "\tDocuments: ['', \"`` In the End '' is a song by American rock band Linkin Park . It is the eighth track on their debut album Hybrid Theory ( 2000 ) and was released as the album 's fourth single October 9 , 2001 . `` In the End '' is one of Linkin Park 's most recognizable and signature songs . It is the second most played song in all of the band 's live performances , after `` One Step Closer '' .   `` In the End '' received positive reviews by music critics , with most reviewers complimenting the song 's signature piano riff , as well as noting rapper Mike Shinoda 's vocal prominence in the song . `` In the End '' also achieved mainstream popularity , and was a commercial success upon release . The song reached the top ten of numerous worldwide music charts and reached number 2 on the Billboard Hot 100 , the band 's highest peak on the chart , as well as their first song that peaked within the Top 40 . It also reached number one on the Z100 Top 100 songs of 2002 countdown . This song also ranked at # 121 in Blender magazine 's The 500 Greatest Songs Since You Were Born . The song is Billboards second most played rock song of the decade . It was also remixed on Reanimation as `` Enth E ND '' . The music video of the song , directed by Nathan Cox and the band 's turntablist Joe Hahn , featured the band in a fantasy setting .   Chester Bennington , the band 's lead vocalist , initially disliked this song and did n't want it to be included on Hybrid Theory . \"]\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "DEBUG 2.1.1:\n",
      "\tNumber of documents: 2\n",
      "\tMax iterations: 2\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "DEBUG 2.1.2:\n",
      "\tClaim: In the End was the only track on Hybrid Theory.\n",
      "\tEntities: End, Hybrid Theory\n",
      "\tKeywords: ['end', 'hybrid', 'theory', 'track']\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "DEBUG 2.3.1:\n",
      "\tFiltered text: \n",
      "`` In the End '' is a song by American rock band Linkin Park . It is the eighth track on their debut album Hybrid Theory ( 2000 ) and was released as the album 's fourth single October 9 , 2001 . `` In the End '' is one of Linkin Park 's most recognizable and signature songs . It is the second most played song in all of the band 's live performances , after `` One Step Closer '' .   `` In the End '' received positive reviews by music critics , with most reviewers complimenting the song 's signature piano riff , as well as noting rapper Mike Shinoda 's vocal prominence in the song . `` In the End '' also achieved mainstream popularity , and was a commercial success upon release . The song reached the top ten of numerous worldwide music charts and reached number 2 on the Billboard Hot 100 , the band 's highest peak on the chart , as well as their first song that peaked within the Top 40 . It also reached number one on the Z100 Top 100 songs of 2002 countdown . This song also ranked at # 121 in Blender magazine 's The 500 Greatest Songs Since You Were Born . The song is Billboards second most played rock song of the decade . It was also remixed on Reanimation as `` Enth E ND '' . The music video of the song , directed by Nathan Cox and the band 's turntablist Joe Hahn , featured the band in a fantasy setting .   Chester Bennington , the band 's lead vocalist , initially disliked this song and did n't want it to be included on Hybrid Theory . \n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "DEBUG 2.3.2:\n",
      "\tPrompt: Retrieve sentences from the list that either support or refute the following claim. Specifically, focus on sentences mentioning End, Hybrid Theory or ['end', 'hybrid', 'theory', 'track']. Order the sentences by relevance, highest first, and return a list separated by the return character. If there are no relevant sentences, respond with 'NOT ENOUGH INFO'. DO NOT CREATE ANY SENTENCES THAT ARE NOT IN THE PROVIDED LIST.\n",
      "\tFiltered text: \n",
      "`` In the End '' is a song by American rock band Linkin Park . It is the eighth track on their debut album Hybrid Theory ( 2000 ) and was released as the album 's fourth single October 9 , 2001 . `` In the End '' is one of Linkin Park 's most recognizable and signature songs . It is the second most played song in all of the band 's live performances , after `` One Step Closer '' .   `` In the End '' received positive reviews by music critics , with most reviewers complimenting the song 's signature piano riff , as well as noting rapper Mike Shinoda 's vocal prominence in the song . `` In the End '' also achieved mainstream popularity , and was a commercial success upon release . The song reached the top ten of numerous worldwide music charts and reached number 2 on the Billboard Hot 100 , the band 's highest peak on the chart , as well as their first song that peaked within the Top 40 . It also reached number one on the Z100 Top 100 songs of 2002 countdown . This song also ranked at # 121 in Blender magazine 's The 500 Greatest Songs Since You Were Born . The song is Billboards second most played rock song of the decade . It was also remixed on Reanimation as `` Enth E ND '' . The music video of the song , directed by Nathan Cox and the band 's turntablist Joe Hahn , featured the band in a fantasy setting .   Chester Bennington , the band 's lead vocalist , initially disliked this song and did n't want it to be included on Hybrid Theory . \n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "DEBUG 2.3.3:\n",
      "\tNumber of retrieved sentences on iteration 0: 3\n",
      "\tNew sentences: [\"`` In the End '' is the eighth track on their debut album Hybrid Theory ( 2000 ) and was released as the album 's fourth single October 9 , 2001 .  \", \"Chester Bennington , the band 's lead vocalist , initially disliked this song and did n't want it to be included on Hybrid Theory .  \", '```']\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "DEBUG 2.4.1:\n",
      "\tClaim: In the End was the only track on Hybrid Theory.\n",
      "\tSentence: `` In the End '' is the eighth track on their debut album Hybrid Theory ( 2000 ) and was released as the album 's fourth single October 9 , 2001 .  \n",
      "\tCosine Similarity: 0.9381941874331423\n",
      "DEBUG 2.4.1:\n",
      "\tClaim: In the End was the only track on Hybrid Theory.\n",
      "\tSentence: Chester Bennington , the band 's lead vocalist , initially disliked this song and did n't want it to be included on Hybrid Theory .  \n",
      "\tCosine Similarity: 0.7216878364870323\n",
      "DEBUG 2.4.1:\n",
      "\tClaim: In the End was the only track on Hybrid Theory.\n",
      "\tSentence: ```\n",
      "\tCosine Similarity: 0.0\n",
      "DEBUG 2.4.2:\n",
      "\tAdded 3 new sentences on iteration 0.\n",
      "\tTotal extracted sentences: 3\n",
      "\tExtracted sentences: [\"`` In the End '' is the eighth track on their debut album Hybrid Theory ( 2000 ) and was released as the album 's fourth single October 9 , 2001 .  \", \"Chester Bennington , the band 's lead vocalist , initially disliked this song and did n't want it to be included on Hybrid Theory .  \", '```']\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "2.4: Found enough sentences, exiting...\n",
      "-------------------------------------------------------------\n",
      "\tExtracted sentences: [\"`` In the End '' is the eighth track on their debut album Hybrid Theory ( 2000 ) and was released as the album 's fourth single October 9 , 2001 .  \", \"Chester Bennington , the band 's lead vocalist , initially disliked this song and did n't want it to be included on Hybrid Theory .  \", '```']\n",
      "\tNumber of extracted sentences: 3\n",
      "\tMax iterations: 2\n",
      "-------------------------------------------------------------\n",
      "Extracted evidence: [\"`` In the End '' is the eighth track on their debut album Hybrid Theory ( 2000 ) and was released as the album 's fourth single October 9 , 2001 .  \", \"Chester Bennington , the band 's lead vocalist , initially disliked this song and did n't want it to be included on Hybrid Theory .  \", '```']\n",
      "Status: OK\n",
      "No matches found between extracted evidence and references.\n"
     ]
    }
   ],
   "source": [
    "#claim, documents_text, label = generate_claim_tab_data(valid_sentEx)  # Generate a claim and its associated documents for tabular data\n",
    "verbose = 1\n",
    "claim, label, evidence_items, documents_text, keywords, entities = get_test_claim(test_jsonl, valid_sentEx, mode='test', verbose=1, debug=True)\n",
    "\n",
    "# Convert documents to list of strings if it's a single string\n",
    "if isinstance(documents_text, str):\n",
    "    documents = documents_text.split('\\n')\n",
    "elif isinstance(documents_text, list):\n",
    "  documents = documents_text\n",
    "else:\n",
    "  raise Exception(f\"documents must be a list of strings, not {type(documents_text)}\")\n",
    "\n",
    "sim_thresh = 0.1\n",
    "extracted_evidence, status = module_2_semi_supervised_distillation(claim, documents, entities, keywords, sim_thresh, verbose=1, debug=True)\n",
    "\n",
    "print(f\"Extracted evidence: {extracted_evidence}\")\n",
    "print(f\"Status: {status}\")\n",
    "\n",
    "# Quick near-match function\n",
    "def near_match(a, b, threshold=0.8, verbose=0):\n",
    "  \"\"\"\n",
    "  Checks if two strings are similar based on a threshold.\n",
    "\n",
    "  Args:\n",
    "      a (str): The first string.\n",
    "      b (str): The second string.\n",
    "      threshold (float): The similarity threshold.\n",
    "\n",
    "  Returns:\n",
    "      bool: True if the strings are similar, False otherwise.\n",
    "  \"\"\"\n",
    "  sim = len(set(a.split()).intersection(set(b.split()))) / max(len(a.split()), len(b.split())) >= threshold\n",
    "  if verbose == 1:\n",
    "      print(f\"Comparing '{a}' with '{b}' at {threshold}: Similarity = {sim}\")\n",
    "  return sim\n",
    "\n",
    "# Match the evidence references with the extracted evidence\n",
    "evidence_refs = []\n",
    "if status == \"OK\":\n",
    "  # Check if the extracted evidence is not empty\n",
    "  if extracted_evidence:\n",
    "    # Iterate through the extracted evidence and evidence items\n",
    "    for sentence in extracted_evidence:\n",
    "        for item in evidence_items:\n",
    "            if near_match(sentence[0], item[0], threshold=0.8, verbose=1):\n",
    "                evidence_refs.append([None, None, item[2], item[3]])\n",
    "                break\n",
    "            else:\n",
    "                if verbose == 1:\n",
    "                    print(f\"Evidence item '{item[0]}' does not match extracted evidence '{sentence[0]}'\")\n",
    "  else:\n",
    "    print(f\"No extracted evidence found, despite status OK.\")\n",
    "else:\n",
    "  print(f\"Status is NOT OK, no evidence references found.\")\n",
    "  evidence_refs = []\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E7_3Fpx4hcxy"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Convert extracted evidence to FEVER format\n",
    "predicted_evidence = []\n",
    "for sentence in extracted_evidence:\n",
    "    for doc in documents:\n",
    "        if sentence in doc:  # or a more robust check if sentences are preprocessed\n",
    "            line_num = doc.split('\\n').index(sentence)  # Assumes sentences are split by newlines in doc\n",
    "            predicted_evidence.append([f\"page_{documents.index(doc)}\", line_num])\n",
    "            break  # Stop searching once sentence is found in a document\n",
    "\n",
    "# Construct FEVER prediction instance\n",
    "prediction_instance = {\n",
    "    \"predicted_label\": \"SUPPORTS\" if status == \"OK\" else \"NOT ENOUGH INFO\",  # Or REFUTES based on your logic\n",
    "    \"predicted_evidence\": predicted_evidence,\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "# Get evidence for the claim (adapt this based on your gold standard data format)\n",
    "actual_evidence = []\n",
    "gold_sentences = valid_sentEx[valid_sentEx[\"claim\"] == claim][\"evidence_sentences\"].iloc[0].split('\\n')\n",
    "for sentence in gold_sentences:\n",
    "    for doc in documents:\n",
    "        if sentence in doc:\n",
    "            line_num = doc.split('\\n').index(sentence)\n",
    "            actual_evidence.append([f\"page_{documents.index(doc)}\", line_num])\n",
    "            break\n",
    "\n",
    "# Construct the actual evidence (gold standard) dictionary:\n",
    "\n",
    "actual_instance = {\"label\": label, \"evidence\": [actual_evidence]}\n",
    "\n",
    "# Calculate FEVER score\n",
    "strict_score, label_accuracy, precision, recall, f1 = fever_score([prediction_instance], [actual_instance])\n",
    "fever_results = {\n",
    "    'strict_score': strict_score,\n",
    "    'label_accuracy': label_accuracy,\n",
    "    'precision': precision,\n",
    "    'recall': recall,\n",
    "    'f1': f1\n",
    "}\n",
    "\n",
    "print(\"FEVER Scores:\", fever_results)\n",
    "print(\"-------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "if status == \"OK\":\n",
    "    print(\"Extracted Evidence Sentences:\")\n",
    "    for sentence in extracted_evidence:\n",
    "        print('\\t' + sentence)\n",
    "else:\n",
    "    print(f\"Claim classification: {status}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PERzNad2ISk2"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "extracted_evidence, status = module_2_semi_supervised_distillation(claim, documents, entities, keywords, verbose=1, debug=True)\n",
    "\n",
    "if status == \"OK\":\n",
    "    print(\"Extracted Evidence Sentences:\")\n",
    "    for sentence in extracted_evidence:\n",
    "        print('\\t' + sentence)\n",
    "    print(f\"Claim classification: {status}\")\n",
    "    print(f\"Label: {label}\")\n",
    "else:\n",
    "    print(f\"Claim classification: {status}\")\n",
    "    print(f\"Label: {label}\")\n",
    "\n",
    "# Evaluation\n",
    "# Assuming you have a gold standard set of sentences for evaluation\n",
    "\n",
    "print(\"Evaluation Scores:\")\n",
    "scores, avg_score = scorer(documents, set(extracted_evidence))\n",
    "for sentence, score in scores.items():\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Scores: {score}\")\n",
    "    print(\"-------------------------------------------------------------\")\n",
    "print(f\"Average Score: {avg_score}\")  # Adjust based on your scoring logic"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
