{"cells":[{"cell_type":"markdown","metadata":{"id":"VQREkhWQcldO"},"source":["## 0. Imports and Introduction\n","<a href=\"https://drive.google.com/file/d/1TYzbcR3QkzQNZR0IX34vhDwzFcJcukHJ/view?usp=sharing\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TM9fLtjScldR"},"outputs":[],"source":["# Ensure fever-scorer is installed correctly (assuming previous steps worked)\n","!git clone -b release-v2.0 https://github.com/sheffieldnlp/fever-scorer.git\n","%cd fever-scorer\n","!pip install -r requirements.txt\n","\n","# Open /setup.py and add 'license=\"MIT\"' on line 12, then overwrite the file\n","import os\n","with open('setup.py', 'r') as f:\n","    lines = f.readlines()\n","    lines[11] = 'license=\"MIT\"\\n'\n","with open('setup.py', 'w') as f:\n","    f.writelines(lines)\n","    f.close()\n","    print(\"setup.py updated\")\n","!pip install .\n","%cd ..\n","\n","# Install necessary libraries\n","!pip install rouge-score sentence-transformers wikipedia\n","import pandas as pd\n","import nltk\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","from rouge_score import rouge_scorer\n","import openai\n","from openai import OpenAI\n","import numpy as np\n","from nltk import Tree, pos_tag, word_tokenize, ne_chunk\n","from nltk.corpus import stopwords\n","import numpy as np\n","from fever.scorer import fever_score # Import the FEVER scorer\n","from nltk import RegexpParser\n","import json\n","from sentence_transformers import SentenceTransformer, util\n","import requests\n","from bs4 import BeautifulSoup\n","import re\n","import ast\n","import time # For logging\n","import wikipedia # For fetching wikipedia content\n","import os\n","from tqdm import tqdm\n","tqdm.pandas()\n","import gc\n","from google.colab import userdata\n","\n","# Download necessary NLTK data files (ensure they are downloaded)\n","nltk.download('punkt', quiet=True)\n","nltk.download('averaged_perceptron_tagger_eng', quiet=True) # Added _eng suffix, common naming\n","nltk.download('maxent_ne_chunker', quiet=True)\n","nltk.download('words', quiet=True)\n","nltk.download('stopwords', quiet=True)\n","nltk.download('treebank', quiet=True)\n","nltk.download('punkt_tab', quiet=True)\n","nltk.download('maxent_ne_chunker_tab', quiet=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1999,"status":"ok","timestamp":1745011755707,"user":{"displayName":"Henry Zelenak","userId":"01809909909045225068"},"user_tz":240},"id":"rnJYlPRpzj4u","outputId":"684b19b2-17d8-46f7-eab2-8339d44c615e"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package treebank to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/treebank.zip.\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n","[nltk_data] Downloading package maxent_ne_chunker_tab to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n"]},{"data":{"text/plain":["True"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["nltk.download('treebank')\n","nltk.download('punkt_tab')\n","nltk.download('maxent_ne_chunker_tab')"]},{"cell_type":"markdown","metadata":{"id":"KBFL9QEacldT"},"source":["## 1. Add data sources & Setup\n","<a id=\"1\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rjIFEGvKj9Ut"},"outputs":[],"source":["del OpenAI"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fc1unvgakAKH"},"outputs":[],"source":["del openai"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lAXIZtTlj6yC"},"outputs":[],"source":["import openai\n","from openai import OpenAI"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yZ5pN36Rjr8B"},"outputs":[],"source":["from google.colab import userdata\n","api_key = userdata.get('openaikey')\n","client = OpenAI(api_key=api_key)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3676,"status":"ok","timestamp":1745008308928,"user":{"displayName":"Henry Zelenak","userId":"01809909909045225068"},"user_tz":240},"id":"kepg2KTQcldT","outputId":"8c49c6e0-ae2f-402d-d4c0-6131ab59588d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","OpenAI API key found. Client initialized.\n","Loaded 9999 items from /content/drive/My Drive/SUNY_Poly_DSA598/datasets/FEVER/paper_test.jsonl\n"]}],"source":["\n","# Mount google drive (if using Colab)\n","try:\n","    from google.colab import drive, userdata\n","    drive.mount('/content/drive')\n","    # Adjust path as needed\n","    BASE_DIR = '/content/drive/My Drive/SUNY_Poly_DSA598/'\n","    DATA_DIR = os.path.join(BASE_DIR, 'datasets/FEVER/')\n","    # Ensure the directory exists\n","    # os.makedirs(DATA_DIR, exist_ok=True)\n","\n","    # Set API Key securely\n","    api_key = userdata.get('openaikey')\n","\n","    # Assuming data files are in DATA_DIR after mounting\n","    test_path = \"/content/drive/My Drive/SUNY_Poly_DSA598/datasets/FEVER/paper_test.jsonl\" # Explicit path example\n","\n","except ModuleNotFoundError:\n","    print(\"Not running in Colab or google libraries not found. Assuming local setup.\")\n","    # Set BASE_DIR, DATA_DIR, and API Key for local execution\n","    # BASE_DIR = '.'\n","    # DATA_DIR = './datasets/FEVER/'\n","    # test_path = os.path.join(DATA_DIR, \"paper_test.jsonl\")\n","    # api_key = os.environ.get(\"OPENAI_API_KEY\") # Example: Load from environment variable\n","\n","    # Fallback path if not in Colab drive structure\n","    if not os.path.exists(\"./datasets/FEVER/paper_test.jsonl\"):\n","         print(\"Warning: paper_test.jsonl not found at default location. Please adjust paths.\")\n","         # Provide a default dummy path or raise error\n","         test_path = \"paper_test.jsonl\" # Assume it's in current dir if not found\n","    else:\n","        test_path = \"./datasets/FEVER/paper_test.jsonl\"\n","\n","\n","# Initialize OpenAI client\n","if api_key:\n","    query_client = OpenAI(api_key=api_key) # Will be reinitialized later on each call\n","    sentEx_client = OpenAI(api_key=api_key) # Will be reinitialized later on each call\n","    rephrase_client = OpenAI(api_key=api_key)\n","    nli_client = OpenAI(api_key=api_key)\n","    print(\"OpenAI API key found. Client initialized.\")\n","else:\n","    print(\"ERROR: OpenAI API key not found. Please set it up.\")\n","    # Handle the absence of the API key (e.g., exit or run in offline mode if possible)\n","    exit() # Or raise an exception\n","\n","def load_jsonl(file_path, encoding='utf-8'):\n","    \"\"\"Loads a JSON Lines file into a list of Python objects.\"\"\"\n","    data = []\n","    try:\n","        with open(file_path, 'r', encoding=encoding) as f:\n","            for line in f:\n","                try:\n","                    data.append(json.loads(line))\n","                except json.JSONDecodeError:\n","                    print(f\"Warning: Skipping invalid JSON line in {file_path}: {line.strip()}\")\n","    except FileNotFoundError:\n","        print(f\"ERROR: File not found at {file_path}\")\n","        return None # Return None or empty list on error\n","    return data\n","\n","# Load test dataset\n","test_data = load_jsonl(test_path)\n","if test_data is None:\n","    print(\"Exiting due to missing test data file.\")\n","    exit()\n","\n","print(f\"Loaded {len(test_data)} items from {test_path}\")\n","\n","# Initialize SBERT model once\n","### TODO: Fine-tune sBERT on on the claim and wikipedia article intro text with the evidence sentences as the target, sBERT is also trained on the wikipedia URL by appending it to the end of each origin sentence\n","sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n","# More performant model\n","#sbert_model = SentanceTransformer('all-mpnet-base-v2')"]},{"cell_type":"markdown","metadata":{"id":"iYkfA_GJcldU"},"source":["## 2. Helper Functions (Entity/Keyword Extraction, Near Match)\n","<a id=\"2\"></a>"]},{"cell_type":"code","execution_count":155,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1745045120585,"user":{"displayName":"Henry Zelenak","userId":"01809909909045225068"},"user_tz":240},"id":"EJNdmjuicldU"},"outputs":[],"source":["# --- Set up Wikipedia API ---\n","# --- Entity and Keyword Extraction ---\n","# (Keep these as they are, they are used before Module 1)\n","stop_words = set(stopwords.words('english'))\n","\n","def extract_entities(text):\n","    \"\"\"Extracts named entities using NLTK.\"\"\"\n","    tokens = word_tokenize(text)\n","    # Optional: filter stop words before POS tagging if desired\n","    # tokens = [word for word in tokens if word.lower() not in stop_words]\n","    tagged_tokens = pos_tag(tokens)\n","    named_entities = ne_chunk(tagged_tokens)\n","    entities = []\n","    for subtree in named_entities:\n","        if isinstance(subtree, Tree):\n","            # Improve entity extraction: filter by common NE types if needed\n","            # if hasattr(subtree, 'label') and subtree.label() in ['PERSON', 'ORGANIZATION', 'GPE', 'LOCATION']:\n","            entity = \" \".join([word for word, tag in subtree.leaves()])\n","            entities.append(entity)\n","    # Simple post-processing: remove duplicates and potentially filter short/generic entities\n","    entities = sorted(list(set(entities)), key=len, reverse=True) # Prioritize longer entities\n","    return entities\n","\n","def extract_keywords(text, max_keywords=5):\n","    \"\"\"Extracts keywords using TF-IDF.\"\"\"\n","    try:\n","        vectorizer = TfidfVectorizer(stop_words='english', max_features=50) # Use more features initially\n","        tfidf_matrix = vectorizer.fit_transform([text])\n","        feature_names = vectorizer.get_feature_names_out()\n","        # Get scores for the single document\n","        scores = tfidf_matrix.toarray().flatten()\n","        # Get indices of top N scores\n","        top_indices = scores.argsort()[-max_keywords:][::-1]\n","        keywords = [feature_names[i] for i in top_indices]\n","        return keywords\n","    except ValueError:\n","        # Handle case where text might be too short or only contains stop words\n","        return []\n","\n","\n","# --- Near Match Function ---\n","def near_match(a, b, threshold=0.9, verbose=0):\n","    \"\"\"\n","    Checks if two strings are similar based on Jaccard similarity of words.\n","    Improved robustness for empty strings.\n","    \"\"\"\n","    if not a or not b: # Handle empty strings\n","        return False\n","    set_a = set(a.lower().split())\n","    set_b = set(b.lower().split())\n","    intersection = len(set_a.intersection(set_b))\n","    union = len(set_a.union(set_b))\n","    if union == 0: # Both strings only contained whitespace or were identical empties\n","        return True if a == b else False # Match if identical, else False\n","    sim = intersection / union # Jaccard similarity\n","\n","    if verbose >= 1:\n","        print(f\"Comparing:\\n  A: '{a}'\\n  B: '{b}'\\n  Similarity: {sim:.4f} (Threshold: {threshold}) -> Match: {sim >= threshold}\")\n","    return sim >= threshold"]},{"cell_type":"markdown","metadata":{"id":"zpdfYw0fcldV"},"source":["## 3. Module 1: Document Retrieval\n","<a id=\"3\"></a>"]},{"cell_type":"code","execution_count":165,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1745045696500,"user":{"displayName":"Henry Zelenak","userId":"01809909909045225068"},"user_tz":240},"id":"NpxmZJQzcldV"},"outputs":[],"source":["# --- Module 1: Document Retrieval ---\n","\n","def query_generator(claim, keywords, entities, max_pages_to_fetch, temp=0.3, debug=False):\n","    \"\"\"\n","    **UPDATED:** Simulates an entity->URL model.\n","    Generates potential Wikipedia page titles based on extracted entities.\n","    Currently uses the entities directly, formatted as potential titles.\n","    A more advanced simulation could involve an LLM call.\n","\n","    Args:\n","        claim (str): The input claim (context).\n","        keywords (list of str): Keywords (less emphasis now).\n","        entities (list of str): The primary entities to use for lookup.\n","\n","    Returns:\n","        list: A list of potential Wikipedia page titles (strings).\n","    \"\"\"\n","    # Simple simulation: Use entities as potential page titles\n","    # Replace spaces with underscores, common Wikipedia format\n","    #potential_titles = [entity.replace(\" \", \"_\") for entity in entities]\n","\n","\n","    ### TODO: Fine-tune GPT-4o-mini on the claim and the entities in the claim with the wikipedia URL as the target\n","\n","    # Reinitialize OpenAI client for each call\n","    query_client = OpenAI(api_key=api_key)\n","    potential_titles = []\n","    try:\n","        prompt = f\"Given the claim '{claim}' and the key entities '{', '.join(entities)}', list the most relevant Wikipedia page titles likely to contain evidence. Include key facts about the claim, such as the type of items mentioned. Respond with only a bracketed list of lowercase page titles with spaces as underscores, each title wrapped in single quotes and separated by a comma.\"\n","        response = query_client.chat.completions.create(\n","          model=\"gpt-4o-mini\",\n","            messages=[\n","               {\"role\": \"system\", \"content\": \"You are an assistant that identifies relevant Wikipedia page titles based on a claim and entities.\"},\n","               {\"role\": \"user\", \"content\": prompt},\n","           ],\n","          max_tokens=256,\n","          temperature=temp,\n","        )\n","        llm_titles_str = response.choices[0].message.content.strip()\n","        if debug:\n","            print(f\"DEBUG 1.0 (query_generator):\")\n","            print(f\"\\tClaim: {claim}\")\n","            print(f\"\\tEntities: {entities}\")\n","            print(f\"\\tLLM Output: {llm_titles_str}\")\n","            print(\"-_-\" * 5)\n","        try:\n","            llm_titles = ast.literal_eval(llm_titles_str)\n","            if isinstance(llm_titles, list):\n","                potential_titles.extend(llm_titles)\n","        except (ValueError, SyntaxError):\n","            print(f\"Warning: LLM returned non-list format for titles: {llm_titles_str}\")\n","    except Exception as e:\n","         print(f\"Warning: LLM call for query generation failed: {e}\")\n","\n","    # Remove duplicates and limit the number of titles\n","    unique_titles = sorted(list(set(potential_titles)), key=len) # Keep unique, maybe shorter titles are base articles\n","\n","    # Limit the number of pages to fetch to avoid excessive API calls/cost\n","    selected_titles = unique_titles[:max_pages_to_fetch]\n","\n","\n","    if debug:\n","        print(f\"DEBUG 1.1 (query_generator):\")\n","        print(f\"\\tEntities: {entities}\")\n","        print(f\"\\tGenerated Potential Titles: {unique_titles}\")\n","        print(f\"\\tSelected Titles for Retrieval: {selected_titles}\")\n","        print(\"-_-\" * 10)\n","\n","    return selected_titles\n","\n","\n","def retrieve_documents_from_wikipedia(page_titles, claim, entities, num_search_results=2, temp=0.2, debug=False):\n","    \"\"\"\n","    **UPDATED:** Retrieves document content (introduction) from specific Wikipedia page titles.\n","    Uses the 'wikipedia' library for API access.\n","\n","    Args:\n","        page_titles (list of str): List of Wikipedia page titles to fetch.\n","        max_intro_sentences (int): Max sentences to take from the intro.\n","        debug (bool): Enable debug printing.\n","\n","    Returns:\n","        tuple: (list of str, list of str):\n","                 - documents: List of retrieved document introduction texts.\n","                 - document_sources: List of corresponding page titles from which content was retrieved.\n","    \"\"\"\n","    documents = []\n","    document_sources = []\n","    wikipedia.set_lang(\"en\") # Ensure English Wikipedia\n","\n","    disambiguate_options_client = OpenAI(api_key=api_key)\n","\n","    if not page_titles:\n","        if debug:\n","            print(\"DEBUG 1.2 (retrieve_documents): No page titles provided.\")\n","        return [], []\n","\n","    for title in page_titles:\n","        try:\n","            # Suggestion handling: wikipedia library can sometimes find pages even with slight title variations\n","            search_results = wikipedia.search(title, results=num_search_results)\n","            if not search_results:\n","                 if debug:\n","                     print(f\"DEBUG 1.2: No Wikipedia page found for potential title: '{title}'\")\n","                 continue\n","\n","            # Get the closest match in the search result titles to the claim using sBERT\n","            claim_embedding = sbert_model.encode(claim, convert_to_tensor=True)\n","            search_results_embeddings = sbert_model.encode(search_results, convert_to_tensor=True)\n","            similarities = util.pytorch_cos_sim(claim_embedding, search_results_embeddings)[0]\n","            closest_index = similarities.argmax().item()\n","            actual_title = search_results[closest_index]\n","\n","            # Get the page object (handle disambiguation / page errors)\n","            page = wikipedia.page(actual_title, auto_suggest=False, redirect=True) # Use actual title now\n","\n","            # Extract introduction (summary)\n","            # The library's summary often captures the intro well. Limit sentences.\n","            intro_text = page.summary\n","            sentences = nltk.sent_tokenize(intro_text)\n","            content = \" \".join(sentences)\n","\n","            # Basic cleaning (redundant if summary is clean, but good practice)\n","            content = re.sub(r'\\s+', ' ', content).strip() # Normalize whitespace\n","\n","            if content:\n","                documents.append(content)\n","                document_sources.append(page.title) # Use the canonical title from the page object\n","                if debug:\n","                    print(f\"DEBUG 1.2: Successfully retrieved intro from '{page.title}' (searched for '{title}')\")\n","            else:\n","                 if debug:\n","                    print(f\"DEBUG 1.2: Empty content retrieved for page '{page.title}'\")\n","\n","        except wikipedia.exceptions.PageError:\n","            if debug:\n","                print(f\"DEBUG 1.2: PageError - Wikipedia page not found for title: '{title}' (or '{actual_title}')\")\n","        except wikipedia.exceptions.DisambiguationError as e:\n","            if debug:\n","                print(f\"DEBUG 1.2: DisambiguationError for title: '{title}'. Options: {e.options[:len(e.options)]}...\")\n","            # match the options to the claim and entities with gpt_4o-mini\n","            prompt = f\"Given the claim '{claim}' and the entities '{entities}', choose the most relevant Wikipedia page title from the following options: {e.options}. Respond with only the selected title.\"\n","            response = disambiguate_options_client.chat.completions.create(\n","              model=\"gpt-4o-mini\",\n","                messages=[\n","                   {\"role\": \"system\", \"content\": \"You are an assistant that selects the most relevant Wikipedia page title from a list of options.\"},\n","                   {\"role\": \"user\", \"content\": prompt},\n","               ],\n","              max_tokens=100,\n","              temperature=0.3,\n","            )\n","            selected_title = response.choices[0].message.content.strip()\n","            if selected_title in e.options:\n","                try:\n","                    page = wikipedia.page(selected_title, auto_suggest=False, redirect=True)\n","                    intro_text = page.summary\n","                    sentences = nltk.sent_tokenize(intro_text)\n","                    content = \" \".join(sentences)\n","                    content = re.sub(r'\\s+', ' ', content).strip() # Normalize whitespace\n","\n","                    if content:\n","                        documents.append(content)\n","                        document_sources.append(page.title) # Use the canonical title from the page object\n","                        if debug:\n","                            print(f\"DEBUG 1.2: Successfully retrieved intro from '{page.title}' (disambiguated to '{selected_title}')\")\n","                    else:\n","                         if debug:\n","                            print(f\"DEBUG 1.2: Empty content retrieved for disambiguated page '{page.title}'\")\n","                except Exception as e:\n","                    print(f\"Warning: Error retrieving disambiguated page '{selected_title}': {e}\")\n","            else:\n","                print(f\"Warning: Selected title '{selected_title}' not in disambiguation options.\")\n","        except requests.exceptions.RequestException as e:\n","            print(f\"Warning: Network error retrieving '{title}': {e}\")\n","            # Optional: Implement retry logic\n","            time.sleep(1) # Basic wait on error\n","        except Exception as e:\n","            print(f\"Warning: Unexpected error retrieving '{title}': {e}\")\n","\n","    if debug:\n","        print(f\"DEBUG 1.2: Retrieved content for {len(documents)} pages out of {len(page_titles)} potential titles.\")\n","        print(\"-_-\" * 10)\n","        print(\"-------------------------------------------------------------------\\n\")\n","\n","    # Concatenate the documents into one string and tokenize it with nltk\n","    all_text = \" \".join(documents)\n","    total_tokens = nltk.word_tokenize(all_text)\n","\n","    return documents, document_sources, total_tokens, len(documents)"]},{"cell_type":"markdown","metadata":{"id":"tOMbJ1uncldW"},"source":["## 4. Module 2: Evidence Sentence Extraction\n","<a id=\"4\"></a>"]},{"cell_type":"code","execution_count":157,"metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1745045120683,"user":{"displayName":"Henry Zelenak","userId":"01809909909045225068"},"user_tz":240},"id":"TVcIPHt8cldW"},"outputs":[],"source":["# --- Module 2 Helper: GPT-4o-mini claim rephrasing (New) --- #\n","def rephrase_claim(claim, n, rephrs_temp, debug=False):\n","  \"\"\"\n","  **NEW:** Rephrases the claim using GPT-4o-mini.\n","\n","  Args:\n","      claim (str): The input claim.\n","\n","  Returns:\n","      list of str: Rephrased claims.\n","  \"\"\"\n","  rephrase_client = OpenAI(api_key=api_key)\n","  claims = []\n","  for _ in range(n):\n","    try:\n","      response = rephrase_client.chat.completions.create(\n","        model=\"gpt-4o-mini\", # Use specific model\n","        messages=[\n","            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Rephrase the claim to encompass the same meaning but with different wording. Do not change the meaning or add any new information.\"},\n","            {\"role\": \"user\", \"content\": claim},\n","        ],\n","        max_tokens=512, # Adjust based on expected output length\n","        n=1,\n","        stop=None,\n","        temperature=rephrs_temp, # Lower temp for more deterministic output\n","      )\n","      rephrased_claim = response.choices[0].message.content.strip()\n","      if debug:\n","          print(f\"DEBUG 1.3 (rephrase_claim):\")\n","          print(f\"\\tOriginal Claim: {claim}\")\n","          print(f\"\\tRephrased Claim: {rephrased_claim}\")\n","          print(\"-_-\" * 5)\n","      claims.append(rephrased_claim)\n","    except Exception as e:\n","      print(f\"Error during LLM call in rephrase_claim: {e}\")\n","      claims.append('') # Append empty string on error\n","\n","  return claims\n","\n","\n","# --- Module 2 Helper: sBERT Filtering (Updated) ---\n","\n","def sbert_slide_filter(documents, doc_sources, claim, sbert_threshold, debug=False):\n","    \"\"\"\n","    **UPDATED:** Performs sentence filtering using sBERT similarity.\n","    Processes each document individually, assigning sentence IDs.\n","    Returns candidates as [source, id, text, score].\n","\n","    Args:\n","        documents (list of str): List of document texts (introductions).\n","        doc_sources (list of str): Corresponding source identifiers (page titles).\n","        claim (str): The claim text.\n","        sbert_threshold (float): The similarity threshold.\n","        debug (bool): Enable debug printing.\n","\n","    Returns:\n","        list: List of candidate sentences: [[page_title, sentence_id, sentence_text, similarity_score], ...]\n","    \"\"\"\n","    all_candidates = []\n","    claim_embedding = sbert_model.encode(claim, convert_to_tensor=True)\n","\n","    if len(documents) != len(doc_sources):\n","        print(\"Error: Mismatch between documents and sources count in sbert_slide_filter.\")\n","        return []\n","\n","    for doc_text, source_id in zip(documents, doc_sources):\n","        sentences = nltk.sent_tokenize(doc_text)\n","        if not sentences:\n","            continue\n","\n","        # Calcualate the total number of tokens\n","        total_tokens = sum(len(sent.split()) for sent in sentences)\n","\n","        # Encode all sentences in the document at once for efficiency\n","        sentence_embeddings = sbert_model.encode(sentences, convert_to_tensor=True)\n","\n","\n","        # Calculate cosine similarities between claim and all sentences in this doc\n","        similarities = util.pytorch_cos_sim(claim_embedding, sentence_embeddings)[0] # Shape [1, num_sentences] -> [num_sentences]\n","\n","        for i, sentence in enumerate(sentences):\n","            similarity_score = similarities[i].item() # Get scalar value\n","\n","            # Idea: Optionally include URL in similarity calc (as discussed in thought process)\n","            # sentence_with_source = sentence + \" \" + source_id # Append source_id (URL/title)\n","            # sentence_embedding = sbert_model.encode(sentence_with_source, convert_to_tensor=True)\n","            # similarity_score = util.pytorch_cos_sim(claim_embedding, sentence_embedding).item()\n","\n","            if similarity_score >= sbert_threshold:\n","                candidate = [source_id, i, sentence, similarity_score]\n","                all_candidates.append(candidate)\n","                if debug > 1: # More verbose debug\n","                     print(f\"DEBUG 2.2.1 (sbert_filter):\")\n","                     print(f\"\\tClaim: {claim[:50]}...\")\n","                     print(f\"\\tDoc: {source_id}, Sent ID: {i}\")\n","                     print(f\"\\tSentence: {sentence[:100]}...\")\n","                     # print(f\"\\tSentence+Source (optional): {sentence_with_source[:100]}...\")\n","                     print(f\"\\tSimilarity: {similarity_score:.4f} (Threshold: {sbert_threshold}) -> PASSED\")\n","                     print(\"-_-\" * 5)\n","\n","    # Sort candidates by similarity score (descending) - helps LLM prioritize\n","    all_candidates.sort(key=lambda x: x[3], reverse=True)\n","\n","    if debug:\n","       print(f\"DEBUG 2.2.2 (sbert_filter):\")\n","       print(f\"\\tTotal candidates found across all docs: {len(all_candidates)}\")\n","       # print(f\"\\tTop 3 candidates: {all_candidates[:3]}\") # Print top few if needed\n","       print(\"-_-\" * 10)\n","\n","    return all_candidates, total_tokens\n","\n","\n","# --- Module 2 Helper: LLM Sentence Selection (Updated Prompt) ---\n","\n","def extract_sentences_with_llm(claim, candidate_sentences_text, prompt, sentEx_temp, debug=False):\n","    \"\"\"\n","    **UPDATED:** Extracts sentences using an LLM based on provided candidates.\n","    Prompt adjusted for selection task.\n","\n","    Args:\n","        claim (str): The input claim.\n","        candidate_sentences_text (list of str): Candidate sentences provided by sBERT.\n","        prompt (str): The specific prompt for the LLM (should guide selection).\n","        debug (bool): Enable debug printing.\n","\n","    Returns:\n","        list of str: Selected sentences (as strings). Returns [\"NOT ENOUGH INFO\"] on failure or specific LLM response.\n","    \"\"\"\n","    sentEx_client = OpenAI(api_key=api_key) # Reinitialize client for each call\n","\n","    if not candidate_sentences_text:\n","      if debug:\n","        print(\"Warning: No candidates from sBERT to select from.\")\n","      return [\"NOT ENOUGH INFO\"] # No candidates to select from\n","\n","    # Format candidates for the prompt (e.g., numbered list)\n","    formatted_candidates = \"\\n\".join([f\"{i+1}. {s}\" for i, s in enumerate(candidate_sentences_text)])\n","\n","    full_prompt = f\"{prompt}\\n\\nClaim: {claim}\\n\\nSelect from these candidate sentences:\\n{formatted_candidates}\"\n","\n","    if debug > 1:\n","        print(f\"DEBUG 2.3.2 (LLM Selection):\")\n","        print(f\"\\tLLM Prompt (partial):\\n{prompt}\\n...\") # Show base prompt\n","        print(f\"\\tNum Candidates Sent to LLM: {len(candidate_sentences_text)}\")\n","        # print(f\"\\tCandidates: {formatted_candidates}\") # Avoid printing too much\n","        print(\"-_-\" * 5)\n","\n","    try:\n","        response = sentEx_client.chat.completions.create(\n","            model=\"gpt-4o-mini\", # Use the specified model\n","            messages=[\n","                {\"role\": \"system\", \"content\": \"You are a helpful assistant. Select sentences from the provided list that are evidence for the claim. Return ONLY the selected sentences, each on a new line. If none are relevant, respond ONLY with 'NOT ENOUGH INFO'.\"},\n","                {\"role\": \"user\", \"content\": full_prompt},\n","            ],\n","            max_tokens=512, # Adjust based on expected output length\n","            n=1,\n","            stop=None,\n","            temperature=sentEx_temp, # Lower temp for more deterministic selection\n","        )\n","        llm_output = response.choices[0].message.content.strip()\n","\n","        if debug:\n","            print(f\"DEBUG 2.3.3 (LLM Selection):\")\n","            print(f\"\\tLLM Raw Output:\\n{llm_output}\")\n","            print(\"-_-\" * 5)\n","\n","        if \"NOT ENOUGH INFO\" in llm_output:\n","             # Check if it's the *only* response, case-insensitive\n","             if llm_output.upper() == \"NOT ENOUGH INFO\":\n","                 return [\"NOT ENOUGH INFO\"]\n","             else:\n","                 # Handle cases where NEI is mixed with sentences - treat as NEI or try to parse?\n","                 # Safer to treat as NEI if the instruction was to only return NEI when applicable.\n","                 print(f\"Warning: LLM output contained 'NOT ENOUGH INFO' along with other text. Interpreting as NEI.\")\n","                 return [\"NOT ENOUGH INFO\"]\n","\n","\n","        # Split the response into sentences, removing empty lines\n","        selected_sentences = [s.strip() for s in llm_output.split('\\n') if s.strip()]\n","\n","        # Optional: Post-process LLM output - remove potential numbering (e.g., \"1. Sentence text\")\n","        processed_sentences = []\n","        for s in selected_sentences:\n","            match = re.match(r'^\\s*\\d+\\.\\s*(.*)', s) # Matches \"1. \", \" 2. \", etc.\n","            if match:\n","                processed_sentences.append(match.group(1).strip())\n","            else:\n","                processed_sentences.append(s) # Keep as is if no numbering pattern\n","\n","        return processed_sentences\n","\n","    except Exception as e:\n","        print(f\"Error during LLM call in extract_sentences_with_llm: {e}\")\n","        return [\"NOT ENOUGH INFO\"] # Treat errors as failure to find info\n","\n","\n","# --- Module 2 Main Control Flow (Updated) ---\n","\n","def module_2_2_controls(claim, documents, doc_sources, entities, keywords, initial_sbert_thresh=0.2, min_sbert_thresh=0.1, thresh_decay=0.05, max_evidence=5, max_iterations=5, near_match_thresh=0.9, rephrs_temp=0.3, sentEx_temp=0.3, verbose=0, debug=False):\n","    \"\"\"\n","    **UPDATED:** Module 2 implementing iterative sBERT -> LLM selection with reassociation.\n","\n","    Args:\n","        claim (str): The input claim.\n","        documents (list of str): List of retrieved document texts.\n","        doc_sources (list of str): Corresponding source identifiers (page titles).\n","        entities (list of str): Entities from the claim.\n","        keywords (list of str): Keywords from the claim.\n","        initial_sbert_thresh (float): Starting sBERT similarity threshold.\n","        min_sbert_thresh (float): Minimum sBERT threshold allowed.\n","        thresh_decay (float): Amount to decrease threshold if LLM selects few sentences.\n","        max_evidence (int): Target number of evidence sentences.\n","        verbose (int): Verbosity level.\n","        debug (bool): Enable debug printing.\n","\n","    Returns:\n","        tuple: (list, str, dict):\n","                 - final_evidence_ids: List of selected evidence: [[page_title, sentence_id], ...]\n","                 - status: \"OK\" or \"NOT ENOUGH INFO\".\n","                 - report: Dictionary with run details.\n","    \"\"\"\n","    if verbose: print(\"###### M2: Starting Evidence Extraction ######\")\n","\n","    final_evidence_ids = [] # Stores [[page_title, sentence_id]]\n","    all_sbert_candidates_map = {} # Store all candidates found { (title, id) : [title, id, text, score] } to avoid duplicates and for re-association\n","    selected_candidate_keys = set() # Keep track of (title, id) keys already selected\n","\n","    current_sbert_thresh = initial_sbert_thresh\n","    sbert_total_tokens = 0\n","\n","    llm_total_tokens = 0\n","    llm_total_sentences = 0\n","\n","    # Rephrase the claim for better context\n","    rephrased_claims = rephrase_claim(claim, max_iterations, rephrs_temp, debug) # Get multiple rephrased claims\n","    all_claims = [claim] + rephrased_claims # Include original claim\n","\n","    # Define prompts (using entities/keywords)\n","    entity_str = \", \".join(entities) if entities else \"relevant entities\"\n","    keyword_str = \", \".join(keywords) if keywords else \"relevant keywords\"\n","    prompts = {\n","      \"init\": f\"Retrieve sentences from the list that either support or refute the following claim. Specifically, focus on sentences mentioning {entity_str}. Order the sentences by relevance, highest first, and return a list separated by the return character. If there are no relevant sentences, respond with 'NOT ENOUGH INFO'. DO NOT CREATE ANY SENTENCES THAT ARE NOT IN THE PROVIDED LIST, AND DO NOT TRUNCATE THE SENTENCE.\",\n","      \"followup\": f\"You didn’t find enough sentences. Find additional (new) sentences that that are relevant to key points in the claim. Order the sentences by relevance, highest first, and return a list separated by the return character. If there are no relevant sentences, respond with 'NOT ENOUGH INFO'. DO NOT CREATE ANY SENTENCES THAT ARE NOT IN THE PROVIDED LIST, AND DO NOT TRUNCATE THE SENTENCE.\",\n","    }\n","\n","    if debug:\n","        print(f\"DEBUG 2.1 (module_2_controls):\")\n","        print(f\"\\tClaim: {claim}\")\n","        print(f\"\\tEntities: {entities}\")\n","        print(f\"\\tKeywords: {keywords}\")\n","        print(f\"\\tInitial sBERT Thresh: {initial_sbert_thresh}, Min Thresh: {min_sbert_thresh}\")\n","        print(f\"\\tMax Evidence Target: {max_evidence}, Max Iterations: {max_iterations}\")\n","        print(\"-_-\" * 10)\n","\n","    for iteration in range(max_iterations):\n","        if len(final_evidence_ids) >= max_evidence:\n","            if verbose: print(f\"M2 Iter {iteration}: Reached target evidence count ({len(final_evidence_ids)}).\")\n","            break\n","\n","        if debug:\n","            print(f\"DEBUG 2. Iteration {iteration+1}/{max_iterations}, Current sBERT Thresh: {current_sbert_thresh:.3f}\")\n","\n","        # 1. Get sBERT Candidates (at current threshold)\n","        # 1.1: Calculate the total number of tokens across all documents\n","        sbert_candidates, iter_tokens = sbert_slide_filter(documents, doc_sources, all_claims[iteration], current_sbert_thresh, debug=debug)\n","        sbert_total_tokens += iter_tokens\n","\n","        # Store new candidates and identify *new* ones for this iteration's LLM input\n","        new_candidates_for_llm = []\n","        current_iter_candidate_details = [] # Store details [[title, id, text],...] for re-association\n","        for cand in sbert_candidates:\n","            key = (cand[0], cand[1]) # (title, id)\n","            if key not in all_sbert_candidates_map:\n","                all_sbert_candidates_map[key] = cand # Store full details\n","            # Only consider candidates not already selected for the LLM input\n","            if key not in selected_candidate_keys:\n","                 new_candidates_for_llm.append(cand[2]) # Add sentence text to LLM input list\n","                 current_iter_candidate_details.append([cand[0], cand[1], cand[2]]) # Store [title, id, text] for matching\n","\n","        if not new_candidates_for_llm:\n","            if verbose: print(f\"M2 Iter {iteration+1}: No new candidates found by sBERT at threshold {current_sbert_thresh:.3f}.\")\n","            # Option: Lower threshold aggressively or break if already low\n","            if current_sbert_thresh > min_sbert_thresh:\n","                 current_sbert_thresh = max(min_sbert_thresh, current_sbert_thresh - thresh_decay * 2) # Faster decay if nothing found\n","                 if verbose: print(f\"   Lowering threshold to {current_sbert_thresh:.3f} for next try.\")\n","                 continue # Try again with lower threshold\n","            else:\n","                 if verbose: print(f\"M2 Iter {iteration+1}: No new candidates and threshold at minimum ({current_sbert_thresh:.3f}). Stopping.\")\n","                 break # Stop if threshold is already at minimum\n","\n","        # 2. LLM Selection\n","        # 2.1: Calculate the number of tokens sent to the LLM at this iteration\n","        llm_total_tokens += sum(len(sent.split()) for sent in new_candidates_for_llm)\n","        llm_total_sentences += len(new_candidates_for_llm)\n","        if verbose: print(f\"M2 Iter {iteration+1}: Sending {len(new_candidates_for_llm)} new candidates to LLM for selection.\")\n","        # Use the appropriate prompt based on the iteration\n","        if iteration == 0:\n","            this_prompt = prompts[\"init\"]\n","        else:\n","            this_prompt = prompts[\"followup\"]\n","        selected_sentences_text = extract_sentences_with_llm(all_claims[iteration], new_candidates_for_llm, this_prompt, sentEx_temp, debug=debug)\n","\n","        # 3. Process LLM Output & Re-association\n","        num_selected_this_iter = 0\n","        if selected_sentences_text == [\"NOT ENOUGH INFO\"]:\n","            if verbose: print(f\"M2 Iter {iteration+1}: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\")\n","            # Decide how to proceed: lower threshold, stop?\n","            # Lower threshold if LLM found nothing on the first or second tries\n","            if iteration < 2 and current_sbert_thresh > min_sbert_thresh:\n","                current_sbert_thresh = max(min_sbert_thresh, current_sbert_thresh - thresh_decay)\n","                if verbose: print(f\"   Lowering threshold to {current_sbert_thresh:.3f} for next try.\")\n","                continue # Try again with lower threshold\n","            else:\n","                if verbose: print(f\"M2 Iter {iteration+1}: Stopping because LLM found no evidence after two tries.\")\n","                break\n","        else:\n","            if verbose: print(f\"M2 Iter {iteration+1}: LLM selected {len(selected_sentences_text)} sentences.\")\n","            # Re-associate selected text with [title, id] using near_match\n","            for llm_sent in selected_sentences_text:\n","                best_match_key = None\n","                highest_sim = -1.0\n","                # Find the best match among the candidates sent to the LLM this iteration\n","                for title, sent_id, orig_text in current_iter_candidate_details:\n","                    key = (title, sent_id)\n","                    # Skip if this candidate was already selected in this iteration by a previous LLM sentence match\n","                    # Or if it was selected in a *previous* iteration\n","                    if key in selected_candidate_keys:\n","                        continue\n","\n","                    # Use near_match to compare LLM output with original sBERT candidate text\n","                    similarity = len(set(llm_sent.lower().split()).intersection(set(orig_text.lower().split()))) / len(set(llm_sent.lower().split()).union(set(orig_text.lower().split())))\n","\n","                    # Using exact match or near_match for robustness\n","                    if near_match(llm_sent, orig_text, threshold=near_match_thresh, verbose=debug>1): # Use near match\n","                         # Crude way to find the 'best' match if multiple near-matches exist\n","                         if similarity > highest_sim:\n","                              highest_sim = similarity\n","                              best_match_key = key\n","\n","                if best_match_key:\n","                    if best_match_key not in selected_candidate_keys:\n","                        final_evidence_ids.append([best_match_key[0], best_match_key[1]]) # Store [title, id]\n","                        #######################################################################################\n","\n","                        ### NOTE: WE ARE ADDING 1 TO EACH SENTENCE INDEX. This is due to observation alone: A few (<40% of the predicted evidence items have the correct page title but the sentence ID is one fewer)\n","                        ### THIS MAY BE VERY CONSEQUENTIAL\n","\n","                        #######################################################################################\n","                        selected_candidate_keys.add(best_match_key) # Mark as selected\n","                        num_selected_this_iter += 1\n","                        if verbose > 1: print(f\"   Matched: '{llm_sent[:50]}...' -> {best_match_key}\")\n","                        if len(final_evidence_ids) >= max_evidence:\n","                            break # Stop if max evidence reached during re-association\n","                    # else: (already selected) - do nothing\n","                else:\n","                    if verbose: print(f\"   Warning: Could not re-associate LLM output with retrieved data: '{llm_sent[:80]}...'\")\n","\n","\n","        # 4. Dynamic Threshold Adjustment (Based on LLM selection)\n","        if num_selected_this_iter < len(new_candidates_for_llm) / 4 and len(new_candidates_for_llm) > 0: # Example: If LLM selected less than 25% of candidates\n","            if current_sbert_thresh > min_sbert_thresh:\n","                current_sbert_thresh = max(min_sbert_thresh, current_sbert_thresh - thresh_decay)\n","                if verbose: print(f\"M2 Iter {iteration+1}: LLM selected few items ({num_selected_this_iter}). Lowering sBERT threshold to {current_sbert_thresh:.3f}\")\n","        # Optional: Increase threshold slightly if LLM selects almost everything? (Less common)\n","        elif num_selected_this_iter > len(new_candidates_for_llm) * 0.8:\n","            current_sbert_thresh = min(initial_sbert_thresh, current_sbert_thresh + thresh_decay / 2)\n","            if verbose: print(f\"   LLM selected many items. Slightly increasing threshold to {current_sbert_thresh:.3f}\")\n","\n","        if debug:\n","            print(f\"DEBUG 2. End Iter {iteration+1}: Total evidence found: {len(final_evidence_ids)}\")\n","            print(\"-_-\" * 10)\n","\n","\n","    # Final Status and Report\n","    status = \"OK\" if final_evidence_ids else \"NOT ENOUGH INFO\"\n","    if not final_evidence_ids and verbose:\n","        print(\"M2: Finished iterations. No evidence selected.\")\n","        print(\"-------------------------------------------------------------------\\n\")\n","    elif verbose:\n","        print(f\"M2: Finished. Selected {len(final_evidence_ids)} evidence items.\")\n","        print(\"-------------------------------------------------------------------\\n\")\n","\n","\n","    # Store all sentences found by sbert (for analysis) and selected ones\n","    # Need to retrieve text for selected IDs for the report\n","    all_sbert_sentences_text = [details[2] for details in all_sbert_candidates_map.values()]\n","    selected_evidence_texts = [all_sbert_candidates_map[key][2] for key in selected_candidate_keys if key in all_sbert_candidates_map]\n","\n","    report = {\n","        \"claim\": claim,\n","        \"final_evidence_ids\": final_evidence_ids, # [[title, id], ...]\n","        \"selected_evidence_texts\": selected_evidence_texts, # List of text for selected evidence\n","        \"status\": status,\n","        \"iterations_run\": iteration + 1,\n","        \"max_evidence\": max_evidence,\n","        \"max_iterations\": max_iterations,\n","        \"mod_2_total_documents\": len(documents),\n","        \"sbert_total_sentences\": len(all_sbert_candidates_map),\n","        \"sbert_total_tokens\": sbert_total_tokens,\n","        \"initial_sbert_thresh\": initial_sbert_thresh,\n","        \"final_sbert_threshold\": current_sbert_thresh,\n","        \"min_sbert_thresh\": min_sbert_thresh,\n","        \"thresh_decay\": thresh_decay,\n","        # \"all_sbert_candidates_text\": all_sbert_sentences_text, # Can be large\n","        \"llm_total_sentences\": llm_total_sentences,\n","        \"llm_total_tokens\": llm_total_tokens,\n","        \"near_match_thresh\": near_match_thresh,\n","    }\n","\n","    return final_evidence_ids, status, report\n","\n"]},{"cell_type":"markdown","metadata":{"id":"zElKAY14cldY"},"source":["## 5. Module 3: Claim Classification\n","<a id=\"5\"></a>\n"]},{"cell_type":"code","execution_count":166,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1745045702043,"user":{"displayName":"Henry Zelenak","userId":"01809909909045225068"},"user_tz":240},"id":"jyGTtn5kcldY"},"outputs":[],"source":["# --- Module 3: Classification (Updated) ---\n","def module_3_classification(claim, evidence_texts, verbose=0, debug=False):\n","    \"\"\"\n","    **UPDATED:** Classifies the claim based on the TEXT of the extracted evidence sentences.\n","\n","    Args:\n","        claim (str): The input claim.\n","        evidence_texts (list of str): List of extracted evidence sentence texts.\n","        verbose (int): Verbosity level.\n","        debug (bool): Enable debug printing.\n","\n","    Returns:\n","        tuple: (str, str, str):\n","                 - classification_result: \"SUPPORTS\", \"REFUTES\", or \"NOT ENOUGH INFO\".\n","                 - exit_status: \"OK\" or \"NOT ENOUGH INFO\".\n","                 - prompt: The prompt used for classification.\n","    \"\"\"\n","\n","    if verbose: print(\"###### M3: Starting Classification ######\")\n","\n","    if not evidence_texts:\n","        if verbose: print(\"M3: No evidence text provided. Classifying as NOT ENOUGH INFO.\")\n","        # Return structure consistent with LLM call but without making one\n","        return \"NOT ENOUGH INFO\", \"NOT ENOUGH INFO\", \"No evidence provided to LLM.\"\n","\n","    # Format evidence for the prompt\n","    formatted_evidence = \"\\n\".join([f\"- {e}\" for e in evidence_texts])\n","    if not formatted_evidence: # Handle case where list might contain only empty strings\n","         if verbose: print(\"M3: Evidence text list was empty or contained only whitespace. Classifying as NOT ENOUGH INFO.\")\n","         return \"NOT ENOUGH INFO\", \"NOT ENOUGH INFO\", \"Evidence text was empty.\"\n","\n","\n","    # 3.1 Prompt\n","    prompt = f\"Based ONLY on the following evidence sentences, classify the claim as SUPPORTS, REFUTES, or NOT ENOUGH INFO.\\n\\nClaim: '{claim}'\\n\\nEvidence:\\n{formatted_evidence}\\n\\nRespond ONLY with SUPPORTS, REFUTES, or NOT ENOUGH INFO.\"\n","\n","    if debug:\n","        print(f\"DEBUG 3.1 (module_3_classification):\")\n","        print(f\"\\tClaim: {claim}\")\n","        print(f\"\\tEvidence Texts Sent ({len(evidence_texts)}):\")\n","        # for i, txt in enumerate(evidence_texts): print(f\"\\t  {i+1}. {txt[:100]}...\") # Print snippet\n","        print(f\"\\tPrompt (partial): {prompt[:200]}...\")\n","        print(\"-_-\" * 10)\n","\n","    # 3.2 Classification Call\n","    nli_client = OpenAI(api_key=api_key) # Reinitialize client for each call\n","    try:\n","        response = nli_client.chat.completions.create(\n","            model=\"gpt-4o-mini\", # Use specified model\n","            messages=[\n","                {\"role\": \"system\", \"content\": \"You are a claim classification assistant. Respond only with SUPPORTS, REFUTES, or NOT ENOUGH INFO.\"},\n","                {\"role\": \"user\", \"content\": prompt},\n","            ],\n","            max_tokens=10,  # Classification is short\n","            n=1,\n","            stop=None,\n","            temperature=0.1, # Low temperature for classification\n","        )\n","        classification_result = response.choices[0].message.content.strip().upper() # Normalize output\n","\n","        # Validate output\n","        valid_labels = [\"SUPPORTS\", \"REFUTES\", \"NOT ENOUGH INFO\"]\n","        if classification_result not in valid_labels:\n","             print(f\"Warning: Module 3 LLM returned invalid label '{classification_result}'. Defaulting to NOT ENOUGH INFO.\")\n","             classification_result = \"NOT ENOUGH INFO\"\n","\n","    except Exception as e:\n","         print(f\"Error during Module 3 classification LLM call: {e}\")\n","         classification_result = \"NOT ENOUGH INFO\" # Default on error\n","\n","\n","    # 3.3 Exit Status\n","    exit_status = \"OK\" if classification_result in [\"SUPPORTS\", \"REFUTES\"] else \"NOT ENOUGH INFO\"\n","\n","    if debug:\n","      print(f\"DEBUG 3.2/3.3 (module_3_classification):\")\n","      print(f\"\\tLLM Classification Result: {classification_result}\")\n","      print(f\"\\tExit Status: {exit_status}\")\n","      print(\"-_-\" * 10)\n","\n","    return classification_result, exit_status, prompt"]},{"cell_type":"markdown","metadata":{"id":"X59R90ZCcldY"},"source":["## 6. Module 0: System Control & Execution\n","<a id=\"6\"></a>"]},{"cell_type":"code","execution_count":169,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1745046942503,"user":{"displayName":"Henry Zelenak","userId":"01809909909045225068"},"user_tz":240},"id":"1BMqmrX7cldY"},"outputs":[],"source":["# --- Helper to get test claim from paper_test.jsonl ---\n","global_test_data_index = 0\n","\n","def get_test_claim(test_data_list, verbose=0, debug=False):\n","    \"\"\"\n","    **UPDATED:** Gets the next claim from the loaded paper_test.jsonl data.\n","\n","    Args:\n","        test_data_list (list): The list loaded from paper_test.jsonl.\n","        verbose (int): Verbosity level.\n","        debug (bool): Enable debug printing.\n","\n","    Returns:\n","        tuple: (claim_id, claim_text) or (None, None) if index is out of bounds.\n","    \"\"\"\n","    global global_test_data_index\n","    if global_test_data_index >= len(test_data_list):\n","        print(\"Reached end of test data.\")\n","        return None, None # Signal end\n","\n","    item = test_data_list[global_test_data_index]\n","    claim_id = item.get(\"id\")\n","    claim_text = item.get(\"claim\")\n","\n","    if claim_id is None or claim_text is None:\n","        print(f\"Warning: Skipping item at index {global_test_data_index} due to missing 'id' or 'claim'. Item: {item}\")\n","        global_test_data_index += 1\n","        return get_test_claim(test_data_list, verbose, debug) # Recursively get next\n","\n","    if verbose > 1:\n","        print(f\"Getting Test Claim {global_test_data_index + 1}/{len(test_data_list)}: ID={claim_id}, Claim='{claim_text[:100]}...'\")\n","\n","    global_test_data_index += 1 # Increment for next call\n","    return claim_id, claim_text\n","\n","# --- Main System Control Flow (Updated) ---\n","\n","def module_0_sys_control(test_items_list, test_size, initial_sbert_thresh, min_sbert_thresh, thresh_decay, max_evidence, max_iterations, near_match_thresh, max_pages_to_fetch, num_search_results, query_client_temp, rephrase_client_temp, sentEx_client_temp, nli_client_temp, disambiguate_client_temp, verbose=0, debug=False):\n","    \"\"\"\n","    **UPDATED:** Orchestrates the full fact-checking pipeline for test data.\n","\n","    Args:\n","        test_items_list (list): List of dicts loaded from paper_test.jsonl.\n","        test_size (int): Number of items to process from the list.\n","        verbose (int): Verbosity level.\n","        debug (bool): Enable detailed debug printing.\n","\n","    Returns:\n","        tuple: (list, pd.DataFrame):\n","                 - predictions_list: List of prediction dicts for output/scoring.\n","                 - run_report_df: DataFrame containing detailed logs for each claim.\n","    \"\"\"\n","    global global_test_data_index\n","    global_test_data_index = 0 # Reset index at the start of a run\n","\n","    predictions_list = [] # Stores final formatted predictions\n","    report_columns = [\n","        'id', 'claim', 'time_to_check', 'entities', 'keywords', 'retrieved_pages',\n","        'module2_status', 'predicted_evidence_ids', 'predicted_evidence_texts',\n","        'module3_result', 'module3_status', 'module3_prompt', 'module1_report_details',\n","        'module2_report_details' # Store the nested report dict from M2\n","    ]\n","    run_report_list = [] # Collect data for DataFrame\n","\n","    # Store original documents fetched by Module 1 for later lookup\n","    # Useful for getting text for Module 3 without re-fetching/storing large texts repeatedly\n","    document_store = {} # { page_title: text }\n","\n","    # Limit processing to test_size or available data\n","    actual_test_size = min(test_size, len(test_items_list))\n","    if actual_test_size <= 0:\n","         print(\"Error: No test data items to process.\")\n","         return [], pd.DataFrame(columns=report_columns)\n","\n","    print(f\"Starting system control. Processing {actual_test_size} test claims...\")\n","\n","    for i in tqdm(range(actual_test_size), desc=\"Processing Claims\"):\n","        # 0. Start timer\n","        start_time = time.time()\n","\n","        # 1. Get Claim from Test Data\n","        claim_id, claim = get_test_claim(test_items_list, verbose=verbose, debug=debug)\n","        if claim_id is None: # Reached end or error\n","            break\n","\n","        if verbose: print(f\"\\n--- Processing Claim ID: {claim_id} ---\")\n","        if verbose > 1: print(f\"Claim: {claim}\")\n","\n","        document_store.clear() # Clear store for new claim\n","\n","        # 2. Extract Entities & Keywords\n","        entities = extract_entities(claim)\n","        keywords = extract_keywords(claim)\n","        if verbose > 1: print(f\"Entities: {entities}, Keywords: {keywords}\")\n","\n","        # --- Module 1 ---\n","\n","        # 3. Generate Potential Page Titles\n","        potential_titles = query_generator(claim, keywords, entities, max_pages_to_fetch, query_client_temp, debug=debug)\n","\n","        # 4. Retrieve Documents\n","        if verbose: print(\"###### M1: Retrieving Documents ######\")\n","        documents, doc_sources, total_document_tokens, mod_1_total_documents = retrieve_documents_from_wikipedia(potential_titles, claim, entities, num_search_results, disambiguate_client_temp, debug=debug)\n","        retrieved_pages_str = \", \".join(doc_sources) if doc_sources else \"None\"\n","\n","        # Store fetched documents for Module 3 lookup\n","        for title, text in zip(doc_sources, documents):\n","            document_store[title] = text\n","\n","        # Create the Module 1 report\n","        module_1_report = {\n","            \"mod_1_total_documents\": mod_1_total_documents,\n","            \"total_document_tokens\": total_document_tokens,\n","            \"potential_titles\": potential_titles,\n","            \"retrieved_titles\": retrieved_pages_str\n","        }\n","\n","        if not documents:\n","            if verbose: print(\"M1: No documents retrieved. Cannot proceed.\\n-------------------------------------------------------\\n\")\n","            # Handle case with no documents: classify as NEI directly\n","            predicted_evidence_ids = []\n","            predicted_evidence_texts = []\n","            classification_result = \"NOT ENOUGH INFO\"\n","            mod2_status = \"NOT ENOUGH INFO\"\n","            mod3_status = \"NOT ENOUGH INFO\"\n","            mod3_prompt = \"Skipped - No documents from M1\"\n","            mod2_report_details = {\"status\": \"Skipped - No documents from M1\"}\n","        else:\n","            # --- Module 2 ---\n","            # 5. Extract Evidence Sentences ([title, id])\n","            # Use appropriate thresholds\n","            predicted_evidence_ids, mod2_status, mod2_report = module_2_2_controls(\n","                claim, documents, doc_sources, entities, keywords,\n","                initial_sbert_thresh=initial_sbert_thresh, # Slightly higher initial threshold?\n","                min_sbert_thresh=min_sbert_thresh,\n","                thresh_decay=thresh_decay,\n","                max_evidence=max_evidence,\n","                max_iterations=max_iterations,\n","                near_match_thresh=near_match_thresh,\n","                rephrs_temp=rephrase_client_temp,\n","                sentEx_temp=sentEx_client_temp,\n","                verbose=verbose,\n","                debug=debug\n","            )\n","            predicted_evidence_texts = mod2_report.get(\"selected_evidence_texts\", [])\n","            mod2_report_details = mod2_report # Store the whole M2 report\n","\n","            # --- Module 3 ---\n","            # 6. Classify Claim based on evidence TEXT\n","            classification_result, mod3_status, mod3_prompt = module_3_classification(\n","                claim,\n","                predicted_evidence_texts, # Pass the actual text\n","                verbose=verbose,\n","                debug=debug\n","            )\n","\n","        # 7. Format Output for FEVER Scorer / Final JSON\n","        # 7.1 Encode the brackets (-LRB-, -RRB-, -LSB-, -RSB-) and replace spaces with underscores for each page title\n","        bracket_mapping = {\n","            \"(\": \"-LRB-\",\n","            \")\": \"-RRB-\",\n","            \"[\": \"-LSB-\",\n","            \"]\": \"-RSB-\"\n","        }\n","        for item in predicted_evidence_ids:\n","            # Encode the page title like the test set (brackets and unerscores)\n","            item[0] = \"\".join(bracket_mapping.get(c, c) for c in item[0])\n","            item[0] = item[0].replace(\" \", \"_\")\n","            # Add 1 to each sentence ID\n","            #item[1] += 1\n","            # Add 1 to each sentence ID\n","            ### NOTE: This is due to observation alone: A few (<40% of the predicted evidence items have the correct page title but the sentence ID is one fewer)\n","            ### THIS MAY BE VERY CONSEQUENTIAL\n","\n","        prediction_item = {\n","            \"id\": claim_id,\n","            \"predicted_label\": classification_result,\n","            \"predicted_evidence\": predicted_evidence_ids # List of [page_title, sentence_id]\n","        }\n","        # The FEVER scorer needs gold labels/evidence\n","        # We add dummy fields here if there is no gold evidence.\n","        if \"label\" in test_items_list[i]: # Check if gold data exists\n","            if verbose: print(\"Adding gold label/evidence to prediction.\")\n","            prediction_item[\"label\"] = test_items_list[i][\"label\"]\n","            # Set th efirst two items of each inner list of test_items_list to None and exclude duplicates\n","            unique_evdc_items = []\n","            for inner_list in test_items_list[i][\"evidence\"]:\n","                if inner_list not in unique_evdc_items:\n","                    unique_evdc_items.append(inner_list)\n","            for evidence_set in test_items_list[i][\"evidence\"]:\n","                for item in evidence_set:\n","                    item[0] = None\n","                    item[1] = None\n","            if test_items_list[i][\"label\"] == \"NOT ENOUGH INFO\":\n","                unique_evdc_items = []\n","            prediction_item[\"evidence\"] = unique_evdc_items\n","            if verbose: print(\"##########################################################################\\n\")\n","        else:\n","            if verbose: print(\"Adding dummy gold label/evidence to prediction because gold data is missing.\")\n","            # Add placeholder fields if running the scorer function is desired,\n","            # otherwise, these can be omitted if just generating predictions.\n","            prediction_item[\"label\"] = \"NOT ENOUGH INFO\" # Dummy\n","            prediction_item[\"evidence\"] = [] # Dummy\n","        if verbose: print(\"##########################################################################\\n\")\n","\n","        predictions_list.append(prediction_item)\n","\n","        time_to_check = time.time() - start_time\n","        if verbose: print(f\"Time to process claim: {time_to_check:.2f} seconds.\\n------------------------------------------------\\n\")\n","\n","        # 8. Log Run Details\n","        run_report_list.append({\n","            'id': claim_id,\n","            'claim': claim,\n","            'time_to_check': time_to_check,\n","            'entities': \", \".join(entities) if entities else \"\",\n","            'keywords': \", \".join(keywords) if keywords else \"\",\n","            'retrieved_pages': retrieved_pages_str,\n","            'module2_status': mod2_status,\n","            'predicted_evidence_ids': json.dumps(predicted_evidence_ids), # Store as JSON string\n","            'predicted_evidence_texts': json.dumps(predicted_evidence_texts),# Store as JSON string\n","            'module3_result': classification_result,\n","            'module3_status': mod3_status,\n","            'module3_prompt': mod3_prompt,\n","            'module1_report_details': json.dumps(module_1_report), # Store M1 report dict as JSON string\n","            'module2_report_details': json.dumps(mod2_report_details) # Store M2 report dict as JSON string\n","        })\n","\n","        # Optional: Garbage collect periodically if memory usage is high\n","        if i % 50 == 0:\n","            gc.collect()\n","\n","    print(f\"\\nFinished processing {len(predictions_list)} claims.\")\n","    run_report_df = pd.DataFrame(run_report_list, columns=report_columns)\n","    return predictions_list, run_report_df\n"]},{"cell_type":"code","execution_count":170,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":662185,"status":"ok","timestamp":1745047608109,"user":{"displayName":"Henry Zelenak","userId":"01809909909045225068"},"user_tz":240},"id":"31-EqN8iztRa","outputId":"20cfff9d-60f8-43e6-c532-a0480c2dac54"},"outputs":[{"output_type":"stream","name":"stdout","text":["Starting system control. Processing 30 test claims...\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing Claims:   0%|          | 0/30 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","--- Processing Claim ID: 113501 ---\n","DEBUG 1.0 (query_generator):\n","\tClaim: Grease had bad reviews.\n","\tEntities: ['Grease']\n","\tLLM Output: ['grease_(musical)', 'grease_(film)', 'grease_(soundtrack)', 'grease_(franchise)', 'critical_response_to_grease']\n","-_--_--_--_--_-\n","DEBUG 1.1 (query_generator):\n","\tEntities: ['Grease']\n","\tGenerated Potential Titles: ['grease_(film)', 'grease_(musical)', 'grease_(franchise)', 'grease_(soundtrack)', 'critical_response_to_grease']\n","\tSelected Titles for Retrieval: ['grease_(film)', 'grease_(musical)', 'grease_(franchise)', 'grease_(soundtrack)', 'critical_response_to_grease']\n","-_--_--_--_--_--_--_--_--_--_-\n","###### M1: Retrieving Documents ######\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n","\n","The code that caused this warning is on line 389 of the file /usr/local/lib/python3.11/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n","\n","  lis = BeautifulSoup(html).find_all('li')\n"]},{"output_type":"stream","name":"stdout","text":["DEBUG 1.2: DisambiguationError for title: 'grease_(film)'. Options: ['Grease (lubricant)', 'petroleum', 'Brown grease', 'Yellow grease', 'Hydrogenated vegetable oil', 'Vegetable shortening', 'bribe', 'killing', 'Pomade', 'Grease (musical)', 'Grease (film)', 'Grease 2', '\"Grease\" (song)', '1971 musical play', 'Grease: The Original Soundtrack from the Motion Picture', 'Grease: The New Broadway Cast Recording (2007 album)', 'Grease: Live', \"Grease: You're the One that I Want!\", 'Grease is the Word', 'Extreme Ghostbusters', 'Grease (franchise)', 'Grease (video game)', 'Mud fever', 'Aglossa cuprina', 'Grease (networking)', 'All pages with titles beginning with Grease ', 'All pages with titles containing Grease', 'Greaser (disambiguation)', 'Greasy (disambiguation)', 'Greece (disambiguation)']...\n","DEBUG 1.2: Successfully retrieved intro from 'Grease (film)' (disambiguated to 'Grease (film)')\n","DEBUG 1.2: DisambiguationError for title: 'grease_(musical)'. Options: ['Grease (lubricant)', 'petroleum', 'Brown grease', 'Yellow grease', 'Hydrogenated vegetable oil', 'Vegetable shortening', 'bribe', 'killing', 'Pomade', 'Grease (musical)', 'Grease (film)', 'Grease 2', '\"Grease\" (song)', '1971 musical play', 'Grease: The Original Soundtrack from the Motion Picture', 'Grease: The New Broadway Cast Recording (2007 album)', 'Grease: Live', \"Grease: You're the One that I Want!\", 'Grease is the Word', 'Extreme Ghostbusters', 'Grease (franchise)', 'Grease (video game)', 'Mud fever', 'Aglossa cuprina', 'Grease (networking)', 'All pages with titles beginning with Grease ', 'All pages with titles containing Grease', 'Greaser (disambiguation)', 'Greasy (disambiguation)', 'Greece (disambiguation)']...\n","DEBUG 1.2: Successfully retrieved intro from 'Grease (film)' (disambiguated to 'Grease (film)')\n","DEBUG 1.2: DisambiguationError for title: 'grease_(franchise)'. Options: ['Grease (lubricant)', 'petroleum', 'Brown grease', 'Yellow grease', 'Hydrogenated vegetable oil', 'Vegetable shortening', 'bribe', 'killing', 'Pomade', 'Grease (musical)', 'Grease (film)', 'Grease 2', '\"Grease\" (song)', '1971 musical play', 'Grease: The Original Soundtrack from the Motion Picture', 'Grease: The New Broadway Cast Recording (2007 album)', 'Grease: Live', \"Grease: You're the One that I Want!\", 'Grease is the Word', 'Extreme Ghostbusters', 'Grease (franchise)', 'Grease (video game)', 'Mud fever', 'Aglossa cuprina', 'Grease (networking)', 'All pages with titles beginning with Grease ', 'All pages with titles containing Grease', 'Greaser (disambiguation)', 'Greasy (disambiguation)', 'Greece (disambiguation)']...\n","DEBUG 1.2: Successfully retrieved intro from 'Grease (film)' (disambiguated to 'Grease (film)')\n","DEBUG 1.2: DisambiguationError for title: 'grease_(soundtrack)'. Options: ['Grease (lubricant)', 'petroleum', 'Brown grease', 'Yellow grease', 'Hydrogenated vegetable oil', 'Vegetable shortening', 'bribe', 'killing', 'Pomade', 'Grease (musical)', 'Grease (film)', 'Grease 2', '\"Grease\" (song)', '1971 musical play', 'Grease: The Original Soundtrack from the Motion Picture', 'Grease: The New Broadway Cast Recording (2007 album)', 'Grease: Live', \"Grease: You're the One that I Want!\", 'Grease is the Word', 'Extreme Ghostbusters', 'Grease (franchise)', 'Grease (video game)', 'Mud fever', 'Aglossa cuprina', 'Grease (networking)', 'All pages with titles beginning with Grease ', 'All pages with titles containing Grease', 'Greaser (disambiguation)', 'Greasy (disambiguation)', 'Greece (disambiguation)']...\n","DEBUG 1.2: Successfully retrieved intro from 'Grease (film)' (disambiguated to 'Grease (film)')\n","DEBUG 1.2: Successfully retrieved intro from 'Grease 2' (searched for 'critical_response_to_grease')\n","DEBUG 1.2: Retrieved content for 5 pages out of 5 potential titles.\n","-_--_--_--_--_--_--_--_--_--_-\n","-------------------------------------------------------------------\n","\n","###### M2: Starting Evidence Extraction ######\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Grease had bad reviews.\n","\tRephrased Claim: Grease received unfavorable reviews.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Grease had bad reviews.\n","\tRephrased Claim: Grease received negative reviews.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Grease had bad reviews.\n","\tRephrased Claim: Grease received negative reviews.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Grease had bad reviews.\n","\tRephrased Claim: Grease received unfavorable reviews.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Grease had bad reviews.\n","\tRephrased Claim: Grease received negative reviews.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Grease had bad reviews.\n","\tRephrased Claim: Grease received negative reviews.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Grease had bad reviews.\n","\tRephrased Claim: Grease received poor reviews.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Grease had bad reviews.\n","\tRephrased Claim: Grease received negative reviews.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Grease had bad reviews.\n","\tRephrased Claim: Grease received negative reviews.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Grease had bad reviews.\n","\tRephrased Claim: Grease received unfavorable reviews.\n","-_--_--_--_--_-\n","DEBUG 2.1 (module_2_controls):\n","\tClaim: Grease had bad reviews.\n","\tEntities: ['Grease']\n","\tKeywords: ['reviews', 'grease', 'bad']\n","\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n","\tMax Evidence Target: 8, Max Iterations: 10\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 23\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 1: Sending 23 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","1. Despite breakthrough roles for Pfeiffer, Adrian Zmed, and Christopher McDonald, the film received mostly negative reviews from critics; however, Grease 2 maintains a devoted fan base decades after its release.\n","-_--_--_--_--_-\n","M2 Iter 1: LLM selected 1 sentences.\n","M2 Iter 1: LLM selected few items (1). Lowering sBERT threshold to 0.350\n","DEBUG 2. End Iter 1: Total evidence found: 1\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 22\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 2: Sending 21 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 2: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","   Lowering threshold to 0.300 for next try.\n","DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 27\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 3: Sending 26 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 3: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","M2 Iter 3: Stopping because LLM found no evidence after two tries.\n","M2: Finished. Selected 1 evidence items.\n","-------------------------------------------------------------------\n","\n","###### M3: Starting Classification ######\n","DEBUG 3.1 (module_3_classification):\n","\tClaim: Grease had bad reviews.\n","\tEvidence Texts Sent (1):\n","\tPrompt (partial): Based ONLY on the following evidence sentences, classify the claim as SUPPORTS, REFUTES, or NOT ENOUGH INFO.\n","\n","Claim: 'Grease had bad reviews.'\n","\n","Evidence:\n","- Despite breakthrough roles for Pfeiffer, Adr...\n","-_--_--_--_--_--_--_--_--_--_-\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing Claims:   3%|▎         | 1/30 [00:31<15:12, 31.47s/it]"]},{"output_type":"stream","name":"stdout","text":["DEBUG 3.2/3.3 (module_3_classification):\n","\tLLM Classification Result: SUPPORTS\n","\tExit Status: OK\n","-_--_--_--_--_--_--_--_--_--_-\n","Adding gold label/evidence to prediction.\n","##########################################################################\n","\n","##########################################################################\n","\n","Time to process claim: 30.62 seconds.\n","------------------------------------------------\n","\n","\n","--- Processing Claim ID: 163803 ---\n","DEBUG 1.0 (query_generator):\n","\tClaim: Ukrainian Soviet Socialist Republic was a founding participant of the UN.\n","\tEntities: ['Socialist Republic', 'Ukrainian', 'Soviet', 'UN']\n","\tLLM Output: ['ukrainian_soviet_socialist_republic', 'founding_members_of_the_un', 'united_nations', 'history_of_the_united_nations', 'soviet_union', 'united_nations_general_assembly']\n","-_--_--_--_--_-\n","DEBUG 1.1 (query_generator):\n","\tEntities: ['Socialist Republic', 'Ukrainian', 'Soviet', 'UN']\n","\tGenerated Potential Titles: ['soviet_union', 'united_nations', 'founding_members_of_the_un', 'history_of_the_united_nations', 'united_nations_general_assembly', 'ukrainian_soviet_socialist_republic']\n","\tSelected Titles for Retrieval: ['soviet_union', 'united_nations', 'founding_members_of_the_un', 'history_of_the_united_nations', 'united_nations_general_assembly', 'ukrainian_soviet_socialist_republic']\n","-_--_--_--_--_--_--_--_--_--_-\n","###### M1: Retrieving Documents ######\n","DEBUG 1.2: Successfully retrieved intro from 'Republics of the Soviet Union' (searched for 'soviet_union')\n","DEBUG 1.2: Successfully retrieved intro from 'Member states of the United Nations' (searched for 'united_nations')\n","DEBUG 1.2: Successfully retrieved intro from 'Byelorussian Soviet Socialist Republic' (searched for 'founding_members_of_the_un')\n","DEBUG 1.2: Successfully retrieved intro from 'History of the United Nations' (searched for 'history_of_the_united_nations')\n","DEBUG 1.2: Successfully retrieved intro from 'Member states of the United Nations' (searched for 'united_nations_general_assembly')\n","DEBUG 1.2: Successfully retrieved intro from 'Ukrainian Soviet Socialist Republic' (searched for 'ukrainian_soviet_socialist_republic')\n","DEBUG 1.2: Retrieved content for 6 pages out of 6 potential titles.\n","-_--_--_--_--_--_--_--_--_--_-\n","-------------------------------------------------------------------\n","\n","###### M2: Starting Evidence Extraction ######\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Ukrainian Soviet Socialist Republic was a founding participant of the UN.\n","\tRephrased Claim: The Ukrainian Soviet Socialist Republic was one of the original members that helped establish the United Nations.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Ukrainian Soviet Socialist Republic was a founding participant of the UN.\n","\tRephrased Claim: The Ukrainian Soviet Socialist Republic was one of the original members that took part in establishing the UN.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Ukrainian Soviet Socialist Republic was a founding participant of the UN.\n","\tRephrased Claim: The Ukrainian Soviet Socialist Republic was one of the original members of the United Nations.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Ukrainian Soviet Socialist Republic was a founding participant of the UN.\n","\tRephrased Claim: The Ukrainian Soviet Socialist Republic was one of the original members of the United Nations.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Ukrainian Soviet Socialist Republic was a founding participant of the UN.\n","\tRephrased Claim: The Ukrainian Soviet Socialist Republic was one of the original members of the United Nations.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Ukrainian Soviet Socialist Republic was a founding participant of the UN.\n","\tRephrased Claim: The Ukrainian Soviet Socialist Republic was one of the original members involved in the establishment of the United Nations.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Ukrainian Soviet Socialist Republic was a founding participant of the UN.\n","\tRephrased Claim: The Ukrainian Soviet Socialist Republic was one of the original members involved in the establishment of the United Nations.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Ukrainian Soviet Socialist Republic was a founding participant of the UN.\n","\tRephrased Claim: The Ukrainian Soviet Socialist Republic was one of the original members involved in the establishment of the United Nations.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Ukrainian Soviet Socialist Republic was a founding participant of the UN.\n","\tRephrased Claim: The Ukrainian Soviet Socialist Republic was one of the original members involved in the establishment of the UN.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Ukrainian Soviet Socialist Republic was a founding participant of the UN.\n","\tRephrased Claim: The Ukrainian Soviet Socialist Republic was one of the original members of the United Nations.\n","-_--_--_--_--_-\n","DEBUG 2.1 (module_2_controls):\n","\tClaim: Ukrainian Soviet Socialist Republic was a founding participant of the UN.\n","\tEntities: ['Socialist Republic', 'Ukrainian', 'Soviet', 'UN']\n","\tKeywords: ['ukrainian', 'soviet', 'socialist', 'republic', 'participant']\n","\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n","\tMax Evidence Target: 8, Max Iterations: 10\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 29\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 1: Sending 29 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","1. As a Soviet quasi-state, the Ukrainian SSR became a founding member of the United Nations in 1945 alongside the Byelorussian SSR, in spite of the fact that they were also legally represented by the Soviet Union in foreign affairs.\n","4. The Ukrainian Soviet Socialist Republic, abbreviated as the Ukrainian SSR, UkrSSR, and also known as Soviet Ukraine or just Ukraine, was one of the constituent republics of the Soviet Union from 1922 until 1991.\n","21. In 1922, it was one of four Soviet republics (with the Russian SFSR, the Byelorussian SSR, and the Transcaucasian SFSR) that signed the Treaty on the Creation of the Soviet Union.\n","-_--_--_--_--_-\n","M2 Iter 1: LLM selected 3 sentences.\n","M2 Iter 1: LLM selected few items (3). Lowering sBERT threshold to 0.350\n","DEBUG 2. End Iter 1: Total evidence found: 3\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 40\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 2: Sending 37 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 2: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","   Lowering threshold to 0.300 for next try.\n","DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 44\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 3: Sending 41 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 3: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","M2 Iter 3: Stopping because LLM found no evidence after two tries.\n","M2: Finished. Selected 3 evidence items.\n","-------------------------------------------------------------------\n","\n","###### M3: Starting Classification ######\n","DEBUG 3.1 (module_3_classification):\n","\tClaim: Ukrainian Soviet Socialist Republic was a founding participant of the UN.\n","\tEvidence Texts Sent (3):\n","\tPrompt (partial): Based ONLY on the following evidence sentences, classify the claim as SUPPORTS, REFUTES, or NOT ENOUGH INFO.\n","\n","Claim: 'Ukrainian Soviet Socialist Republic was a founding participant of the UN.'\n","\n","Eviden...\n","-_--_--_--_--_--_--_--_--_--_-\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing Claims:   7%|▋         | 2/30 [00:58<13:31, 29.00s/it]"]},{"output_type":"stream","name":"stdout","text":["DEBUG 3.2/3.3 (module_3_classification):\n","\tLLM Classification Result: SUPPORTS\n","\tExit Status: OK\n","-_--_--_--_--_--_--_--_--_--_-\n","Adding gold label/evidence to prediction.\n","##########################################################################\n","\n","##########################################################################\n","\n","Time to process claim: 27.26 seconds.\n","------------------------------------------------\n","\n","\n","--- Processing Claim ID: 70041 ---\n","DEBUG 1.0 (query_generator):\n","\tClaim: 2 Hearts is a musical composition by Minogue.\n","\tEntities: ['Minogue']\n","\tLLM Output: ['kylie_minogue', '2_hearts', 'list_of_songs_recorded_by_kylie_minogue']\n","-_--_--_--_--_-\n","DEBUG 1.1 (query_generator):\n","\tEntities: ['Minogue']\n","\tGenerated Potential Titles: ['2_hearts', 'kylie_minogue', 'list_of_songs_recorded_by_kylie_minogue']\n","\tSelected Titles for Retrieval: ['2_hearts', 'kylie_minogue', 'list_of_songs_recorded_by_kylie_minogue']\n","-_--_--_--_--_--_--_--_--_--_-\n","###### M1: Retrieving Documents ######\n","DEBUG 1.2: Successfully retrieved intro from '2 Hearts (film)' (searched for '2_hearts')\n","DEBUG 1.2: Successfully retrieved intro from 'Kylie Minogue (album)' (searched for 'kylie_minogue')\n","DEBUG 1.2: Successfully retrieved intro from 'Kylie Minogue (album)' (searched for 'list_of_songs_recorded_by_kylie_minogue')\n","DEBUG 1.2: Retrieved content for 3 pages out of 3 potential titles.\n","-_--_--_--_--_--_--_--_--_--_-\n","-------------------------------------------------------------------\n","\n","###### M2: Starting Evidence Extraction ######\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: 2 Hearts is a musical composition by Minogue.\n","\tRephrased Claim: \"2 Hearts\" is a musical work created by Minogue.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: 2 Hearts is a musical composition by Minogue.\n","\tRephrased Claim: \"2 Hearts\" is a musical piece created by Minogue.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: 2 Hearts is a musical composition by Minogue.\n","\tRephrased Claim: \"2 Hearts\" is a musical piece created by Minogue.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: 2 Hearts is a musical composition by Minogue.\n","\tRephrased Claim: \"2 Hearts\" is a musical piece created by Minogue.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: 2 Hearts is a musical composition by Minogue.\n","\tRephrased Claim: \"2 Hearts\" is a musical work created by Minogue.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: 2 Hearts is a musical composition by Minogue.\n","\tRephrased Claim: \"2 Hearts\" is a musical piece created by Minogue.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: 2 Hearts is a musical composition by Minogue.\n","\tRephrased Claim: \"2 Hearts\" is a musical piece created by Minogue.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: 2 Hearts is a musical composition by Minogue.\n","\tRephrased Claim: \"2 Hearts\" is a musical piece by Minogue.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: 2 Hearts is a musical composition by Minogue.\n","\tRephrased Claim: \"2 Hearts\" is a musical piece created by Minogue.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: 2 Hearts is a musical composition by Minogue.\n","\tRephrased Claim: \"2 Hearts\" is a musical work created by Minogue.\n","-_--_--_--_--_-\n","DEBUG 2.1 (module_2_controls):\n","\tClaim: 2 Hearts is a musical composition by Minogue.\n","\tEntities: ['Minogue']\n","\tKeywords: ['musical', 'minogue', 'hearts', 'composition']\n","\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n","\tMax Evidence Target: 8, Max Iterations: 10\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 5\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 1: Sending 5 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 1: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","   Lowering threshold to 0.350 for next try.\n","DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 11\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 2: Sending 11 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 2: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","   Lowering threshold to 0.300 for next try.\n","DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 11\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 3: Sending 11 new candidates to LLM for selection.\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing Claims:  10%|█         | 3/30 [01:13<10:10, 22.62s/it]"]},{"output_type":"stream","name":"stdout","text":["DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 3: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","M2 Iter 3: Stopping because LLM found no evidence after two tries.\n","M2: Finished iterations. No evidence selected.\n","-------------------------------------------------------------------\n","\n","###### M3: Starting Classification ######\n","M3: No evidence text provided. Classifying as NOT ENOUGH INFO.\n","Adding gold label/evidence to prediction.\n","##########################################################################\n","\n","##########################################################################\n","\n","Time to process claim: 15.03 seconds.\n","------------------------------------------------\n","\n","\n","--- Processing Claim ID: 202314 ---\n","DEBUG 1.0 (query_generator):\n","\tClaim: The New Jersey Turnpike has zero shoulders.\n","\tEntities: ['New Jersey Turnpike']\n","\tLLM Output: ['new_jersey_turnpike', 'highway_safety', 'road_design', 'shoulder_(road)', 'road_infrastructure']\n","-_--_--_--_--_-\n","DEBUG 1.1 (query_generator):\n","\tEntities: ['New Jersey Turnpike']\n","\tGenerated Potential Titles: ['road_design', 'highway_safety', 'shoulder_(road)', 'road_infrastructure', 'new_jersey_turnpike']\n","\tSelected Titles for Retrieval: ['road_design', 'highway_safety', 'shoulder_(road)', 'road_infrastructure', 'new_jersey_turnpike']\n","-_--_--_--_--_--_--_--_--_--_-\n","###### M1: Retrieving Documents ######\n","DEBUG 1.2: Successfully retrieved intro from 'Cycling infrastructure' (searched for 'road_design')\n","DEBUG 1.2: Successfully retrieved intro from 'National Highway Traffic Safety Administration' (searched for 'highway_safety')\n","DEBUG 1.2: Successfully retrieved intro from 'Shoulder (road)' (searched for 'shoulder_(road)')\n","DEBUG 1.2: Successfully retrieved intro from 'Toll road' (searched for 'road_infrastructure')\n","DEBUG 1.2: Successfully retrieved intro from 'New Jersey Turnpike' (searched for 'new_jersey_turnpike')\n","DEBUG 1.2: Retrieved content for 5 pages out of 5 potential titles.\n","-_--_--_--_--_--_--_--_--_--_-\n","-------------------------------------------------------------------\n","\n","###### M2: Starting Evidence Extraction ######\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: The New Jersey Turnpike has zero shoulders.\n","\tRephrased Claim: The New Jersey Turnpike does not have any shoulders.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: The New Jersey Turnpike has zero shoulders.\n","\tRephrased Claim: The New Jersey Turnpike has no shoulders.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: The New Jersey Turnpike has zero shoulders.\n","\tRephrased Claim: The New Jersey Turnpike has no shoulders.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: The New Jersey Turnpike has zero shoulders.\n","\tRephrased Claim: The New Jersey Turnpike has no shoulders at all.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: The New Jersey Turnpike has zero shoulders.\n","\tRephrased Claim: The New Jersey Turnpike has no shoulders.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: The New Jersey Turnpike has zero shoulders.\n","\tRephrased Claim: The New Jersey Turnpike has no shoulders.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: The New Jersey Turnpike has zero shoulders.\n","\tRephrased Claim: The New Jersey Turnpike does not have any shoulders.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: The New Jersey Turnpike has zero shoulders.\n","\tRephrased Claim: The New Jersey Turnpike lacks any shoulders.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: The New Jersey Turnpike has zero shoulders.\n","\tRephrased Claim: The New Jersey Turnpike does not have any shoulders.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: The New Jersey Turnpike has zero shoulders.\n","\tRephrased Claim: The New Jersey Turnpike has no shoulders.\n","-_--_--_--_--_-\n","DEBUG 2.1 (module_2_controls):\n","\tClaim: The New Jersey Turnpike has zero shoulders.\n","\tEntities: ['New Jersey Turnpike']\n","\tKeywords: ['zero', 'turnpike', 'shoulders', 'new', 'jersey']\n","\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n","\tMax Evidence Target: 8, Max Iterations: 10\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 7\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 1: Sending 7 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 1: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","   Lowering threshold to 0.350 for next try.\n","DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 13\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 2: Sending 13 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 2: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","   Lowering threshold to 0.300 for next try.\n","DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 14\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 3: Sending 14 new candidates to LLM for selection.\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing Claims:  13%|█▎        | 4/30 [01:32<09:03, 20.91s/it]"]},{"output_type":"stream","name":"stdout","text":["DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 3: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","M2 Iter 3: Stopping because LLM found no evidence after two tries.\n","M2: Finished iterations. No evidence selected.\n","-------------------------------------------------------------------\n","\n","###### M3: Starting Classification ######\n","M3: No evidence text provided. Classifying as NOT ENOUGH INFO.\n","Adding gold label/evidence to prediction.\n","##########################################################################\n","\n","##########################################################################\n","\n","Time to process claim: 18.29 seconds.\n","------------------------------------------------\n","\n","\n","--- Processing Claim ID: 57085 ---\n","DEBUG 1.0 (query_generator):\n","\tClaim: Legendary Entertainment is the owner of Wanda Cinemas.\n","\tEntities: ['Wanda Cinemas']\n","\tLLM Output: ['wanda_cinemas', 'legendary_entertainment', 'wanda_group']\n","-_--_--_--_--_-\n","DEBUG 1.1 (query_generator):\n","\tEntities: ['Wanda Cinemas']\n","\tGenerated Potential Titles: ['wanda_group', 'wanda_cinemas', 'legendary_entertainment']\n","\tSelected Titles for Retrieval: ['wanda_group', 'wanda_cinemas', 'legendary_entertainment']\n","-_--_--_--_--_--_--_--_--_--_-\n","###### M1: Retrieving Documents ######\n","DEBUG 1.2: Successfully retrieved intro from 'Wanda Media' (searched for 'wanda_group')\n","DEBUG 1.2: Successfully retrieved intro from 'Wanda Film' (searched for 'wanda_cinemas')\n","DEBUG 1.2: Successfully retrieved intro from 'Legendary Entertainment' (searched for 'legendary_entertainment')\n","DEBUG 1.2: Retrieved content for 3 pages out of 3 potential titles.\n","-_--_--_--_--_--_--_--_--_--_-\n","-------------------------------------------------------------------\n","\n","###### M2: Starting Evidence Extraction ######\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Legendary Entertainment is the owner of Wanda Cinemas.\n","\tRephrased Claim: Wanda Cinemas is owned by Legendary Entertainment.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Legendary Entertainment is the owner of Wanda Cinemas.\n","\tRephrased Claim: Wanda Cinemas is owned by Legendary Entertainment.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Legendary Entertainment is the owner of Wanda Cinemas.\n","\tRephrased Claim: Wanda Cinemas is owned by Legendary Entertainment.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Legendary Entertainment is the owner of Wanda Cinemas.\n","\tRephrased Claim: Wanda Cinemas is owned by Legendary Entertainment.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Legendary Entertainment is the owner of Wanda Cinemas.\n","\tRephrased Claim: Wanda Cinemas is owned by Legendary Entertainment.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Legendary Entertainment is the owner of Wanda Cinemas.\n","\tRephrased Claim: Wanda Cinemas is owned by Legendary Entertainment.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Legendary Entertainment is the owner of Wanda Cinemas.\n","\tRephrased Claim: Wanda Cinemas is owned by Legendary Entertainment.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Legendary Entertainment is the owner of Wanda Cinemas.\n","\tRephrased Claim: Wanda Cinemas is owned by Legendary Entertainment.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Legendary Entertainment is the owner of Wanda Cinemas.\n","\tRephrased Claim: Wanda Cinemas is owned by Legendary Entertainment.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Legendary Entertainment is the owner of Wanda Cinemas.\n","\tRephrased Claim: Wanda Cinemas is owned by Legendary Entertainment.\n","-_--_--_--_--_-\n","DEBUG 2.1 (module_2_controls):\n","\tClaim: Legendary Entertainment is the owner of Wanda Cinemas.\n","\tEntities: ['Wanda Cinemas']\n","\tKeywords: ['wanda', 'owner', 'legendary', 'entertainment', 'cinemas']\n","\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n","\tMax Evidence Target: 8, Max Iterations: 10\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 11\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 1: Sending 11 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","7. In 2024, Legendary Entertainement bought out Wanda's stake, making Legendary and Apollo equal partners.  \n","9. In 2016, Legendary became a subsidiary of the Chinese conglomerate Wanda Group, with American equity firm Apollo buying a minority stake in 2022.  \n","8. It is a part of the Dalian Wanda Group.\n","-_--_--_--_--_-\n","M2 Iter 1: LLM selected 3 sentences.\n","DEBUG 2. End Iter 1: Total evidence found: 3\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.400\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 12\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 2: Sending 9 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 2: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","   Lowering threshold to 0.350 for next try.\n","DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.350\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 12\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 3: Sending 9 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 3: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","M2 Iter 3: Stopping because LLM found no evidence after two tries.\n","M2: Finished. Selected 3 evidence items.\n","-------------------------------------------------------------------\n","\n","###### M3: Starting Classification ######\n","DEBUG 3.1 (module_3_classification):\n","\tClaim: Legendary Entertainment is the owner of Wanda Cinemas.\n","\tEvidence Texts Sent (3):\n","\tPrompt (partial): Based ONLY on the following evidence sentences, classify the claim as SUPPORTS, REFUTES, or NOT ENOUGH INFO.\n","\n","Claim: 'Legendary Entertainment is the owner of Wanda Cinemas.'\n","\n","Evidence:\n","- It is a part ...\n","-_--_--_--_--_--_--_--_--_--_-\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing Claims:  17%|█▋        | 5/30 [01:45<07:37, 18.29s/it]"]},{"output_type":"stream","name":"stdout","text":["DEBUG 3.2/3.3 (module_3_classification):\n","\tLLM Classification Result: REFUTES\n","\tExit Status: OK\n","-_--_--_--_--_--_--_--_--_--_-\n","Adding gold label/evidence to prediction.\n","##########################################################################\n","\n","##########################################################################\n","\n","Time to process claim: 13.63 seconds.\n","------------------------------------------------\n","\n","\n","--- Processing Claim ID: 6032 ---\n","DEBUG 1.0 (query_generator):\n","\tClaim: Aruba is the only ABC Island.\n","\tEntities: ['ABC Island', 'Aruba']\n","\tLLM Output: ['abc_islands', 'aruba']\n","-_--_--_--_--_-\n","DEBUG 1.1 (query_generator):\n","\tEntities: ['ABC Island', 'Aruba']\n","\tGenerated Potential Titles: ['aruba', 'abc_islands']\n","\tSelected Titles for Retrieval: ['aruba', 'abc_islands']\n","-_--_--_--_--_--_--_--_--_--_-\n","###### M1: Retrieving Documents ######\n","DEBUG 1.2: Successfully retrieved intro from 'Aruba' (searched for 'aruba')\n","DEBUG 1.2: Successfully retrieved intro from 'Aruba' (searched for 'abc_islands')\n","DEBUG 1.2: Retrieved content for 2 pages out of 2 potential titles.\n","-_--_--_--_--_--_--_--_--_--_-\n","-------------------------------------------------------------------\n","\n","###### M2: Starting Evidence Extraction ######\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Aruba is the only ABC Island.\n","\tRephrased Claim: Aruba is the sole island among the ABC Islands.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Aruba is the only ABC Island.\n","\tRephrased Claim: Aruba is the sole island among the ABC Islands.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Aruba is the only ABC Island.\n","\tRephrased Claim: Aruba is the sole ABC Island.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Aruba is the only ABC Island.\n","\tRephrased Claim: Aruba is the sole island among the ABC Islands.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Aruba is the only ABC Island.\n","\tRephrased Claim: Aruba is the sole island of the ABC Islands.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Aruba is the only ABC Island.\n","\tRephrased Claim: Aruba is the sole island among the ABC Islands.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Aruba is the only ABC Island.\n","\tRephrased Claim: Aruba is the sole island among the ABC Islands.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Aruba is the only ABC Island.\n","\tRephrased Claim: Aruba is the sole island among the ABC Islands.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Aruba is the only ABC Island.\n","\tRephrased Claim: Aruba is the sole island among the ABC Islands.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Aruba is the only ABC Island.\n","\tRephrased Claim: Aruba is the sole island among the ABC Islands.\n","-_--_--_--_--_-\n","DEBUG 2.1 (module_2_controls):\n","\tClaim: Aruba is the only ABC Island.\n","\tEntities: ['ABC Island', 'Aruba']\n","\tKeywords: ['island', 'aruba', 'abc']\n","\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n","\tMax Evidence Target: 8, Max Iterations: 10\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 20\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 1: Sending 20 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","1. Alongside Bonaire and Curaçao, Aruba forms a group referred to as the ABC islands.\n","2. Alongside Bonaire and Curaçao, Aruba forms a group referred to as the ABC islands.\n","-_--_--_--_--_-\n","M2 Iter 1: LLM selected 2 sentences.\n","   Warning: Could not re-associate LLM output with retrieved data: 'Alongside Bonaire and Curaçao, Aruba forms a group referred to as the ABC island...'\n","M2 Iter 1: LLM selected few items (1). Lowering sBERT threshold to 0.350\n","DEBUG 2. End Iter 1: Total evidence found: 1\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 22\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 2: Sending 20 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","1. The Dutch Caribbean encompasses the ABC islands along with the other three substantial islands, the SSS islands.\n","-_--_--_--_--_-\n","M2 Iter 2: LLM selected 1 sentences.\n","M2 Iter 2: LLM selected few items (1). Lowering sBERT threshold to 0.300\n","DEBUG 2. End Iter 2: Total evidence found: 2\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 22\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 3: Sending 18 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 3: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","M2 Iter 3: Stopping because LLM found no evidence after two tries.\n","M2: Finished. Selected 2 evidence items.\n","-------------------------------------------------------------------\n","\n","###### M3: Starting Classification ######\n","DEBUG 3.1 (module_3_classification):\n","\tClaim: Aruba is the only ABC Island.\n","\tEvidence Texts Sent (2):\n","\tPrompt (partial): Based ONLY on the following evidence sentences, classify the claim as SUPPORTS, REFUTES, or NOT ENOUGH INFO.\n","\n","Claim: 'Aruba is the only ABC Island.'\n","\n","Evidence:\n","- Alongside Bonaire and Curaçao, Aruba f...\n","-_--_--_--_--_--_--_--_--_--_-\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing Claims:  20%|██        | 6/30 [02:03<07:17, 18.23s/it]"]},{"output_type":"stream","name":"stdout","text":["DEBUG 3.2/3.3 (module_3_classification):\n","\tLLM Classification Result: REFUTES\n","\tExit Status: OK\n","-_--_--_--_--_--_--_--_--_--_-\n","Adding gold label/evidence to prediction.\n","##########################################################################\n","\n","##########################################################################\n","\n","Time to process claim: 18.11 seconds.\n","------------------------------------------------\n","\n","\n","--- Processing Claim ID: 176630 ---\n","DEBUG 1.0 (query_generator):\n","\tClaim: Great white sharks do not prefer dolphins as prey.\n","\tEntities: ['Great']\n","\tLLM Output: ['great_white_shark', 'dolphin', 'predation', 'marine_ecosystem']\n","-_--_--_--_--_-\n","DEBUG 1.1 (query_generator):\n","\tEntities: ['Great']\n","\tGenerated Potential Titles: ['dolphin', 'predation', 'marine_ecosystem', 'great_white_shark']\n","\tSelected Titles for Retrieval: ['dolphin', 'predation', 'marine_ecosystem', 'great_white_shark']\n","-_--_--_--_--_--_--_--_--_--_-\n","###### M1: Retrieving Documents ######\n","DEBUG 1.2: Successfully retrieved intro from 'Dolphin' (searched for 'dolphin')\n","DEBUG 1.2: Successfully retrieved intro from 'Anti-predator adaptation' (searched for 'predation')\n","DEBUG 1.2: Successfully retrieved intro from 'Marine mammal' (searched for 'marine_ecosystem')\n","DEBUG 1.2: Successfully retrieved intro from 'Great white shark' (searched for 'great_white_shark')\n","DEBUG 1.2: Retrieved content for 4 pages out of 4 potential titles.\n","-_--_--_--_--_--_--_--_--_--_-\n","-------------------------------------------------------------------\n","\n","###### M2: Starting Evidence Extraction ######\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Great white sharks do not prefer dolphins as prey.\n","\tRephrased Claim: Great white sharks do not have a preference for dolphins as their food source.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Great white sharks do not prefer dolphins as prey.\n","\tRephrased Claim: Great white sharks do not consider dolphins a favored food source.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Great white sharks do not prefer dolphins as prey.\n","\tRephrased Claim: Great white sharks do not choose dolphins as their preferred food.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Great white sharks do not prefer dolphins as prey.\n","\tRephrased Claim: Great white sharks do not favor dolphins as a food source.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Great white sharks do not prefer dolphins as prey.\n","\tRephrased Claim: Great white sharks do not choose dolphins as their favored prey.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Great white sharks do not prefer dolphins as prey.\n","\tRephrased Claim: Great white sharks do not typically choose dolphins as their food source.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Great white sharks do not prefer dolphins as prey.\n","\tRephrased Claim: Great white sharks do not regard dolphins as their preferred food.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Great white sharks do not prefer dolphins as prey.\n","\tRephrased Claim: Great white sharks do not favor dolphins as a food source.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Great white sharks do not prefer dolphins as prey.\n","\tRephrased Claim: Great white sharks do not favor dolphins as a food source.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Great white sharks do not prefer dolphins as prey.\n","\tRephrased Claim: Great white sharks do not have a preference for dolphins as their food source.\n","-_--_--_--_--_-\n","DEBUG 2.1 (module_2_controls):\n","\tClaim: Great white sharks do not prefer dolphins as prey.\n","\tEntities: ['Great']\n","\tKeywords: ['white', 'sharks', 'prey', 'prefer', 'great']\n","\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n","\tMax Evidence Target: 8, Max Iterations: 10\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 33\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 1: Sending 33 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","2. The great white shark is arguably the world's largest-known extant macropredatory fish, and is one of the primary predators of marine mammals, such as pinnipeds and dolphins.  \n","32. The diets of marine mammals vary considerably as well; some eat zooplankton, others eat fish, squid, shellfish, or seagrass, and a few eat other mammals.\n","-_--_--_--_--_-\n","M2 Iter 1: LLM selected 2 sentences.\n","M2 Iter 1: LLM selected few items (2). Lowering sBERT threshold to 0.350\n","DEBUG 2. End Iter 1: Total evidence found: 2\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 41\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 2: Sending 39 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","The great white shark is also known to prey upon a variety of other animals, including fish, other sharks, and seabirds.  \n","NOT ENOUGH INFO.\n","-_--_--_--_--_-\n","Warning: LLM output contained 'NOT ENOUGH INFO' along with other text. Interpreting as NEI.\n","M2 Iter 2: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","   Lowering threshold to 0.300 for next try.\n","DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 44\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 3: Sending 42 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 3: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","M2 Iter 3: Stopping because LLM found no evidence after two tries.\n","M2: Finished. Selected 2 evidence items.\n","-------------------------------------------------------------------\n","\n","###### M3: Starting Classification ######\n","DEBUG 3.1 (module_3_classification):\n","\tClaim: Great white sharks do not prefer dolphins as prey.\n","\tEvidence Texts Sent (2):\n","\tPrompt (partial): Based ONLY on the following evidence sentences, classify the claim as SUPPORTS, REFUTES, or NOT ENOUGH INFO.\n","\n","Claim: 'Great white sharks do not prefer dolphins as prey.'\n","\n","Evidence:\n","- The diets of mari...\n","-_--_--_--_--_--_--_--_--_--_-\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing Claims:  23%|██▎       | 7/30 [02:25<07:25, 19.36s/it]"]},{"output_type":"stream","name":"stdout","text":["DEBUG 3.2/3.3 (module_3_classification):\n","\tLLM Classification Result: REFUTES\n","\tExit Status: OK\n","-_--_--_--_--_--_--_--_--_--_-\n","Adding gold label/evidence to prediction.\n","##########################################################################\n","\n","##########################################################################\n","\n","Time to process claim: 21.70 seconds.\n","------------------------------------------------\n","\n","\n","--- Processing Claim ID: 130048 ---\n","DEBUG 1.0 (query_generator):\n","\tClaim: Burbank, California has always been completely void of industry.\n","\tEntities: ['California', 'Burbank']\n","\tLLM Output: ['burbank,_california', 'history_of_burbank,_california', 'economy_of_burbank,_california', 'california', 'industry_in_california']\n","-_--_--_--_--_-\n","DEBUG 1.1 (query_generator):\n","\tEntities: ['California', 'Burbank']\n","\tGenerated Potential Titles: ['california', 'burbank,_california', 'industry_in_california', 'economy_of_burbank,_california', 'history_of_burbank,_california']\n","\tSelected Titles for Retrieval: ['california', 'burbank,_california', 'industry_in_california', 'economy_of_burbank,_california', 'history_of_burbank,_california']\n","-_--_--_--_--_--_--_--_--_--_-\n","###### M1: Retrieving Documents ######\n","DEBUG 1.2: DisambiguationError for title: 'california'. Options: ['California (1927 film)', 'California (1947 film)', 'California (1963 film)', 'California (1977 film)', 'Hopalong Cassidy', 'Robert California', 'Les Fradkin', 'California (American Music Club album)', 'California (Blink-182 album)', 'California (The Electric Prunes album)', 'California (Gianna Nannini album)', 'California (Mr. Bungle album)', 'California (Wilson Phillips album)', 'California (Datarock EP)', 'California (Diplo EP)', 'Riverdogs', '\"California\" (Amelia Lily song)', '\"California\" (Belinda Carlisle song)', '\"California\" (Big & Rich song)', '\"California\" (Delta Spirit song)', '\"California\" (Joni Mitchell song)', '\"California\" (Lenny Kravitz song)', '\"California\" (Mylène Farmer song)', '\"California\" (Phantom Planet song)', '\"California\" (Usher song)', '\"California\" (Wave song)', 'This Is How Tomorrow Moves', 'NCIS: The Official TV Soundtrack - Vol. 2', 'The Rise and Fall of a Midwest Princess', '\"Awaken, My Love!\"', 'Rockit', 'Screen Violence', 'Midstream', 'West a Fool Away', 'Art Angels', 'Swan Songs', 'The Afterlove', 'The Turning Point', 'Keith Stegall', 'Norman Fucking Rockwell!', 'Solar Power', 'The Great Destroyer', 'Stories Since Seventy Nine', 'Seasons of Your Day', 'Metro Station', 'What Is Love?', 'Cowboy Tears', 'Rich Brian', 'Poses', 'Feeling Strangely Fine', 'This Is How the Wind Shifts', 'Northwest Passage', 'The Band Camino', 'The Growlers', 'Damn Country Music', 'Songs and Music from \"She\\'s the One\"', 'twlv', '13 Unlucky Numbers', 'Hit Parade 1', \"Bet You Think I'm Lonely\", 'Against the World', 'New Gods', 'Generation Rx', 'Songs of Innocence', 'Las sergas de Esplandián', 'California (novel)', 'California (sculpture)', 'Star Trek Lower Decks', 'USS California', 'California-class cruiser', 'ARM California', 'California Molefe', 'California Odha Zertuche Díaz', 'Randy California', 'California Republic', 'The Californias', 'Province of Las Californias', 'California Department', 'Roman Catholic Diocese of California', 'Island of California', 'California, Bedfordshire', 'California, Berkshire', 'California, Birmingham', 'California, Buckinghamshire', 'California, Derby', 'California, Falkirk', 'California, Ipswich', 'California, Norfolk', 'California City, California', 'California, Kentucky', 'California, Louisville', 'California, Maine', 'California, Maryland', 'California Township, Michigan', 'California, Missouri', 'Califon, New Jersey', 'California, Cincinnati', 'Big Plain, Ohio', 'California, Pennsylvania', 'California, West Virginia', 'Califórnia, Paraná', 'California, Ontario', 'California, Santander', 'California, Usulután', 'La California', 'California, Ubay', 'California, Trinidad and Tobago', 'California Nebula', '341 California', 'Gulf of California', 'University of California', 'California Golden Bears', 'California University of Science and Medicine', 'California University of Pennsylvania', 'California Vulcans', 'California High School (San Ramon, California)', 'California High School (Whittier, California)', 'Orange Unified School District', 'Ferrari California', 'Moto Guzzi California', 'Volkswagen California', 'Chevrolet Corvette', 'Ferrari 250 GT California Spyder', 'Ferrari 365 California', 'SS California', 'Northwest Passage expedition of 1746', 'California station (CTA Blue Line)', 'California station (CTA Congress Line)', 'California station (CTA Green Line)', 'California station (CTA Pink Line)', 'La California station', 'Hotel California (disambiguation)', 'California magazine', 'California macrophylla', 'Alta California (disambiguation)', 'Baja California (disambiguation)', 'California Star (disambiguation)', 'Californian (disambiguation)', 'Californië (disambiguation)', 'Californios', 'Californium', 'Kalifornia (disambiguation)', 'All pages with titles beginning with California', 'All pages with titles containing California']...\n","DEBUG 1.2: Successfully retrieved intro from 'California Republic' (disambiguated to 'California Republic')\n","DEBUG 1.2: Successfully retrieved intro from 'Burbank, California' (searched for 'burbank,_california')\n","DEBUG 1.2: Successfully retrieved intro from 'City of Industry, California' (searched for 'industry_in_california')\n","DEBUG 1.2: Successfully retrieved intro from 'Burbank, California' (searched for 'economy_of_burbank,_california')\n","DEBUG 1.2: Successfully retrieved intro from 'Burbank, California' (searched for 'history_of_burbank,_california')\n","DEBUG 1.2: Retrieved content for 5 pages out of 5 potential titles.\n","-_--_--_--_--_--_--_--_--_--_-\n","-------------------------------------------------------------------\n","\n","###### M2: Starting Evidence Extraction ######\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Burbank, California has always been completely void of industry.\n","\tRephrased Claim: Burbank, California has historically lacked any industrial presence.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Burbank, California has always been completely void of industry.\n","\tRephrased Claim: Burbank, California has consistently lacked any industrial presence.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Burbank, California has always been completely void of industry.\n","\tRephrased Claim: Burbank, California has consistently lacked any industry.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Burbank, California has always been completely void of industry.\n","\tRephrased Claim: Burbank, California has consistently had no industry at all.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Burbank, California has always been completely void of industry.\n","\tRephrased Claim: Burbank, California has historically lacked any industry.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Burbank, California has always been completely void of industry.\n","\tRephrased Claim: Burbank, California has consistently lacked any form of industry.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Burbank, California has always been completely void of industry.\n","\tRephrased Claim: Burbank, California has consistently lacked any industrial presence.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Burbank, California has always been completely void of industry.\n","\tRephrased Claim: Burbank, California has consistently lacked any form of industry.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Burbank, California has always been completely void of industry.\n","\tRephrased Claim: Burbank, California has historically had no presence of industry whatsoever.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Burbank, California has always been completely void of industry.\n","\tRephrased Claim: Burbank, California has consistently lacked any industrial presence.\n","-_--_--_--_--_-\n","DEBUG 2.1 (module_2_controls):\n","\tClaim: Burbank, California has always been completely void of industry.\n","\tEntities: ['California', 'Burbank']\n","\tKeywords: ['void', 'industry', 'completely', 'california', 'burbank']\n","\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n","\tMax Evidence Target: 8, Max Iterations: 10\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 7\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 1: Sending 7 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","Numerous media and entertainment companies are headquartered or have significant production facilities in Burbank—often called the \"Media Capital of the World\" and only a few miles northeast of Hollywood—including Warner Bros. Entertainment, the Walt Disney Company, Nickelodeon Animation Studio, The Burbank Studios, Cartoon Network Studios with the West Coast branch of Cartoon Network, and Insomniac Games.\n","Numerous media and entertainment companies are headquartered or have significant production facilities in Burbank—often called the \"Media Capital of the World\" and only a few miles northeast of Hollywood—including Warner Bros. Entertainment, the Walt Disney Company, Nickelodeon Animation Studio, The Burbank Studios, Cartoon Network Studios with the West Coast branch of Cartoon Network, and Insomniac Games.\n","Numerous media and entertainment companies are headquartered or have significant production facilities in Burbank—often called the \"Media Capital of the World\" and only a few miles northeast of Hollywood—including Warner Bros. Entertainment, the Walt Disney Company, Nickelodeon Animation Studio, The Burbank Studios, Cartoon Network Studios with the West Coast branch of Cartoon Network, and Insomniac Games.\n","-_--_--_--_--_-\n","M2 Iter 1: LLM selected 3 sentences.\n","   Warning: Could not re-associate LLM output with retrieved data: 'Numerous media and entertainment companies are headquartered or have significant...'\n","   Warning: Could not re-associate LLM output with retrieved data: 'Numerous media and entertainment companies are headquartered or have significant...'\n","M2 Iter 1: LLM selected few items (1). Lowering sBERT threshold to 0.350\n","DEBUG 2. End Iter 1: Total evidence found: 1\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 15\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 2: Sending 12 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 2: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","   Lowering threshold to 0.300 for next try.\n","DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 19\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 3: Sending 16 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 3: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","M2 Iter 3: Stopping because LLM found no evidence after two tries.\n","M2: Finished. Selected 1 evidence items.\n","-------------------------------------------------------------------\n","\n","###### M3: Starting Classification ######\n","DEBUG 3.1 (module_3_classification):\n","\tClaim: Burbank, California has always been completely void of industry.\n","\tEvidence Texts Sent (1):\n","\tPrompt (partial): Based ONLY on the following evidence sentences, classify the claim as SUPPORTS, REFUTES, or NOT ENOUGH INFO.\n","\n","Claim: 'Burbank, California has always been completely void of industry.'\n","\n","Evidence:\n","- Num...\n","-_--_--_--_--_--_--_--_--_--_-\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing Claims:  27%|██▋       | 8/30 [02:51<07:53, 21.52s/it]"]},{"output_type":"stream","name":"stdout","text":["DEBUG 3.2/3.3 (module_3_classification):\n","\tLLM Classification Result: REFUTES\n","\tExit Status: OK\n","-_--_--_--_--_--_--_--_--_--_-\n","Adding gold label/evidence to prediction.\n","##########################################################################\n","\n","##########################################################################\n","\n","Time to process claim: 26.14 seconds.\n","------------------------------------------------\n","\n","\n","--- Processing Claim ID: 100046 ---\n","DEBUG 1.0 (query_generator):\n","\tClaim: The Guthrie Theater's second building began operating in 1963.\n","\tEntities: ['Guthrie']\n","\tLLM Output: ['guthrie_theater', 'guthrie_theater#history']\n","-_--_--_--_--_-\n","DEBUG 1.1 (query_generator):\n","\tEntities: ['Guthrie']\n","\tGenerated Potential Titles: ['guthrie_theater', 'guthrie_theater#history']\n","\tSelected Titles for Retrieval: ['guthrie_theater', 'guthrie_theater#history']\n","-_--_--_--_--_--_--_--_--_--_-\n","###### M1: Retrieving Documents ######\n","DEBUG 1.2: Successfully retrieved intro from 'Guthrie Theater production history' (searched for 'guthrie_theater')\n","DEBUG 1.2: Successfully retrieved intro from 'Guthrie Theater production history' (searched for 'guthrie_theater#history')\n","DEBUG 1.2: Retrieved content for 2 pages out of 2 potential titles.\n","-_--_--_--_--_--_--_--_--_--_-\n","-------------------------------------------------------------------\n","\n","###### M2: Starting Evidence Extraction ######\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: The Guthrie Theater's second building began operating in 1963.\n","\tRephrased Claim: The second building of the Guthrie Theater commenced operations in 1963.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: The Guthrie Theater's second building began operating in 1963.\n","\tRephrased Claim: The second building of the Guthrie Theater commenced operations in 1963.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: The Guthrie Theater's second building began operating in 1963.\n","\tRephrased Claim: The second building of the Guthrie Theater commenced operations in 1963.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: The Guthrie Theater's second building began operating in 1963.\n","\tRephrased Claim: The second structure of the Guthrie Theater commenced operations in 1963.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: The Guthrie Theater's second building began operating in 1963.\n","\tRephrased Claim: The second building of the Guthrie Theater commenced its operations in 1963.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: The Guthrie Theater's second building began operating in 1963.\n","\tRephrased Claim: The second building of the Guthrie Theater started its operations in 1963.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: The Guthrie Theater's second building began operating in 1963.\n","\tRephrased Claim: The second building of the Guthrie Theater commenced operations in 1963.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: The Guthrie Theater's second building began operating in 1963.\n","\tRephrased Claim: The second building of the Guthrie Theater started its operations in 1963.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: The Guthrie Theater's second building began operating in 1963.\n","\tRephrased Claim: The second building of the Guthrie Theater commenced operations in 1963.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: The Guthrie Theater's second building began operating in 1963.\n","\tRephrased Claim: The second building of the Guthrie Theater commenced operations in 1963.\n","-_--_--_--_--_-\n","DEBUG 2.1 (module_2_controls):\n","\tClaim: The Guthrie Theater's second building began operating in 1963.\n","\tEntities: ['Guthrie']\n","\tKeywords: ['theater', 'second', 'operating', 'guthrie', 'building']\n","\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n","\tMax Evidence Target: 8, Max Iterations: 10\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 4\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 1: Sending 4 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 1: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","   Lowering threshold to 0.350 for next try.\n","DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 4\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 2: Sending 4 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 2: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","   Lowering threshold to 0.300 for next try.\n","DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 4\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 3: Sending 4 new candidates to LLM for selection.\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing Claims:  30%|███       | 9/30 [03:03<06:27, 18.47s/it]"]},{"output_type":"stream","name":"stdout","text":["DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 3: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","M2 Iter 3: Stopping because LLM found no evidence after two tries.\n","M2: Finished iterations. No evidence selected.\n","-------------------------------------------------------------------\n","\n","###### M3: Starting Classification ######\n","M3: No evidence text provided. Classifying as NOT ENOUGH INFO.\n","Adding gold label/evidence to prediction.\n","##########################################################################\n","\n","##########################################################################\n","\n","Time to process claim: 11.76 seconds.\n","------------------------------------------------\n","\n","\n","--- Processing Claim ID: 204575 ---\n","DEBUG 1.0 (query_generator):\n","\tClaim: Commodore is ranked above a rear admiral.\n","\tEntities: ['Commodore']\n","\tLLM Output: ['commodore', 'rear_admiral', 'naval_rank', 'military_ranks']\n","-_--_--_--_--_-\n","DEBUG 1.1 (query_generator):\n","\tEntities: ['Commodore']\n","\tGenerated Potential Titles: ['commodore', 'naval_rank', 'rear_admiral', 'military_ranks']\n","\tSelected Titles for Retrieval: ['commodore', 'naval_rank', 'rear_admiral', 'military_ranks']\n","-_--_--_--_--_--_--_--_--_--_-\n","###### M1: Retrieving Documents ######\n","DEBUG 1.2: Successfully retrieved intro from 'Commodore (rank)' (searched for 'commodore')\n","DEBUG 1.2: Successfully retrieved intro from 'Commodore (rank)' (searched for 'naval_rank')\n","DEBUG 1.2: Successfully retrieved intro from 'Commodore admiral' (searched for 'rear_admiral')\n","DEBUG 1.2: Successfully retrieved intro from 'Military ranks of Singapore' (searched for 'military_ranks')\n","DEBUG 1.2: Retrieved content for 4 pages out of 4 potential titles.\n","-_--_--_--_--_--_--_--_--_--_-\n","-------------------------------------------------------------------\n","\n","###### M2: Starting Evidence Extraction ######\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Commodore is ranked above a rear admiral.\n","\tRephrased Claim: A commodore holds a higher rank than a rear admiral.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Commodore is ranked above a rear admiral.\n","\tRephrased Claim: A commodore holds a higher rank than a rear admiral.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Commodore is ranked above a rear admiral.\n","\tRephrased Claim: A commodore holds a higher rank than a rear admiral.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Commodore is ranked above a rear admiral.\n","\tRephrased Claim: A commodore holds a higher rank than a rear admiral.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Commodore is ranked above a rear admiral.\n","\tRephrased Claim: A commodore holds a higher rank than a rear admiral.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Commodore is ranked above a rear admiral.\n","\tRephrased Claim: A commodore holds a rank higher than that of a rear admiral.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Commodore is ranked above a rear admiral.\n","\tRephrased Claim: A commodore holds a higher rank than a rear admiral.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Commodore is ranked above a rear admiral.\n","\tRephrased Claim: A commodore holds a higher rank than a rear admiral.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Commodore is ranked above a rear admiral.\n","\tRephrased Claim: A commodore holds a higher rank than a rear admiral.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Commodore is ranked above a rear admiral.\n","\tRephrased Claim: A commodore holds a higher rank than a rear admiral.\n","-_--_--_--_--_-\n","DEBUG 2.1 (module_2_controls):\n","\tClaim: Commodore is ranked above a rear admiral.\n","\tEntities: ['Commodore']\n","\tKeywords: ['rear', 'ranked', 'commodore', 'admiral']\n","\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n","\tMax Evidence Target: 8, Max Iterations: 10\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 15\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 1: Sending 15 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","5. \"Commodore\" is typically regarded as a one-star rank with a NATO code of OF-6, known in the U.S. as \"rear admiral (lower half)\", but whether it is regarded as a flag rank varies among countries.\n","7. It is superior to a navy captain, but below a rear admiral.\n","8. It is superior to a navy captain, but below a rear admiral.\n","-_--_--_--_--_-\n","M2 Iter 1: LLM selected 3 sentences.\n","   Warning: Could not re-associate LLM output with retrieved data: 'It is superior to a navy captain, but below a rear admiral....'\n","M2 Iter 1: LLM selected few items (2). Lowering sBERT threshold to 0.350\n","DEBUG 2. End Iter 1: Total evidence found: 2\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 17\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 2: Sending 13 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","1. Commodore is a senior naval rank used in many navies which is equivalent to brigadier or brigadier general and air commodore.\n","2. As an official rank, a commodore typically commands a flotilla or squadron of ships as part of a larger task force or naval fleet commanded by an admiral.\n","3. Traditionally, \"commodore\" is the title for any officer assigned to command more than one ship, even temporarily, much as \"captain\" is the traditional title for the commanding officer of a single ship even if the officer's official title in the service is a lower rank.\n","4. A commodore's ship is typically designated by the flying of a broad pennant, as compared to an admiral's flag.\n","-_--_--_--_--_-\n","M2 Iter 2: LLM selected 4 sentences.\n","DEBUG 2. End Iter 2: Total evidence found: 6\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.350\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 17\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 3: Sending 5 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 3: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","M2 Iter 3: Stopping because LLM found no evidence after two tries.\n","M2: Finished. Selected 6 evidence items.\n","-------------------------------------------------------------------\n","\n","###### M3: Starting Classification ######\n","DEBUG 3.1 (module_3_classification):\n","\tClaim: Commodore is ranked above a rear admiral.\n","\tEvidence Texts Sent (6):\n","\tPrompt (partial): Based ONLY on the following evidence sentences, classify the claim as SUPPORTS, REFUTES, or NOT ENOUGH INFO.\n","\n","Claim: 'Commodore is ranked above a rear admiral.'\n","\n","Evidence:\n","- Traditionally, \"commodore\"...\n","-_--_--_--_--_--_--_--_--_--_-\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing Claims:  33%|███▎      | 10/30 [03:22<06:10, 18.52s/it]"]},{"output_type":"stream","name":"stdout","text":["DEBUG 3.2/3.3 (module_3_classification):\n","\tLLM Classification Result: REFUTES\n","\tExit Status: OK\n","-_--_--_--_--_--_--_--_--_--_-\n","Adding gold label/evidence to prediction.\n","##########################################################################\n","\n","##########################################################################\n","\n","Time to process claim: 18.64 seconds.\n","------------------------------------------------\n","\n","\n","--- Processing Claim ID: 107539 ---\n","DEBUG 1.0 (query_generator):\n","\tClaim: Moscovium is a halogen.\n","\tEntities: ['Moscovium']\n","\tLLM Output: ['moscovium', 'halogen', 'periodic_table', 'chemical_element', 'group_17']\n","-_--_--_--_--_-\n","DEBUG 1.1 (query_generator):\n","\tEntities: ['Moscovium']\n","\tGenerated Potential Titles: ['halogen', 'group_17', 'moscovium', 'periodic_table', 'chemical_element']\n","\tSelected Titles for Retrieval: ['halogen', 'group_17', 'moscovium', 'periodic_table', 'chemical_element']\n","-_--_--_--_--_--_--_--_--_--_-\n","###### M1: Retrieving Documents ######\n","DEBUG 1.2: DisambiguationError for title: 'halogen'. Options: ['Halogen (album)', 'Halogen (band)', 'Halogen lamp', 'Halogen oven', 'Halogen Foundation', 'Halogen Software', 'Halogen TV']...\n","Warning: Selected title 'None of the provided options are relevant to the claim about Moscovium being a halogen. However, if I must select the most relevant title from the list, I would choose:\n","\n","Halogen (album)' not in disambiguation options.\n","DEBUG 1.2: Successfully retrieved intro from 'Halogen' (searched for 'group_17')\n","DEBUG 1.2: Successfully retrieved intro from 'Moscovium' (searched for 'moscovium')\n","DEBUG 1.2: Successfully retrieved intro from 'Hydrogen' (searched for 'periodic_table')\n","DEBUG 1.2: Successfully retrieved intro from 'Mercury (element)' (searched for 'chemical_element')\n","DEBUG 1.2: Retrieved content for 4 pages out of 5 potential titles.\n","-_--_--_--_--_--_--_--_--_--_-\n","-------------------------------------------------------------------\n","\n","###### M2: Starting Evidence Extraction ######\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Moscovium is a halogen.\n","\tRephrased Claim: Moscovium is classified as a halogen.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Moscovium is a halogen.\n","\tRephrased Claim: Moscovium belongs to the group of halogens.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Moscovium is a halogen.\n","\tRephrased Claim: Moscovium is classified as a halogen.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Moscovium is a halogen.\n","\tRephrased Claim: Moscovium is classified as a halogen.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Moscovium is a halogen.\n","\tRephrased Claim: Moscovium is classified as a halogen.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Moscovium is a halogen.\n","\tRephrased Claim: Moscovium belongs to the group of halogens.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Moscovium is a halogen.\n","\tRephrased Claim: Moscovium is classified as a halogen.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Moscovium is a halogen.\n","\tRephrased Claim: Moscovium is classified as a halogen.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Moscovium is a halogen.\n","\tRephrased Claim: Moscovium is classified as a halogen.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Moscovium is a halogen.\n","\tRephrased Claim: Moscovium belongs to the group of halogens.\n","-_--_--_--_--_-\n","DEBUG 2.1 (module_2_controls):\n","\tClaim: Moscovium is a halogen.\n","\tEntities: ['Moscovium']\n","\tKeywords: ['moscovium', 'halogen']\n","\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n","\tMax Evidence Target: 8, Max Iterations: 10\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 12\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 1: Sending 12 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 1: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","   Lowering threshold to 0.350 for next try.\n","DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 14\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 2: Sending 14 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 2: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","   Lowering threshold to 0.300 for next try.\n","DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 17\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 3: Sending 17 new candidates to LLM for selection.\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing Claims:  37%|███▋      | 11/30 [03:41<05:55, 18.72s/it]"]},{"output_type":"stream","name":"stdout","text":["DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 3: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","M2 Iter 3: Stopping because LLM found no evidence after two tries.\n","M2: Finished iterations. No evidence selected.\n","-------------------------------------------------------------------\n","\n","###### M3: Starting Classification ######\n","M3: No evidence text provided. Classifying as NOT ENOUGH INFO.\n","Adding gold label/evidence to prediction.\n","##########################################################################\n","\n","##########################################################################\n","\n","Time to process claim: 19.15 seconds.\n","------------------------------------------------\n","\n","\n","--- Processing Claim ID: 164883 ---\n","DEBUG 1.0 (query_generator):\n","\tClaim: Hezbollah received a type of training from Iran.\n","\tEntities: ['Hezbollah', 'Iran']\n","\tLLM Output: ['hezbollah', 'iran', 'hezbollah–iran_relations', 'military_training', 'iranian_special_forces', 'foreign_military_training']\n","-_--_--_--_--_-\n","DEBUG 1.1 (query_generator):\n","\tEntities: ['Hezbollah', 'Iran']\n","\tGenerated Potential Titles: ['iran', 'hezbollah', 'military_training', 'iranian_special_forces', 'hezbollah–iran_relations', 'foreign_military_training']\n","\tSelected Titles for Retrieval: ['iran', 'hezbollah', 'military_training', 'iranian_special_forces', 'hezbollah–iran_relations', 'foreign_military_training']\n","-_--_--_--_--_--_--_--_--_--_-\n","###### M1: Retrieving Documents ######\n","DEBUG 1.2: Successfully retrieved intro from 'Nuclear program of Iran' (searched for 'iran')\n","DEBUG 1.2: Successfully retrieved intro from 'Hezbollah–Iran relations' (searched for 'hezbollah')\n","DEBUG 1.2: Successfully retrieved intro from 'International Military Education and Training' (searched for 'military_training')\n","DEBUG 1.2: Successfully retrieved intro from 'Islamic Republic of Iran Army' (searched for 'iranian_special_forces')\n","DEBUG 1.2: Successfully retrieved intro from 'Hezbollah–Iran relations' (searched for 'hezbollah–iran_relations')\n","DEBUG 1.2: Successfully retrieved intro from 'International Military Education and Training' (searched for 'foreign_military_training')\n","DEBUG 1.2: Retrieved content for 6 pages out of 6 potential titles.\n","-_--_--_--_--_--_--_--_--_--_-\n","-------------------------------------------------------------------\n","\n","###### M2: Starting Evidence Extraction ######\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Hezbollah received a type of training from Iran.\n","\tRephrased Claim: Hezbollah was trained by Iran in a specific manner.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Hezbollah received a type of training from Iran.\n","\tRephrased Claim: Hezbollah was trained in a certain way by Iran.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Hezbollah received a type of training from Iran.\n","\tRephrased Claim: Hezbollah was provided with a certain kind of training by Iran.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Hezbollah received a type of training from Iran.\n","\tRephrased Claim: Hezbollah underwent a form of training provided by Iran.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Hezbollah received a type of training from Iran.\n","\tRephrased Claim: Hezbollah was trained by Iran in a specific manner.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Hezbollah received a type of training from Iran.\n","\tRephrased Claim: Hezbollah obtained a form of training from Iran.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Hezbollah received a type of training from Iran.\n","\tRephrased Claim: Hezbollah was trained by Iran in a specific manner.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Hezbollah received a type of training from Iran.\n","\tRephrased Claim: Hezbollah underwent a specific form of training provided by Iran.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Hezbollah received a type of training from Iran.\n","\tRephrased Claim: Hezbollah was trained in a specific manner by Iran.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Hezbollah received a type of training from Iran.\n","\tRephrased Claim: Hezbollah was trained by Iran in a specific manner.\n","-_--_--_--_--_-\n","DEBUG 2.1 (module_2_controls):\n","\tClaim: Hezbollah received a type of training from Iran.\n","\tEntities: ['Hezbollah', 'Iran']\n","\tKeywords: ['type', 'training', 'received', 'iran', 'hezbollah']\n","\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n","\tMax Evidence Target: 8, Max Iterations: 10\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 22\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 1: Sending 22 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","1. Additionally, Iran provides weapons, training, and other forms of assistance to Hezbollah.\n","3. Iranian support, including financial aid, deployment of Revolutionary Guards, and training, has played an important role in Hezbollah's formation and development.\n","15. The organization's founders adopted the model outlined by Ayatollah Khomeini after the 1979 Iranian Revolution, and its forces were trained by a contingent of Revolutionary Guards from Iran.\n","-_--_--_--_--_-\n","M2 Iter 1: LLM selected 3 sentences.\n","M2 Iter 1: LLM selected few items (3). Lowering sBERT threshold to 0.350\n","DEBUG 2. End Iter 1: Total evidence found: 3\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 30\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 2: Sending 24 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","3. Hezbollah officially aligned itself with the Iranian regime in 1985, and the close relationship between Hezbollah and Iran has persisted ever since.  \n","5. Hezbollah itself, founded in 1982, originated as an Iranian-backed Shi'ite militant group in Lebanon.  \n","9. The Islamic Republic of Iran is a key patron of the Lebanese Shia Islamist militia and political party Hezbollah.  \n","11. Hezbollah has received substantial financial support from Iran, estimated to range from $700 million to $1 billion annually.\n","-_--_--_--_--_-\n","M2 Iter 2: LLM selected 4 sentences.\n","M2 Iter 2: LLM selected few items (4). Lowering sBERT threshold to 0.300\n","DEBUG 2. End Iter 2: Total evidence found: 7\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 32\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 3: Sending 18 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","1. Hezbollah has functioned as Iran's proxy since its inception, and is considered to be part of the \"Axis of Resistance\".\n","2. Iran considers its relationship with Hezbollah as crucial, as it provides Iran with a means to expand its influence in the Levant, exert pressure on Israel and US interests, discourage any attempts at regime change, and uphold its ideological commitments.\n","-_--_--_--_--_-\n","M2 Iter 3: LLM selected 2 sentences.\n","M2 Iter 3: LLM selected few items (1). Lowering sBERT threshold to 0.250\n","DEBUG 2. End Iter 3: Total evidence found: 8\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 3: Reached target evidence count (8).\n","M2: Finished. Selected 8 evidence items.\n","-------------------------------------------------------------------\n","\n","###### M3: Starting Classification ######\n","DEBUG 3.1 (module_3_classification):\n","\tClaim: Hezbollah received a type of training from Iran.\n","\tEvidence Texts Sent (8):\n","\tPrompt (partial): Based ONLY on the following evidence sentences, classify the claim as SUPPORTS, REFUTES, or NOT ENOUGH INFO.\n","\n","Claim: 'Hezbollah received a type of training from Iran.'\n","\n","Evidence:\n","- Iranian support, in...\n","-_--_--_--_--_--_--_--_--_--_-\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing Claims:  40%|████      | 12/30 [04:03<05:56, 19.78s/it]"]},{"output_type":"stream","name":"stdout","text":["DEBUG 3.2/3.3 (module_3_classification):\n","\tLLM Classification Result: SUPPORTS\n","\tExit Status: OK\n","-_--_--_--_--_--_--_--_--_--_-\n","Adding gold label/evidence to prediction.\n","##########################################################################\n","\n","##########################################################################\n","\n","Time to process claim: 22.21 seconds.\n","------------------------------------------------\n","\n","\n","--- Processing Claim ID: 54298 ---\n","DEBUG 1.0 (query_generator):\n","\tClaim: In states still employing the electric chair to execute people, the prisoner is allowed the choice of lethal injection as an alternative method.\n","\tEntities: []\n","\tLLM Output: ['capital_punishment', 'death_penalty_in_the_United_States', 'lethal_injection', 'electric_chair', 'methods_of_execution', 'prisoner_rights']\n","-_--_--_--_--_-\n","DEBUG 1.1 (query_generator):\n","\tEntities: []\n","\tGenerated Potential Titles: ['electric_chair', 'prisoner_rights', 'lethal_injection', 'capital_punishment', 'methods_of_execution', 'death_penalty_in_the_United_States']\n","\tSelected Titles for Retrieval: ['electric_chair', 'prisoner_rights', 'lethal_injection', 'capital_punishment', 'methods_of_execution', 'death_penalty_in_the_United_States']\n","-_--_--_--_--_--_--_--_--_--_-\n","###### M1: Retrieving Documents ######\n","DEBUG 1.2: DisambiguationError for title: 'electric_chair'. Options: ['electric-powered wheelchair', 'Electric chair', 'stairlift', 'Big Electric Chair', 'Electric Chair (album)', 'Steelheart', 'Prince', 'Wayne County & the Electric Chairs']...\n","DEBUG 1.2: Successfully retrieved intro from 'Electric chair' (disambiguated to 'Electric chair')\n","DEBUG 1.2: Successfully retrieved intro from 'Prisoner rights in the United States' (searched for 'prisoner_rights')\n","DEBUG 1.2: Successfully retrieved intro from 'Lethal injection' (searched for 'lethal_injection')\n","DEBUG 1.2: Successfully retrieved intro from 'Capital punishment in the United States' (searched for 'capital_punishment')\n","DEBUG 1.2: Successfully retrieved intro from 'Lethal injection' (searched for 'methods_of_execution')\n","DEBUG 1.2: Successfully retrieved intro from 'Capital punishment debate in the United States' (searched for 'death_penalty_in_the_United_States')\n","DEBUG 1.2: Retrieved content for 6 pages out of 6 potential titles.\n","-_--_--_--_--_--_--_--_--_--_-\n","-------------------------------------------------------------------\n","\n","###### M2: Starting Evidence Extraction ######\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: In states still employing the electric chair to execute people, the prisoner is allowed the choice of lethal injection as an alternative method.\n","\tRephrased Claim: In states that continue to use the electric chair for executions, inmates have the option to choose lethal injection as an alternative method.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: In states still employing the electric chair to execute people, the prisoner is allowed the choice of lethal injection as an alternative method.\n","\tRephrased Claim: In states that continue to use the electric chair for executions, inmates have the option to choose lethal injection as an alternative method.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: In states still employing the electric chair to execute people, the prisoner is allowed the choice of lethal injection as an alternative method.\n","\tRephrased Claim: In states that continue to use the electric chair for executions, inmates have the option to select lethal injection as a different method.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: In states still employing the electric chair to execute people, the prisoner is allowed the choice of lethal injection as an alternative method.\n","\tRephrased Claim: In states that continue to use the electric chair for executions, inmates have the option to select lethal injection as an alternative method.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: In states still employing the electric chair to execute people, the prisoner is allowed the choice of lethal injection as an alternative method.\n","\tRephrased Claim: In states that still use the electric chair for executions, inmates have the option to choose lethal injection as an alternative method.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: In states still employing the electric chair to execute people, the prisoner is allowed the choice of lethal injection as an alternative method.\n","\tRephrased Claim: In states that continue to use the electric chair for executions, inmates have the option to select lethal injection as an alternative method.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: In states still employing the electric chair to execute people, the prisoner is allowed the choice of lethal injection as an alternative method.\n","\tRephrased Claim: In states that continue to use the electric chair for executions, inmates have the option to choose lethal injection as an alternative method.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: In states still employing the electric chair to execute people, the prisoner is allowed the choice of lethal injection as an alternative method.\n","\tRephrased Claim: In states that continue to use the electric chair for executions, inmates have the option to choose lethal injection as an alternative method.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: In states still employing the electric chair to execute people, the prisoner is allowed the choice of lethal injection as an alternative method.\n","\tRephrased Claim: In states that continue to use the electric chair for executions, prisoners have the option to select lethal injection as an alternative method.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: In states still employing the electric chair to execute people, the prisoner is allowed the choice of lethal injection as an alternative method.\n","\tRephrased Claim: In states that continue to use the electric chair for executions, inmates are permitted to opt for lethal injection as a substitute method.\n","-_--_--_--_--_-\n","DEBUG 2.1 (module_2_controls):\n","\tClaim: In states still employing the electric chair to execute people, the prisoner is allowed the choice of lethal injection as an alternative method.\n","\tEntities: []\n","\tKeywords: ['states', 'prisoner', 'people', 'method', 'lethal']\n","\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n","\tMax Evidence Target: 8, Max Iterations: 10\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 52\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 1: Sending 52 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","As of 2025, electrocution remains an option in states like Alabama, South Carolina and Florida, where inmates may choose lethal injection instead.  \n","The electric chair remains an accepted alternative in Louisiana, Mississippi, and Oklahoma if other execution methods are ruled unconstitutional at the time of execution.  \n","Although it is a legal penalty in 27 states, 21 of them have authority to execute death sentences, with the other 6, subject to moratoriums.  \n","Arkansas currently has no death row inmates sentenced before their select date.\n","-_--_--_--_--_-\n","M2 Iter 1: LLM selected 4 sentences.\n","M2 Iter 1: LLM selected few items (4). Lowering sBERT threshold to 0.350\n","DEBUG 2. End Iter 1: Total evidence found: 4\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 55\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 2: Sending 51 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","4. These three states also authorize electrocution as an alternative if lethal injection is deemed unavailable.  \n","5. While some states retain electrocution as a legal execution method, it is often a secondary option based on the condemned's preference.  \n","12. Arkansas, Kentucky, and Tennessee offer the electric chair to those sentenced before a certain date.\n","-_--_--_--_--_-\n","M2 Iter 2: LLM selected 3 sentences.\n","M2 Iter 2: LLM selected few items (3). Lowering sBERT threshold to 0.300\n","DEBUG 2. End Iter 2: Total evidence found: 7\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 69\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 3: Sending 62 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","4. Exceptions include South Carolina, where it is the primary method, and Tennessee, where it can be used without prisoner input if lethal injection drugs are unavailable.  \n","7. Inmates not selecting this method or convicted after the specified date face lethal injection.  \n","1. Despite its historical significance in American capital punishment, electric chair use has declined with the adoption of lethal injection which was perceived as more humane.\n","-_--_--_--_--_-\n","M2 Iter 3: LLM selected 3 sentences.\n","M2 Iter 3: LLM selected few items (1). Lowering sBERT threshold to 0.250\n","DEBUG 2. End Iter 3: Total evidence found: 8\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 3: Reached target evidence count (8).\n","M2: Finished. Selected 8 evidence items.\n","-------------------------------------------------------------------\n","\n","###### M3: Starting Classification ######\n","DEBUG 3.1 (module_3_classification):\n","\tClaim: In states still employing the electric chair to execute people, the prisoner is allowed the choice of lethal injection as an alternative method.\n","\tEvidence Texts Sent (8):\n","\tPrompt (partial): Based ONLY on the following evidence sentences, classify the claim as SUPPORTS, REFUTES, or NOT ENOUGH INFO.\n","\n","Claim: 'In states still employing the electric chair to execute people, the prisoner is al...\n","-_--_--_--_--_--_--_--_--_--_-\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing Claims:  43%|████▎     | 13/30 [04:32<06:23, 22.54s/it]"]},{"output_type":"stream","name":"stdout","text":["DEBUG 3.2/3.3 (module_3_classification):\n","\tLLM Classification Result: SUPPORTS\n","\tExit Status: OK\n","-_--_--_--_--_--_--_--_--_--_-\n","Adding gold label/evidence to prediction.\n","##########################################################################\n","\n","##########################################################################\n","\n","Time to process claim: 28.88 seconds.\n","------------------------------------------------\n","\n","\n","--- Processing Claim ID: 222749 ---\n","DEBUG 1.0 (query_generator):\n","\tClaim: Practical Magic is an American romantic drama film.\n","\tEntities: ['Practical', 'American', 'Magic']\n","\tLLM Output: ['practical_magic', 'american_romantic_drama_films', 'practical_magic_(film)']\n","-_--_--_--_--_-\n","DEBUG 1.1 (query_generator):\n","\tEntities: ['Practical', 'American', 'Magic']\n","\tGenerated Potential Titles: ['practical_magic', 'practical_magic_(film)', 'american_romantic_drama_films']\n","\tSelected Titles for Retrieval: ['practical_magic', 'practical_magic_(film)', 'american_romantic_drama_films']\n","-_--_--_--_--_--_--_--_--_--_-\n","###### M1: Retrieving Documents ######\n","DEBUG 1.2: Successfully retrieved intro from 'Practical Magic (novel)' (searched for 'practical_magic')\n","DEBUG 1.2: Successfully retrieved intro from 'Practical Magic (novel)' (searched for 'practical_magic_(film)')\n","DEBUG 1.2: Successfully retrieved intro from 'The Drama (film)' (searched for 'american_romantic_drama_films')\n","DEBUG 1.2: Retrieved content for 3 pages out of 3 potential titles.\n","-_--_--_--_--_--_--_--_--_--_-\n","-------------------------------------------------------------------\n","\n","###### M2: Starting Evidence Extraction ######\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Practical Magic is an American romantic drama film.\n","\tRephrased Claim: Practical Magic is a romantic drama film from the United States.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Practical Magic is an American romantic drama film.\n","\tRephrased Claim: Practical Magic is a romantic drama film from the United States.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Practical Magic is an American romantic drama film.\n","\tRephrased Claim: Practical Magic is a romantic drama film from the United States.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Practical Magic is an American romantic drama film.\n","\tRephrased Claim: Practical Magic is a romantic drama film from the United States.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Practical Magic is an American romantic drama film.\n","\tRephrased Claim: Practical Magic is a romantic drama film from the United States.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Practical Magic is an American romantic drama film.\n","\tRephrased Claim: Practical Magic is a romantic drama film from the United States.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Practical Magic is an American romantic drama film.\n","\tRephrased Claim: Practical Magic is a romantic drama film from the United States.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Practical Magic is an American romantic drama film.\n","\tRephrased Claim: Practical Magic is a romantic drama film from the United States.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Practical Magic is an American romantic drama film.\n","\tRephrased Claim: Practical Magic is a romantic drama film from the United States.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Practical Magic is an American romantic drama film.\n","\tRephrased Claim: Practical Magic is a romantic drama film from the United States.\n","-_--_--_--_--_-\n","DEBUG 2.1 (module_2_controls):\n","\tClaim: Practical Magic is an American romantic drama film.\n","\tEntities: ['Practical', 'American', 'Magic']\n","\tKeywords: ['romantic', 'practical', 'magic', 'film', 'drama']\n","\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n","\tMax Evidence Target: 8, Max Iterations: 10\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 5\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 1: Sending 5 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 1: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","   Lowering threshold to 0.350 for next try.\n","DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 5\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 2: Sending 5 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 2: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","   Lowering threshold to 0.300 for next try.\n","DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 5\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 3: Sending 5 new candidates to LLM for selection.\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing Claims:  47%|████▋     | 14/30 [04:44<05:12, 19.54s/it]"]},{"output_type":"stream","name":"stdout","text":["DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 3: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","M2 Iter 3: Stopping because LLM found no evidence after two tries.\n","M2: Finished iterations. No evidence selected.\n","-------------------------------------------------------------------\n","\n","###### M3: Starting Classification ######\n","M3: No evidence text provided. Classifying as NOT ENOUGH INFO.\n","Adding gold label/evidence to prediction.\n","##########################################################################\n","\n","##########################################################################\n","\n","Time to process claim: 12.62 seconds.\n","------------------------------------------------\n","\n","\n","--- Processing Claim ID: 219675 ---\n","DEBUG 1.0 (query_generator):\n","\tClaim: Corsica belongs to Italy.\n","\tEntities: ['Corsica', 'Italy']\n","\tLLM Output: ['corsica', 'italy', 'history_of_corsica', 'political_status_of_corsica', 'italy_in_the_21st_century', 'italy_history', 'geography_of_corsica']\n","-_--_--_--_--_-\n","DEBUG 1.1 (query_generator):\n","\tEntities: ['Corsica', 'Italy']\n","\tGenerated Potential Titles: ['italy', 'corsica', 'italy_history', 'history_of_corsica', 'geography_of_corsica', 'italy_in_the_21st_century', 'political_status_of_corsica']\n","\tSelected Titles for Retrieval: ['italy', 'corsica', 'italy_history', 'history_of_corsica', 'geography_of_corsica', 'italy_in_the_21st_century', 'political_status_of_corsica']\n","-_--_--_--_--_--_--_--_--_--_-\n","###### M1: Retrieving Documents ######\n","DEBUG 1.2: Successfully retrieved intro from 'Kingdom of Italy' (searched for 'italy')\n","DEBUG 1.2: DisambiguationError for title: 'corsica'. Options: ['Corsica, Pennsylvania', 'Corsica, South Dakota', 'Chevrolet Corsica', 'Artemisia Gentileschi', 'Sluggy Freelance', 'Corsica (album)', 'Corsica Coachworks', 'Corsican (disambiguation)', 'Corse (disambiguation)', 'Corsa (disambiguation)']...\n","Warning: Error retrieving disambiguated page 'Corsican (disambiguation)': \"Corsican\" may refer to: \n","Corsica\n","Corsicans\n","Corsican language\n","Corsican Republic\n","Hearts of Oak militia\n","List of all pages beginning with \"Corsican\"\n","List of Corsicans\n","Corsicana, Texas\n","Corsica (disambiguation)\n","Corse (disambiguation)\n","DEBUG 1.2: Successfully retrieved intro from 'Roman Italy' (searched for 'italy_history')\n","DEBUG 1.2: Successfully retrieved intro from 'Corsica' (searched for 'history_of_corsica')\n","DEBUG 1.2: Successfully retrieved intro from 'Corsica' (searched for 'geography_of_corsica')\n","DEBUG 1.2: Successfully retrieved intro from 'Timeline of Italian history' (searched for 'italy_in_the_21st_century')\n","DEBUG 1.2: Successfully retrieved intro from 'Corsica' (searched for 'political_status_of_corsica')\n","DEBUG 1.2: Retrieved content for 6 pages out of 7 potential titles.\n","-_--_--_--_--_--_--_--_--_--_-\n","-------------------------------------------------------------------\n","\n","###### M2: Starting Evidence Extraction ######\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Corsica belongs to Italy.\n","\tRephrased Claim: Corsica is a part of Italy.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Corsica belongs to Italy.\n","\tRephrased Claim: Corsica is a part of Italy.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Corsica belongs to Italy.\n","\tRephrased Claim: Corsica is a part of Italy.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Corsica belongs to Italy.\n","\tRephrased Claim: Corsica is part of Italy.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Corsica belongs to Italy.\n","\tRephrased Claim: Corsica is part of Italy.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Corsica belongs to Italy.\n","\tRephrased Claim: Corsica is a part of Italy.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Corsica belongs to Italy.\n","\tRephrased Claim: Corsica is a part of Italy.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Corsica belongs to Italy.\n","\tRephrased Claim: Corsica is a part of Italy.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Corsica belongs to Italy.\n","\tRephrased Claim: Corsica is a territory of Italy.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Corsica belongs to Italy.\n","\tRephrased Claim: Corsica is a part of Italy.\n","-_--_--_--_--_-\n","DEBUG 2.1 (module_2_controls):\n","\tClaim: Corsica belongs to Italy.\n","\tEntities: ['Corsica', 'Italy']\n","\tKeywords: ['italy', 'corsica', 'belongs']\n","\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n","\tMax Evidence Target: 8, Max Iterations: 10\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 46\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 1: Sending 46 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","1. Because of Corsica's historical ties to Tuscany, the island has retained many Italian cultural elements, and many Corsican surnames are rooted in the Italian peninsula.\n","2. Corsica was ruled by the Republic of Genoa from 1284 to 1755, when it seceded to become a self-proclaimed, Italian-speaking Republic.\n","3. Meanwhile, the islands of Corsica, Sardinia, Sicily and Malta were added to Italy by Diocletian.\n","4. It is the fourth-largest island in the Mediterranean and lies southeast of the French mainland, west of the Italian Peninsula and immediately north of the Italian island of Sardinia, the nearest land mass.\n","-_--_--_--_--_-\n","M2 Iter 1: LLM selected 4 sentences.\n","M2 Iter 1: LLM selected few items (4). Lowering sBERT threshold to 0.350\n","DEBUG 2. End Iter 1: Total evidence found: 4\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 53\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 2: Sending 43 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 2: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","   Lowering threshold to 0.300 for next try.\n","DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 69\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 3: Sending 59 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 3: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","M2 Iter 3: Stopping because LLM found no evidence after two tries.\n","M2: Finished. Selected 4 evidence items.\n","-------------------------------------------------------------------\n","\n","###### M3: Starting Classification ######\n","DEBUG 3.1 (module_3_classification):\n","\tClaim: Corsica belongs to Italy.\n","\tEvidence Texts Sent (4):\n","\tPrompt (partial): Based ONLY on the following evidence sentences, classify the claim as SUPPORTS, REFUTES, or NOT ENOUGH INFO.\n","\n","Claim: 'Corsica belongs to Italy.'\n","\n","Evidence:\n","- Because of Corsica's historical ties to Tu...\n","-_--_--_--_--_--_--_--_--_--_-\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing Claims:  50%|█████     | 15/30 [05:17<05:51, 23.42s/it]"]},{"output_type":"stream","name":"stdout","text":["DEBUG 3.2/3.3 (module_3_classification):\n","\tLLM Classification Result: REFUTES\n","\tExit Status: OK\n","-_--_--_--_--_--_--_--_--_--_-\n","Adding gold label/evidence to prediction.\n","##########################################################################\n","\n","##########################################################################\n","\n","Time to process claim: 32.40 seconds.\n","------------------------------------------------\n","\n","\n","--- Processing Claim ID: 134850 ---\n","DEBUG 1.0 (query_generator):\n","\tClaim: Ice-T refused to ever make hip-hop music.\n","\tEntities: []\n","\tLLM Output: ['ice-t', 'hip_hop_music', 'ice-t_discography', 'ice-t_career', 'hip_hop_artists']\n","-_--_--_--_--_-\n","DEBUG 1.1 (query_generator):\n","\tEntities: []\n","\tGenerated Potential Titles: ['ice-t', 'ice-t_career', 'hip_hop_music', 'hip_hop_artists', 'ice-t_discography']\n","\tSelected Titles for Retrieval: ['ice-t', 'ice-t_career', 'hip_hop_music', 'hip_hop_artists', 'ice-t_discography']\n","-_--_--_--_--_--_--_--_--_--_-\n","###### M1: Retrieving Documents ######\n","DEBUG 1.2: Successfully retrieved intro from 'Ice-T's Rap School' (searched for 'ice-t')\n","DEBUG 1.2: Successfully retrieved intro from 'Ice-T' (searched for 'ice-t_career')\n","DEBUG 1.2: Successfully retrieved intro from 'Alternative hip-hop' (searched for 'hip_hop_music')\n","DEBUG 1.2: Successfully retrieved intro from 'Hip-hop' (searched for 'hip_hop_artists')\n","DEBUG 1.2: Successfully retrieved intro from 'Ice-T' (searched for 'ice-t_discography')\n","DEBUG 1.2: Retrieved content for 5 pages out of 5 potential titles.\n","-_--_--_--_--_--_--_--_--_--_-\n","-------------------------------------------------------------------\n","\n","###### M2: Starting Evidence Extraction ######\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Ice-T refused to ever make hip-hop music.\n","\tRephrased Claim: Ice-T has stated that he will never create hip-hop music.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Ice-T refused to ever make hip-hop music.\n","\tRephrased Claim: Ice-T has declined to create hip-hop music.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Ice-T refused to ever make hip-hop music.\n","\tRephrased Claim: Ice-T has stated that he will never create hip-hop music.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Ice-T refused to ever make hip-hop music.\n","\tRephrased Claim: Ice-T has stated that he will never create hip-hop music.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Ice-T refused to ever make hip-hop music.\n","\tRephrased Claim: Ice-T has stated that he will never create hip-hop music.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Ice-T refused to ever make hip-hop music.\n","\tRephrased Claim: Ice-T has stated that he would never create hip-hop music.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Ice-T refused to ever make hip-hop music.\n","\tRephrased Claim: Ice-T has stated that he would never create hip-hop music.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Ice-T refused to ever make hip-hop music.\n","\tRephrased Claim: Ice-T has declined to create hip-hop music.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Ice-T refused to ever make hip-hop music.\n","\tRephrased Claim: Ice-T has stated that he will never create hip-hop music.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Ice-T refused to ever make hip-hop music.\n","\tRephrased Claim: Ice-T has stated that he will never create hip-hop music.\n","-_--_--_--_--_-\n","DEBUG 2.1 (module_2_controls):\n","\tClaim: Ice-T refused to ever make hip-hop music.\n","\tEntities: []\n","\tKeywords: ['refused', 'music', 'make', 'ice', 'hop']\n","\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n","\tMax Evidence Target: 8, Max Iterations: 10\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 23\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 1: Sending 23 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 1: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","   Lowering threshold to 0.350 for next try.\n","DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 21\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 2: Sending 21 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 2: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","   Lowering threshold to 0.300 for next try.\n","DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 55\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 3: Sending 55 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","1. Ice-T released two more albums in the late 1990s and one in the 2000s before focusing on both his acting career and Body Count, who have released eight studio albums to date, the latest being 2024's Merciless.\n","2. Ice-T co-founded the heavy metal band Body Count in 1990, which he introduced on O.G.\n","3. He is active in both hip hop and heavy metal.\n","-_--_--_--_--_-\n","M2 Iter 3: LLM selected 3 sentences.\n","M2 Iter 3: LLM selected few items (3). Lowering sBERT threshold to 0.250\n","DEBUG 2. End Iter 3: Total evidence found: 3\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 4/10, Current sBERT Thresh: 0.250\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 50\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 4: Sending 44 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 4: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","M2 Iter 4: Stopping because LLM found no evidence after two tries.\n","M2: Finished. Selected 3 evidence items.\n","-------------------------------------------------------------------\n","\n","###### M3: Starting Classification ######\n","DEBUG 3.1 (module_3_classification):\n","\tClaim: Ice-T refused to ever make hip-hop music.\n","\tEvidence Texts Sent (3):\n","\tPrompt (partial): Based ONLY on the following evidence sentences, classify the claim as SUPPORTS, REFUTES, or NOT ENOUGH INFO.\n","\n","Claim: 'Ice-T refused to ever make hip-hop music.'\n","\n","Evidence:\n","- Ice-T co-founded the heavy...\n","-_--_--_--_--_--_--_--_--_--_-\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing Claims:  53%|█████▎    | 16/30 [05:49<06:04, 26.04s/it]"]},{"output_type":"stream","name":"stdout","text":["DEBUG 3.2/3.3 (module_3_classification):\n","\tLLM Classification Result: REFUTES\n","\tExit Status: OK\n","-_--_--_--_--_--_--_--_--_--_-\n","Adding gold label/evidence to prediction.\n","##########################################################################\n","\n","##########################################################################\n","\n","Time to process claim: 32.13 seconds.\n","------------------------------------------------\n","\n","\n","--- Processing Claim ID: 124578 ---\n","DEBUG 1.0 (query_generator):\n","\tClaim: The Gettysburg Address is a speech.\n","\tEntities: ['Gettysburg Address']\n","\tLLM Output: ['gettysburg_address', 'abraham_lincoln', 'american_civil_war', 'speeches_by_abraham_lincoln', 'historical_speeches']\n","-_--_--_--_--_-\n","DEBUG 1.1 (query_generator):\n","\tEntities: ['Gettysburg Address']\n","\tGenerated Potential Titles: ['abraham_lincoln', 'american_civil_war', 'gettysburg_address', 'historical_speeches', 'speeches_by_abraham_lincoln']\n","\tSelected Titles for Retrieval: ['abraham_lincoln', 'american_civil_war', 'gettysburg_address', 'historical_speeches', 'speeches_by_abraham_lincoln']\n","-_--_--_--_--_--_--_--_--_--_-\n","###### M1: Retrieving Documents ######\n","DEBUG 1.2: Successfully retrieved intro from 'State funeral of Abraham Lincoln' (searched for 'abraham_lincoln')\n","DEBUG 1.2: Successfully retrieved intro from 'Conclusion of the American Civil War' (searched for 'american_civil_war')\n","DEBUG 1.2: Successfully retrieved intro from 'A Gettysburg Address' (searched for 'gettysburg_address')\n","DEBUG 1.2: Successfully retrieved intro from 'Gettysburg Address' (searched for 'historical_speeches')\n","DEBUG 1.2: Successfully retrieved intro from 'Gettysburg Address' (searched for 'speeches_by_abraham_lincoln')\n","DEBUG 1.2: Retrieved content for 5 pages out of 5 potential titles.\n","-_--_--_--_--_--_--_--_--_--_-\n","-------------------------------------------------------------------\n","\n","###### M2: Starting Evidence Extraction ######\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: The Gettysburg Address is a speech.\n","\tRephrased Claim: The Gettysburg Address is a type of speech.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: The Gettysburg Address is a speech.\n","\tRephrased Claim: The Gettysburg Address is a formal address.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: The Gettysburg Address is a speech.\n","\tRephrased Claim: The Gettysburg Address is a formal address.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: The Gettysburg Address is a speech.\n","\tRephrased Claim: The Gettysburg Address is a form of address.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: The Gettysburg Address is a speech.\n","\tRephrased Claim: The Gettysburg Address is a formal oration.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: The Gettysburg Address is a speech.\n","\tRephrased Claim: The Gettysburg Address is a form of speech.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: The Gettysburg Address is a speech.\n","\tRephrased Claim: The Gettysburg Address is a verbal presentation.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: The Gettysburg Address is a speech.\n","\tRephrased Claim: The Gettysburg Address constitutes a speech.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: The Gettysburg Address is a speech.\n","\tRephrased Claim: The Gettysburg Address is an oration.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: The Gettysburg Address is a speech.\n","\tRephrased Claim: The Gettysburg Address is a formal oration.\n","-_--_--_--_--_-\n","DEBUG 2.1 (module_2_controls):\n","\tClaim: The Gettysburg Address is a speech.\n","\tEntities: ['Gettysburg Address']\n","\tKeywords: ['speech', 'gettysburg', 'address']\n","\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n","\tMax Evidence Target: 8, Max Iterations: 10\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 25\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 1: Sending 25 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","1. The Gettysburg Address is a speech delivered by Abraham Lincoln, the 16th U.S. president, following the Battle of Gettysburg during the American Civil War.\n","2. The Gettysburg Address is a speech delivered by Abraham Lincoln, the 16th U.S. president, following the Battle of Gettysburg during the American Civil War.\n","3. \"The Gettysburg Address did not enter the broader American canon until decades after Lincoln’s death, following World War I and the 1922 opening of the Lincoln Memorial, where the speech is etched in marble.\n","4. \"The Gettysburg Address did not enter the broader American canon until decades after Lincoln’s death, following World War I and the 1922 opening of the Lincoln Memorial, where the speech is etched in marble.\n","14. The speech has come to be viewed as one of the most famous, enduring, and historically significant speeches in American history.\n","15. The speech has come to be viewed as one of the most famous, enduring, and historically significant speeches in American history.\n","20. In his brief but historical speech, Lincoln described the Union's cause in the Civil War as necessary to validate that the sovereignty and freedoms the nation successfully secured less than nine decades earlier in the American Revolution, Revolutionary War, and nation's establishment, could prove enduring.\n","21. In his brief but historical speech, Lincoln described the Union's cause in the Civil War as necessary to validate that the sovereignty and freedoms the nation successfully secured less than nine decades earlier in the American Revolution, Revolutionary War, and nation's establishment, could prove enduring.\n","-_--_--_--_--_-\n","M2 Iter 1: LLM selected 8 sentences.\n","   Warning: Could not re-associate LLM output with retrieved data: 'The Gettysburg Address is a speech delivered by Abraham Lincoln, the 16th U.S. p...'\n","   Warning: Could not re-associate LLM output with retrieved data: '\"The Gettysburg Address did not enter the broader American canon until decades a...'\n","   Warning: Could not re-associate LLM output with retrieved data: 'The speech has come to be viewed as one of the most famous, enduring, and histor...'\n","   Warning: Could not re-associate LLM output with retrieved data: 'In his brief but historical speech, Lincoln described the Union's cause in the C...'\n","M2 Iter 1: LLM selected few items (4). Lowering sBERT threshold to 0.350\n","DEBUG 2. End Iter 1: Total evidence found: 4\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 28\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 2: Sending 20 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","3. The historical and enduring significance and fame of the Gettysburg Address is at least partly attributable to its conscious brevity, including only 271 words and comprising less than two minutes before a crowd of approximately 15,000 people, which gathered to join in commemorating the sacrifice of the Union soldiers, over 23,000 of whom were killed over the three day Battle of Gettysburg.  \n","5. Lincoln delivered the speech on the afternoon of November 19, 1863, during a formal dedication of Soldiers' National Cemetery, now known as Gettysburg National Cemetery, on the grounds where the Battle of Gettysburg was fought four and a half months earlier, between July 1 and July 3, 1863, in Gettysburg, Pennsylvania.  \n","7. In the Gettysburg Address, Lincoln prominently referenced the nation's founding, describing it as having been \"conceived in Liberty, and dedicated to the proposition that all men are created equal\", a reference to a phrase incorporated into the Declaration by Thomas Jefferson.  \n","10. Lincoln began his 271-word address in Gettysburg with the now famed phrase, \"Four score and seven years ago\", a reference to the nation's founding in the American Revolution, during which the Founding Fathers ultimately concluded that they could not reconcile their differences with King George III and instead needed to enjoin and prevail in the Revolutionary War in pursuit of full independence from British colonial rule.  \n","14. Lincoln described the Civil War as questioning and testing whether such a nation could endure, extolled the sacrifices of the thousands who died in the Battle of Gettysburg in defense of those principles, and then argued that their sacrifice should elevate the nation's commitment to ensuring the Union prevailed and the nation endured, famously saying: that these dead shall not have died in vain—that this nation, under God, shall have a new birth of freedom—and that government of the people, by the people, for the people, shall not perish from the earth.  \n","19. Over time, however, it came to be viewed widely as one of the greatest and most influential statements ever delivered on the American national purpose, enduring in significance throughout the nation's history, and also as one of the most prominent examples of the successful use of the English language and rhetoric to advance a political cause.\n","-_--_--_--_--_-\n","M2 Iter 2: LLM selected 6 sentences.\n","M2 Iter 2: LLM selected few items (4). Lowering sBERT threshold to 0.300\n","DEBUG 2. End Iter 2: Total evidence found: 8\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 2: Reached target evidence count (8).\n","M2: Finished. Selected 8 evidence items.\n","-------------------------------------------------------------------\n","\n","###### M3: Starting Classification ######\n","DEBUG 3.1 (module_3_classification):\n","\tClaim: The Gettysburg Address is a speech.\n","\tEvidence Texts Sent (8):\n","\tPrompt (partial): Based ONLY on the following evidence sentences, classify the claim as SUPPORTS, REFUTES, or NOT ENOUGH INFO.\n","\n","Claim: 'The Gettysburg Address is a speech.'\n","\n","Evidence:\n","- Lincoln delivered the speech on ...\n","-_--_--_--_--_--_--_--_--_--_-\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing Claims:  57%|█████▋    | 17/30 [06:18<05:50, 26.93s/it]"]},{"output_type":"stream","name":"stdout","text":["DEBUG 3.2/3.3 (module_3_classification):\n","\tLLM Classification Result: SUPPORTS\n","\tExit Status: OK\n","-_--_--_--_--_--_--_--_--_--_-\n","Adding gold label/evidence to prediction.\n","##########################################################################\n","\n","##########################################################################\n","\n","Time to process claim: 28.99 seconds.\n","------------------------------------------------\n","\n","\n","--- Processing Claim ID: 134126 ---\n","DEBUG 1.0 (query_generator):\n","\tClaim: Jason Bourne removed Riz Ahmed from the movie's cast.\n","\tEntities: ['Riz Ahmed', 'Bourne', 'Jason']\n","\tLLM Output: ['jason_bourne', 'riz_ahmed', 'the_bourne_identity', 'the_bourne_supremacy', 'the_bourne_ultimatum', 'the_bourne_legacy', 'jason_bourne_(film)', 'riz_ahmed_filmography']\n","-_--_--_--_--_-\n","DEBUG 1.1 (query_generator):\n","\tEntities: ['Riz Ahmed', 'Bourne', 'Jason']\n","\tGenerated Potential Titles: ['riz_ahmed', 'jason_bourne', 'the_bourne_legacy', 'jason_bourne_(film)', 'the_bourne_identity', 'the_bourne_supremacy', 'the_bourne_ultimatum', 'riz_ahmed_filmography']\n","\tSelected Titles for Retrieval: ['riz_ahmed', 'jason_bourne', 'the_bourne_legacy', 'jason_bourne_(film)', 'the_bourne_identity', 'the_bourne_supremacy', 'the_bourne_ultimatum', 'riz_ahmed_filmography']\n","-_--_--_--_--_--_--_--_--_--_-\n","###### M1: Retrieving Documents ######\n","DEBUG 1.2: Successfully retrieved intro from 'Jason Bourne (film)' (searched for 'riz_ahmed')\n","DEBUG 1.2: Successfully retrieved intro from 'Jason Bourne' (searched for 'jason_bourne')\n","DEBUG 1.2: Successfully retrieved intro from 'Jason Bourne' (searched for 'the_bourne_legacy')\n","DEBUG 1.2: Successfully retrieved intro from 'Jason Bourne' (searched for 'jason_bourne_(film)')\n","DEBUG 1.2: Successfully retrieved intro from 'Jason Bourne' (searched for 'the_bourne_identity')\n","DEBUG 1.2: Successfully retrieved intro from 'Jason Bourne' (searched for 'the_bourne_supremacy')\n","DEBUG 1.2: Successfully retrieved intro from 'Jason Bourne' (searched for 'the_bourne_ultimatum')\n","DEBUG 1.2: Successfully retrieved intro from 'Riz Ahmed' (searched for 'riz_ahmed_filmography')\n","DEBUG 1.2: Retrieved content for 8 pages out of 8 potential titles.\n","-_--_--_--_--_--_--_--_--_--_-\n","-------------------------------------------------------------------\n","\n","###### M2: Starting Evidence Extraction ######\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Jason Bourne removed Riz Ahmed from the movie's cast.\n","\tRephrased Claim: Jason Bourne eliminated Riz Ahmed from the film's cast.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Jason Bourne removed Riz Ahmed from the movie's cast.\n","\tRephrased Claim: Jason Bourne eliminated Riz Ahmed from the film's cast.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Jason Bourne removed Riz Ahmed from the movie's cast.\n","\tRephrased Claim: Jason Bourne excluded Riz Ahmed from the film's cast.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Jason Bourne removed Riz Ahmed from the movie's cast.\n","\tRephrased Claim: Jason Bourne eliminated Riz Ahmed from the film's lineup.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Jason Bourne removed Riz Ahmed from the movie's cast.\n","\tRephrased Claim: Jason Bourne eliminated Riz Ahmed from the film's cast.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Jason Bourne removed Riz Ahmed from the movie's cast.\n","\tRephrased Claim: Jason Bourne eliminated Riz Ahmed from the film's cast.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Jason Bourne removed Riz Ahmed from the movie's cast.\n","\tRephrased Claim: Jason Bourne eliminated Riz Ahmed from the film's cast.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Jason Bourne removed Riz Ahmed from the movie's cast.\n","\tRephrased Claim: Jason Bourne eliminated Riz Ahmed from the film's cast.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Jason Bourne removed Riz Ahmed from the movie's cast.\n","\tRephrased Claim: Jason Bourne eliminated Riz Ahmed from the film's lineup.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Jason Bourne removed Riz Ahmed from the movie's cast.\n","\tRephrased Claim: Jason Bourne eliminated Riz Ahmed from the film's cast.\n","-_--_--_--_--_-\n","DEBUG 2.1 (module_2_controls):\n","\tClaim: Jason Bourne removed Riz Ahmed from the movie's cast.\n","\tEntities: ['Riz Ahmed', 'Bourne', 'Jason']\n","\tKeywords: ['riz', 'removed', 'movie', 'jason', 'cast']\n","\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n","\tMax Evidence Target: 8, Max Iterations: 10\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 39\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 1: Sending 39 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 1: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","   Lowering threshold to 0.350 for next try.\n","DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 39\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 2: Sending 39 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 2: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","   Lowering threshold to 0.300 for next try.\n","DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 41\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 3: Sending 41 new candidates to LLM for selection.\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing Claims:  60%|██████    | 18/30 [06:43<05:17, 26.44s/it]"]},{"output_type":"stream","name":"stdout","text":["DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 3: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","M2 Iter 3: Stopping because LLM found no evidence after two tries.\n","M2: Finished iterations. No evidence selected.\n","-------------------------------------------------------------------\n","\n","###### M3: Starting Classification ######\n","M3: No evidence text provided. Classifying as NOT ENOUGH INFO.\n","Adding gold label/evidence to prediction.\n","##########################################################################\n","\n","##########################################################################\n","\n","Time to process claim: 25.29 seconds.\n","------------------------------------------------\n","\n","\n","--- Processing Claim ID: 125577 ---\n","DEBUG 1.0 (query_generator):\n","\tClaim: Ron Dennis is unemployed.\n","\tEntities: ['Dennis', 'Ron']\n","\tLLM Output: ['ron_dennis', 'formula_one', 'mclaren', 'mclaren_automotive', 'ron_dennis#career']\n","-_--_--_--_--_-\n","DEBUG 1.1 (query_generator):\n","\tEntities: ['Dennis', 'Ron']\n","\tGenerated Potential Titles: ['mclaren', 'ron_dennis', 'formula_one', 'ron_dennis#career', 'mclaren_automotive']\n","\tSelected Titles for Retrieval: ['mclaren', 'ron_dennis', 'formula_one', 'ron_dennis#career', 'mclaren_automotive']\n","-_--_--_--_--_--_--_--_--_--_-\n","###### M1: Retrieving Documents ######\n","DEBUG 1.2: Successfully retrieved intro from 'Oscar Piastri' (searched for 'mclaren')\n","DEBUG 1.2: Successfully retrieved intro from 'Ron Dennis' (searched for 'ron_dennis')\n","DEBUG 1.2: Successfully retrieved intro from 'Racing Bulls' (searched for 'formula_one')\n","DEBUG 1.2: Successfully retrieved intro from 'Ron Dennis' (searched for 'ron_dennis#career')\n","DEBUG 1.2: Successfully retrieved intro from 'Bruce McLaren' (searched for 'mclaren_automotive')\n","DEBUG 1.2: Retrieved content for 5 pages out of 5 potential titles.\n","-_--_--_--_--_--_--_--_--_--_-\n","-------------------------------------------------------------------\n","\n","###### M2: Starting Evidence Extraction ######\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Ron Dennis is unemployed.\n","\tRephrased Claim: Ron Dennis is currently without a job.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Ron Dennis is unemployed.\n","\tRephrased Claim: Ron Dennis is currently without a job.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Ron Dennis is unemployed.\n","\tRephrased Claim: Ron Dennis is currently out of work.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Ron Dennis is unemployed.\n","\tRephrased Claim: Ron Dennis is out of work.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Ron Dennis is unemployed.\n","\tRephrased Claim: Ron Dennis is out of work.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Ron Dennis is unemployed.\n","\tRephrased Claim: Ron Dennis is currently without a job.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Ron Dennis is unemployed.\n","\tRephrased Claim: Ron Dennis is currently out of work.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Ron Dennis is unemployed.\n","\tRephrased Claim: Ron Dennis is without a job.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Ron Dennis is unemployed.\n","\tRephrased Claim: Ron Dennis is currently without a job.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Ron Dennis is unemployed.\n","\tRephrased Claim: Ron Dennis is not currently employed.\n","-_--_--_--_--_-\n","DEBUG 2.1 (module_2_controls):\n","\tClaim: Ron Dennis is unemployed.\n","\tEntities: ['Dennis', 'Ron']\n","\tKeywords: ['unemployed', 'ron', 'dennis']\n","\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n","\tMax Evidence Target: 8, Max Iterations: 10\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 4\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 1: Sending 4 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 1: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","   Lowering threshold to 0.350 for next try.\n","DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 7\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 2: Sending 7 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 2: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","   Lowering threshold to 0.300 for next try.\n","DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 11\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 3: Sending 11 new candidates to LLM for selection.\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing Claims:  63%|██████▎   | 19/30 [07:03<04:28, 24.41s/it]"]},{"output_type":"stream","name":"stdout","text":["DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 3: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","M2 Iter 3: Stopping because LLM found no evidence after two tries.\n","M2: Finished iterations. No evidence selected.\n","-------------------------------------------------------------------\n","\n","###### M3: Starting Classification ######\n","M3: No evidence text provided. Classifying as NOT ENOUGH INFO.\n","Adding gold label/evidence to prediction.\n","##########################################################################\n","\n","##########################################################################\n","\n","Time to process claim: 19.68 seconds.\n","------------------------------------------------\n","\n","\n","--- Processing Claim ID: 132244 ---\n","DEBUG 1.0 (query_generator):\n","\tClaim: Wolfgang Amadeus Mozart showed he was a child protege.\n","\tEntities: ['Amadeus Mozart', 'Wolfgang']\n","\tLLM Output: ['wolfgang_amadeus_mozart', 'child_prodigy', 'classical_music', 'history_of_music']\n","-_--_--_--_--_-\n","DEBUG 1.1 (query_generator):\n","\tEntities: ['Amadeus Mozart', 'Wolfgang']\n","\tGenerated Potential Titles: ['child_prodigy', 'classical_music', 'history_of_music', 'wolfgang_amadeus_mozart']\n","\tSelected Titles for Retrieval: ['child_prodigy', 'classical_music', 'history_of_music', 'wolfgang_amadeus_mozart']\n","-_--_--_--_--_--_--_--_--_--_-\n","###### M1: Retrieving Documents ######\n","DEBUG 1.2: Successfully retrieved intro from 'The Child Prodigy' (searched for 'child_prodigy')\n","DEBUG 1.2: Successfully retrieved intro from 'Classical music' (searched for 'classical_music')\n","DEBUG 1.2: Successfully retrieved intro from 'Music history of Italy' (searched for 'history_of_music')\n","DEBUG 1.2: Successfully retrieved intro from 'Wolfgang Amadeus Mozart' (searched for 'wolfgang_amadeus_mozart')\n","DEBUG 1.2: Retrieved content for 4 pages out of 4 potential titles.\n","-_--_--_--_--_--_--_--_--_--_-\n","-------------------------------------------------------------------\n","\n","###### M2: Starting Evidence Extraction ######\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Wolfgang Amadeus Mozart showed he was a child protege.\n","\tRephrased Claim: Wolfgang Amadeus Mozart demonstrated that he was a prodigious child talent.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Wolfgang Amadeus Mozart showed he was a child protege.\n","\tRephrased Claim: Wolfgang Amadeus Mozart demonstrated that he was a musical prodigy from a young age.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Wolfgang Amadeus Mozart showed he was a child protege.\n","\tRephrased Claim: Wolfgang Amadeus Mozart demonstrated that he was a prodigy from a young age.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Wolfgang Amadeus Mozart showed he was a child protege.\n","\tRephrased Claim: Wolfgang Amadeus Mozart demonstrated that he was a musical prodigy as a child.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Wolfgang Amadeus Mozart showed he was a child protege.\n","\tRephrased Claim: Wolfgang Amadeus Mozart demonstrated that he was a prodigious child musician.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Wolfgang Amadeus Mozart showed he was a child protege.\n","\tRephrased Claim: Wolfgang Amadeus Mozart demonstrated that he was a musical prodigy from a young age.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Wolfgang Amadeus Mozart showed he was a child protege.\n","\tRephrased Claim: Wolfgang Amadeus Mozart demonstrated that he was a prodigious talent from a young age.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Wolfgang Amadeus Mozart showed he was a child protege.\n","\tRephrased Claim: Wolfgang Amadeus Mozart demonstrated that he was a child prodigy.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Wolfgang Amadeus Mozart showed he was a child protege.\n","\tRephrased Claim: Wolfgang Amadeus Mozart demonstrated that he was a prodigious child talent.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Wolfgang Amadeus Mozart showed he was a child protege.\n","\tRephrased Claim: Wolfgang Amadeus Mozart demonstrated that he was a prodigy from a young age.\n","-_--_--_--_--_-\n","DEBUG 2.1 (module_2_controls):\n","\tClaim: Wolfgang Amadeus Mozart showed he was a child protege.\n","\tEntities: ['Amadeus Mozart', 'Wolfgang']\n","\tKeywords: ['wolfgang', 'showed', 'protege', 'mozart', 'child']\n","\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n","\tMax Evidence Target: 8, Max Iterations: 10\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 11\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 1: Sending 11 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","1. Born in Salzburg, Mozart showed prodigious ability from his earliest childhood.\n","11. At age five, he was already competent on keyboard and violin, had begun to compose, and performed before European royalty.\n","-_--_--_--_--_-\n","M2 Iter 1: LLM selected 2 sentences.\n","M2 Iter 1: LLM selected few items (2). Lowering sBERT threshold to 0.350\n","DEBUG 2. End Iter 1: Total evidence found: 2\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 16\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 2: Sending 14 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","9. His father Leopold Mozart took him on a grand tour of Europe and then three trips to Italy.  \n","11. Despite his short life, his rapid pace of composition and proficiency from an early age resulted in more than 800 works representing virtually every Western classical genre of his time.\n","-_--_--_--_--_-\n","M2 Iter 2: LLM selected 2 sentences.\n","M2 Iter 2: LLM selected few items (2). Lowering sBERT threshold to 0.300\n","DEBUG 2. End Iter 2: Total evidence found: 4\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 24\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 3: Sending 20 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 3: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","M2 Iter 3: Stopping because LLM found no evidence after two tries.\n","M2: Finished. Selected 4 evidence items.\n","-------------------------------------------------------------------\n","\n","###### M3: Starting Classification ######\n","DEBUG 3.1 (module_3_classification):\n","\tClaim: Wolfgang Amadeus Mozart showed he was a child protege.\n","\tEvidence Texts Sent (4):\n","\tPrompt (partial): Based ONLY on the following evidence sentences, classify the claim as SUPPORTS, REFUTES, or NOT ENOUGH INFO.\n","\n","Claim: 'Wolfgang Amadeus Mozart showed he was a child protege.'\n","\n","Evidence:\n","- At age five, ...\n","-_--_--_--_--_--_--_--_--_--_-\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing Claims:  67%|██████▋   | 20/30 [07:22<03:48, 22.85s/it]"]},{"output_type":"stream","name":"stdout","text":["DEBUG 3.2/3.3 (module_3_classification):\n","\tLLM Classification Result: SUPPORTS\n","\tExit Status: OK\n","-_--_--_--_--_--_--_--_--_--_-\n","Adding gold label/evidence to prediction.\n","##########################################################################\n","\n","##########################################################################\n","\n","Time to process claim: 19.21 seconds.\n","------------------------------------------------\n","\n","\n","--- Processing Claim ID: 225798 ---\n","DEBUG 1.0 (query_generator):\n","\tClaim: Chinatown's writer is a convicted statutory rapist.\n","\tEntities: ['Chinatown']\n","\tLLM Output: ['chinatown', 'robert_towne', 'roman_polanski', 'statutory_rape']\n","-_--_--_--_--_-\n","DEBUG 1.1 (query_generator):\n","\tEntities: ['Chinatown']\n","\tGenerated Potential Titles: ['chinatown', 'robert_towne', 'roman_polanski', 'statutory_rape']\n","\tSelected Titles for Retrieval: ['chinatown', 'robert_towne', 'roman_polanski', 'statutory_rape']\n","-_--_--_--_--_--_--_--_--_--_-\n","###### M1: Retrieving Documents ######\n","DEBUG 1.2: Successfully retrieved intro from 'Detective Chinatown 1900' (searched for 'chinatown')\n","DEBUG 1.2: Successfully retrieved intro from 'Chinatown (1974 film)' (searched for 'robert_towne')\n","DEBUG 1.2: Successfully retrieved intro from 'Roman Polanski sexual abuse case' (searched for 'roman_polanski')\n","DEBUG 1.2: Successfully retrieved intro from 'Statutory rape' (searched for 'statutory_rape')\n","DEBUG 1.2: Retrieved content for 4 pages out of 4 potential titles.\n","-_--_--_--_--_--_--_--_--_--_-\n","-------------------------------------------------------------------\n","\n","###### M2: Starting Evidence Extraction ######\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Chinatown's writer is a convicted statutory rapist.\n","\tRephrased Claim: The writer of Chinatown has been convicted of statutory rape.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Chinatown's writer is a convicted statutory rapist.\n","\tRephrased Claim: The writer of Chinatown has been convicted of statutory rape.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Chinatown's writer is a convicted statutory rapist.\n","\tRephrased Claim: The writer of Chinatown has been convicted of statutory rape.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Chinatown's writer is a convicted statutory rapist.\n","\tRephrased Claim: The writer of Chinatown has been convicted of statutory rape.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Chinatown's writer is a convicted statutory rapist.\n","\tRephrased Claim: The writer of Chinatown is a convicted individual for statutory rape.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Chinatown's writer is a convicted statutory rapist.\n","\tRephrased Claim: The writer of Chinatown has been convicted of statutory rape.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Chinatown's writer is a convicted statutory rapist.\n","\tRephrased Claim: The writer of Chinatown is a convicted offender of statutory rape.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Chinatown's writer is a convicted statutory rapist.\n","\tRephrased Claim: The author of Chinatown is a convicted statutory rapist.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Chinatown's writer is a convicted statutory rapist.\n","\tRephrased Claim: The author of Chinatown has been convicted of statutory rape.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Chinatown's writer is a convicted statutory rapist.\n","\tRephrased Claim: The writer of Chinatown has been convicted of statutory rape.\n","-_--_--_--_--_-\n","DEBUG 2.1 (module_2_controls):\n","\tClaim: Chinatown's writer is a convicted statutory rapist.\n","\tEntities: ['Chinatown']\n","\tKeywords: ['writer', 'statutory', 'rapist', 'convicted', 'chinatown']\n","\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n","\tMax Evidence Target: 8, Max Iterations: 10\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 8\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 1: Sending 8 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","7. Chinatown is a 1974 American neo-noir mystery film directed by Roman Polanski and written by Robert Towne.  \n","8. On March 10, 1977, 43-year-old film director Roman Polanski was arrested and charged in Los Angeles with six offenses against Samantha Gailey (now Geimer), a 13-year-old girl: unlawful sexual intercourse with a minor, rape by use of drugs, perversion, sodomy, a lewd and lascivious act upon a child under the age of 14, and furnishing a controlled substance to a minor.\n","-_--_--_--_--_-\n","M2 Iter 1: LLM selected 2 sentences.\n","DEBUG 2. End Iter 1: Total evidence found: 2\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.400\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 8\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 2: Sending 8 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 2: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","   Lowering threshold to 0.350 for next try.\n","DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.350\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 12\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 3: Sending 10 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 3: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","M2 Iter 3: Stopping because LLM found no evidence after two tries.\n","M2: Finished. Selected 2 evidence items.\n","-------------------------------------------------------------------\n","\n","###### M3: Starting Classification ######\n","DEBUG 3.1 (module_3_classification):\n","\tClaim: Chinatown's writer is a convicted statutory rapist.\n","\tEvidence Texts Sent (2):\n","\tPrompt (partial): Based ONLY on the following evidence sentences, classify the claim as SUPPORTS, REFUTES, or NOT ENOUGH INFO.\n","\n","Claim: 'Chinatown's writer is a convicted statutory rapist.'\n","\n","Evidence:\n","- Chinatown is a 1...\n","-_--_--_--_--_--_--_--_--_--_-\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing Claims:  70%|███████   | 21/30 [07:40<03:12, 21.38s/it]"]},{"output_type":"stream","name":"stdout","text":["DEBUG 3.2/3.3 (module_3_classification):\n","\tLLM Classification Result: REFUTES\n","\tExit Status: OK\n","-_--_--_--_--_--_--_--_--_--_-\n","Adding gold label/evidence to prediction.\n","##########################################################################\n","\n","##########################################################################\n","\n","Time to process claim: 17.95 seconds.\n","------------------------------------------------\n","\n","\n","--- Processing Claim ID: 46810 ---\n","DEBUG 1.0 (query_generator):\n","\tClaim: One Dance has always been banned in the Netherlands.\n","\tEntities: ['Netherlands', 'Dance']\n","\tLLM Output: ['one_dance', 'netherlands', 'dance', 'music_bans_in_the_netherlands']\n","-_--_--_--_--_-\n","DEBUG 1.1 (query_generator):\n","\tEntities: ['Netherlands', 'Dance']\n","\tGenerated Potential Titles: ['dance', 'one_dance', 'netherlands', 'music_bans_in_the_netherlands']\n","\tSelected Titles for Retrieval: ['dance', 'one_dance', 'netherlands', 'music_bans_in_the_netherlands']\n","-_--_--_--_--_--_--_--_--_--_-\n","###### M1: Retrieving Documents ######\n","DEBUG 1.2: Successfully retrieved intro from 'Dance Dance Danseur' (searched for 'dance')\n","DEBUG 1.2: Successfully retrieved intro from 'One Dance' (searched for 'one_dance')\n","DEBUG 1.2: Successfully retrieved intro from 'Pornography in the Netherlands' (searched for 'netherlands')\n","DEBUG 1.2: Successfully retrieved intro from 'Carnival in the Netherlands' (searched for 'music_bans_in_the_netherlands')\n","DEBUG 1.2: Retrieved content for 4 pages out of 4 potential titles.\n","-_--_--_--_--_--_--_--_--_--_-\n","-------------------------------------------------------------------\n","\n","###### M2: Starting Evidence Extraction ######\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: One Dance has always been banned in the Netherlands.\n","\tRephrased Claim: One Dance has consistently been prohibited in the Netherlands.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: One Dance has always been banned in the Netherlands.\n","\tRephrased Claim: One Dance has consistently been prohibited in the Netherlands.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: One Dance has always been banned in the Netherlands.\n","\tRephrased Claim: One Dance has consistently been prohibited in the Netherlands.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: One Dance has always been banned in the Netherlands.\n","\tRephrased Claim: One Dance has consistently been prohibited in the Netherlands.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: One Dance has always been banned in the Netherlands.\n","\tRephrased Claim: One Dance has consistently been prohibited in the Netherlands.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: One Dance has always been banned in the Netherlands.\n","\tRephrased Claim: One Dance has consistently been prohibited in the Netherlands.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: One Dance has always been banned in the Netherlands.\n","\tRephrased Claim: One Dance has consistently been prohibited in the Netherlands.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: One Dance has always been banned in the Netherlands.\n","\tRephrased Claim: One Dance has consistently been prohibited in the Netherlands.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: One Dance has always been banned in the Netherlands.\n","\tRephrased Claim: One Dance has consistently been prohibited in the Netherlands.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: One Dance has always been banned in the Netherlands.\n","\tRephrased Claim: The Netherlands has consistently prohibited One Dance.\n","-_--_--_--_--_-\n","DEBUG 2.1 (module_2_controls):\n","\tClaim: One Dance has always been banned in the Netherlands.\n","\tEntities: ['Netherlands', 'Dance']\n","\tKeywords: ['netherlands', 'dance', 'banned']\n","\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n","\tMax Evidence Target: 8, Max Iterations: 10\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 7\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 1: Sending 7 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 1: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","   Lowering threshold to 0.350 for next try.\n","DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 9\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 2: Sending 9 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 2: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","   Lowering threshold to 0.300 for next try.\n","DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 11\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 3: Sending 11 new candidates to LLM for selection.\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing Claims:  73%|███████▎  | 22/30 [07:57<02:40, 20.03s/it]"]},{"output_type":"stream","name":"stdout","text":["DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 3: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","M2 Iter 3: Stopping because LLM found no evidence after two tries.\n","M2: Finished iterations. No evidence selected.\n","-------------------------------------------------------------------\n","\n","###### M3: Starting Classification ######\n","M3: No evidence text provided. Classifying as NOT ENOUGH INFO.\n","Adding gold label/evidence to prediction.\n","##########################################################################\n","\n","##########################################################################\n","\n","Time to process claim: 16.88 seconds.\n","------------------------------------------------\n","\n","\n","--- Processing Claim ID: 85923 ---\n","DEBUG 1.0 (query_generator):\n","\tClaim: Adidas designs items.\n","\tEntities: ['Adidas']\n","\tLLM Output: ['adidas', 'adidas_products', 'adidas_sportswear', 'adidas_football', 'adidas_basketball', 'adidas_running', 'adidas_footwear', 'adidas_apparel']\n","-_--_--_--_--_-\n","DEBUG 1.1 (query_generator):\n","\tEntities: ['Adidas']\n","\tGenerated Potential Titles: ['adidas', 'adidas_apparel', 'adidas_running', 'adidas_football', 'adidas_footwear', 'adidas_products', 'adidas_basketball', 'adidas_sportswear']\n","\tSelected Titles for Retrieval: ['adidas', 'adidas_apparel', 'adidas_running', 'adidas_football', 'adidas_footwear', 'adidas_products', 'adidas_basketball', 'adidas_sportswear']\n","-_--_--_--_--_--_--_--_--_--_-\n","###### M1: Retrieving Documents ######\n","DEBUG 1.2: Successfully retrieved intro from 'Adidas' (searched for 'adidas')\n","DEBUG 1.2: Successfully retrieved intro from 'Adidas' (searched for 'adidas_apparel')\n","DEBUG 1.2: Successfully retrieved intro from 'Adidas' (searched for 'adidas_running')\n","DEBUG 1.2: Successfully retrieved intro from 'Adidas' (searched for 'adidas_football')\n","DEBUG 1.2: Successfully retrieved intro from 'Adidas' (searched for 'adidas_footwear')\n","DEBUG 1.2: Successfully retrieved intro from 'Adidas' (searched for 'adidas_products')\n","DEBUG 1.2: Successfully retrieved intro from 'Adidas' (searched for 'adidas_basketball')\n","DEBUG 1.2: Successfully retrieved intro from 'Adidas' (searched for 'adidas_sportswear')\n","DEBUG 1.2: Retrieved content for 8 pages out of 8 potential titles.\n","-_--_--_--_--_--_--_--_--_--_-\n","-------------------------------------------------------------------\n","\n","###### M2: Starting Evidence Extraction ######\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Adidas designs items.\n","\tRephrased Claim: Adidas creates products.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Adidas designs items.\n","\tRephrased Claim: Adidas creates products.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Adidas designs items.\n","\tRephrased Claim: Adidas creates products.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Adidas designs items.\n","\tRephrased Claim: Adidas creates products.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Adidas designs items.\n","\tRephrased Claim: Adidas creates products.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Adidas designs items.\n","\tRephrased Claim: Adidas creates products.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Adidas designs items.\n","\tRephrased Claim: Adidas creates products.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Adidas designs items.\n","\tRephrased Claim: Adidas creates products.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Adidas designs items.\n","\tRephrased Claim: Adidas creates products.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Adidas designs items.\n","\tRephrased Claim: Adidas creates products.\n","-_--_--_--_--_-\n","DEBUG 2.1 (module_2_controls):\n","\tClaim: Adidas designs items.\n","\tEntities: ['Adidas']\n","\tKeywords: ['items', 'designs', 'adidas']\n","\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n","\tMax Evidence Target: 8, Max Iterations: 10\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 56\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 1: Sending 56 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","9. The three stripes are Adidas's identity mark, having been used on the company's clothing and shoe designs as a marketing aid.\n","10. The three stripes are Adidas's identity mark, having been used on the company's clothing and shoe designs as a marketing aid.\n","11. The three stripes are Adidas's identity mark, having been used on the company's clothing and shoe designs as a marketing aid.\n","12. The three stripes are Adidas's identity mark, having been used on the company's clothing and shoe designs as a marketing aid.\n","13. The three stripes are Adidas's identity mark, having been used on the company's clothing and shoe designs as a marketing aid.\n","14. The three stripes are Adidas's identity mark, having been used on the company's clothing and shoe designs as a marketing aid.\n","15. The three stripes are Adidas's identity mark, having been used on the company's clothing and shoe designs as a marketing aid.\n","16. The three stripes are Adidas's identity mark, having been used on the company's clothing and shoe designs as a marketing aid.\n","41. In 1949, following a breakdown in the relationship between the brothers, Adolf created Adidas and Rudolf established Puma, which became Adidas's business rival.\n","42. In 1949, following a breakdown in the relationship between the brothers, Adolf created Adidas and Rudolf established Puma, which became Adidas's business rival.\n","43. In 1949, following a breakdown in the relationship between the brothers, Adolf created Adidas and Rudolf established Puma, which became Adidas's business rival.\n","44. In 1949, following a breakdown in the relationship between the brothers, Adolf created Adidas and Rudolf established Puma, which became Adidas's business rival.\n","45. In 1949, following a breakdown in the relationship between the brothers, Adolf created Adidas and Rudolf established Puma, which became Adidas's business rival.\n","46. In 1949, following a breakdown in the relationship between the brothers, Adolf created Adidas and Rudolf established Puma, which became Adidas's business rival.\n","47. In 1949, following a breakdown in the relationship between the brothers, Adolf created Adidas and Rudolf established Puma, which became Adidas's business rival.\n","48. In 1949, following a breakdown in the relationship between the brothers, Adolf created Adidas and Rudolf established Puma, which became Adidas's business rival.\n","-_--_--_--_--_-\n","M2 Iter 1: LLM selected 16 sentences.\n","   Warning: Could not re-associate LLM output with retrieved data: 'The three stripes are Adidas's identity mark, having been used on the company's ...'\n","   Warning: Could not re-associate LLM output with retrieved data: 'The three stripes are Adidas's identity mark, having been used on the company's ...'\n","   Warning: Could not re-associate LLM output with retrieved data: 'The three stripes are Adidas's identity mark, having been used on the company's ...'\n","   Warning: Could not re-associate LLM output with retrieved data: 'The three stripes are Adidas's identity mark, having been used on the company's ...'\n","   Warning: Could not re-associate LLM output with retrieved data: 'The three stripes are Adidas's identity mark, having been used on the company's ...'\n","   Warning: Could not re-associate LLM output with retrieved data: 'The three stripes are Adidas's identity mark, having been used on the company's ...'\n","   Warning: Could not re-associate LLM output with retrieved data: 'The three stripes are Adidas's identity mark, having been used on the company's ...'\n","   Warning: Could not re-associate LLM output with retrieved data: 'In 1949, following a breakdown in the relationship between the brothers, Adolf c...'\n","   Warning: Could not re-associate LLM output with retrieved data: 'In 1949, following a breakdown in the relationship between the brothers, Adolf c...'\n","   Warning: Could not re-associate LLM output with retrieved data: 'In 1949, following a breakdown in the relationship between the brothers, Adolf c...'\n","   Warning: Could not re-associate LLM output with retrieved data: 'In 1949, following a breakdown in the relationship between the brothers, Adolf c...'\n","   Warning: Could not re-associate LLM output with retrieved data: 'In 1949, following a breakdown in the relationship between the brothers, Adolf c...'\n","   Warning: Could not re-associate LLM output with retrieved data: 'In 1949, following a breakdown in the relationship between the brothers, Adolf c...'\n","   Warning: Could not re-associate LLM output with retrieved data: 'In 1949, following a breakdown in the relationship between the brothers, Adolf c...'\n","M2 Iter 1: LLM selected few items (2). Lowering sBERT threshold to 0.350\n","DEBUG 2. End Iter 1: Total evidence found: 2\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 56\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 2: Sending 40 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","9. Adidas AG (German pronunciation: [ˈʔadiˌdas] ; stylized in all lowercase since 1949) is a German athletic apparel and footwear corporation headquartered in Herzogenaurach, Bavaria, Germany.  \n","10. Adidas AG (German pronunciation: [ˈʔadiˌdas] ; stylized in all lowercase since 1949) is a German athletic apparel and footwear corporation headquartered in Herzogenaurach, Bavaria, Germany.  \n","11. Adidas AG (German pronunciation: [ˈʔadiˌdas] ; stylized in all lowercase since 1949) is a German athletic apparel and footwear corporation headquartered in Herzogenaurach, Bavaria, Germany.  \n","12. Adidas AG (German pronunciation: [ˈʔadiˌdas] ; stylized in all lowercase since 1949) is a German athletic apparel and footwear corporation headquartered in Herzogenaurach, Bavaria, Germany.  \n","13. Adidas AG (German pronunciation: [ˈʔadiˌdas] ; stylized in all lowercase since 1949) is a German athletic apparel and footwear corporation headquartered in Herzogenaurach, Bavaria, Germany.  \n","14. Adidas AG (German pronunciation: [ˈʔadiˌdas] ; stylized in all lowercase since 1949) is a German athletic apparel and footwear corporation headquartered in Herzogenaurach, Bavaria, Germany.  \n","15. Adidas AG (German pronunciation: [ˈʔadiˌdas] ; stylized in all lowercase since 1949) is a German athletic apparel and footwear corporation headquartered in Herzogenaurach, Bavaria, Germany.  \n","16. Adidas AG (German pronunciation: [ˈʔadiˌdas] ; stylized in all lowercase since 1949) is a German athletic apparel and footwear corporation headquartered in Herzogenaurach, Bavaria, Germany.  \n","33. It is the largest sportswear manufacturer in Europe, and the second largest in the world, after Nike.  \n","34. It is the largest sportswear manufacturer in Europe, and the second largest in the world, after Nike.  \n","35. It is the largest sportswear manufacturer in Europe, and the second largest in the world, after Nike.  \n","36. It is the largest sportswear manufacturer in Europe, and the second largest in the world, after Nike.  \n","37. It is the largest sportswear manufacturer in Europe, and the second largest in the world, after Nike.\n","-_--_--_--_--_-\n","M2 Iter 2: LLM selected 13 sentences.\n","   Warning: Could not re-associate LLM output with retrieved data: 'Adidas AG (German pronunciation: [ˈʔadiˌdas] ; stylized in all lowercase since 1...'\n","   Warning: Could not re-associate LLM output with retrieved data: 'Adidas AG (German pronunciation: [ˈʔadiˌdas] ; stylized in all lowercase since 1...'\n","   Warning: Could not re-associate LLM output with retrieved data: 'Adidas AG (German pronunciation: [ˈʔadiˌdas] ; stylized in all lowercase since 1...'\n","   Warning: Could not re-associate LLM output with retrieved data: 'Adidas AG (German pronunciation: [ˈʔadiˌdas] ; stylized in all lowercase since 1...'\n","   Warning: Could not re-associate LLM output with retrieved data: 'Adidas AG (German pronunciation: [ˈʔadiˌdas] ; stylized in all lowercase since 1...'\n","   Warning: Could not re-associate LLM output with retrieved data: 'Adidas AG (German pronunciation: [ˈʔadiˌdas] ; stylized in all lowercase since 1...'\n","   Warning: Could not re-associate LLM output with retrieved data: 'Adidas AG (German pronunciation: [ˈʔadiˌdas] ; stylized in all lowercase since 1...'\n","   Warning: Could not re-associate LLM output with retrieved data: 'It is the largest sportswear manufacturer in Europe, and the second largest in t...'\n","   Warning: Could not re-associate LLM output with retrieved data: 'It is the largest sportswear manufacturer in Europe, and the second largest in t...'\n","   Warning: Could not re-associate LLM output with retrieved data: 'It is the largest sportswear manufacturer in Europe, and the second largest in t...'\n","   Warning: Could not re-associate LLM output with retrieved data: 'It is the largest sportswear manufacturer in Europe, and the second largest in t...'\n","M2 Iter 2: LLM selected few items (2). Lowering sBERT threshold to 0.300\n","DEBUG 2. End Iter 2: Total evidence found: 4\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 64\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 3: Sending 32 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","25. Dassler assisted in the development of spiked running shoes (spikes) for multiple athletic events.\n","26. Dassler assisted in the development of spiked running shoes (spikes) for multiple athletic events.\n","27. Dassler assisted in the development of spiked running shoes (spikes) for multiple athletic events.\n","28. Dassler assisted in the development of spiked running shoes (spikes) for multiple athletic events.\n","29. Dassler assisted in the development of spiked running shoes (spikes) for multiple athletic events.\n","30. Dassler assisted in the development of spiked running shoes (spikes) for multiple athletic events.\n","31. Dassler assisted in the development of spiked running shoes (spikes) for multiple athletic events.\n","32. Dassler assisted in the development of spiked running shoes (spikes) for multiple athletic events.\n","-_--_--_--_--_-\n","M2 Iter 3: LLM selected 8 sentences.\n","   Warning: Could not re-associate LLM output with retrieved data: 'Dassler assisted in the development of spiked running shoes (spikes) for multipl...'\n","   Warning: Could not re-associate LLM output with retrieved data: 'Dassler assisted in the development of spiked running shoes (spikes) for multipl...'\n","   Warning: Could not re-associate LLM output with retrieved data: 'Dassler assisted in the development of spiked running shoes (spikes) for multipl...'\n","   Warning: Could not re-associate LLM output with retrieved data: 'Dassler assisted in the development of spiked running shoes (spikes) for multipl...'\n","   Warning: Could not re-associate LLM output with retrieved data: 'Dassler assisted in the development of spiked running shoes (spikes) for multipl...'\n","   Warning: Could not re-associate LLM output with retrieved data: 'Dassler assisted in the development of spiked running shoes (spikes) for multipl...'\n","   Warning: Could not re-associate LLM output with retrieved data: 'Dassler assisted in the development of spiked running shoes (spikes) for multipl...'\n","M2 Iter 3: LLM selected few items (1). Lowering sBERT threshold to 0.250\n","DEBUG 2. End Iter 3: Total evidence found: 5\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 4/10, Current sBERT Thresh: 0.250\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 88\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 4: Sending 48 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","33. To enhance the quality of spiked athletic footwear, he transitioned from a previous model of heavy metal spikes to utilising canvas and rubber.\n","41. Dassler persuaded U.S. sprinter Jesse Owens to use his handmade spikes at the 1936 Summer Olympics.\n","-_--_--_--_--_-\n","M2 Iter 4: LLM selected 2 sentences.\n","M2 Iter 4: LLM selected few items (2). Lowering sBERT threshold to 0.200\n","DEBUG 2. End Iter 4: Total evidence found: 7\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 5/10, Current sBERT Thresh: 0.200\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 96\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 5: Sending 40 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 5: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","M2 Iter 5: Stopping because LLM found no evidence after two tries.\n","M2: Finished. Selected 7 evidence items.\n","-------------------------------------------------------------------\n","\n","###### M3: Starting Classification ######\n","DEBUG 3.1 (module_3_classification):\n","\tClaim: Adidas designs items.\n","\tEvidence Texts Sent (7):\n","\tPrompt (partial): Based ONLY on the following evidence sentences, classify the claim as SUPPORTS, REFUTES, or NOT ENOUGH INFO.\n","\n","Claim: 'Adidas designs items.'\n","\n","Evidence:\n","- Dassler persuaded U.S. sprinter Jesse Owens to...\n","-_--_--_--_--_--_--_--_--_--_-\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing Claims:  77%|███████▋  | 23/30 [08:41<03:11, 27.30s/it]"]},{"output_type":"stream","name":"stdout","text":["DEBUG 3.2/3.3 (module_3_classification):\n","\tLLM Classification Result: SUPPORTS\n","\tExit Status: OK\n","-_--_--_--_--_--_--_--_--_--_-\n","Adding gold label/evidence to prediction.\n","##########################################################################\n","\n","##########################################################################\n","\n","Time to process claim: 44.25 seconds.\n","------------------------------------------------\n","\n","\n","--- Processing Claim ID: 181252 ---\n","DEBUG 1.0 (query_generator):\n","\tClaim: Sean Gunn is an American poet.\n","\tEntities: ['American', 'Sean', 'Gunn']\n","\tLLM Output: ['sean_gunn', 'american_poets', 'poetry_in_the_united_states']\n","-_--_--_--_--_-\n","DEBUG 1.1 (query_generator):\n","\tEntities: ['American', 'Sean', 'Gunn']\n","\tGenerated Potential Titles: ['sean_gunn', 'american_poets', 'poetry_in_the_united_states']\n","\tSelected Titles for Retrieval: ['sean_gunn', 'american_poets', 'poetry_in_the_united_states']\n","-_--_--_--_--_--_--_--_--_--_-\n","###### M1: Retrieving Documents ######\n","DEBUG 1.2: Successfully retrieved intro from 'Sean Gunn' (searched for 'sean_gunn')\n","DEBUG 1.2: Successfully retrieved intro from 'American poetry' (searched for 'american_poets')\n","DEBUG 1.2: Successfully retrieved intro from 'American poetry' (searched for 'poetry_in_the_united_states')\n","DEBUG 1.2: Retrieved content for 3 pages out of 3 potential titles.\n","-_--_--_--_--_--_--_--_--_--_-\n","-------------------------------------------------------------------\n","\n","###### M2: Starting Evidence Extraction ######\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Sean Gunn is an American poet.\n","\tRephrased Claim: Sean Gunn is a poet from the United States.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Sean Gunn is an American poet.\n","\tRephrased Claim: Sean Gunn is a poet from the United States.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Sean Gunn is an American poet.\n","\tRephrased Claim: Sean Gunn is a poet from the United States.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Sean Gunn is an American poet.\n","\tRephrased Claim: Sean Gunn is a poet from the United States.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Sean Gunn is an American poet.\n","\tRephrased Claim: Sean Gunn is a poet from the United States.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Sean Gunn is an American poet.\n","\tRephrased Claim: Sean Gunn is a poet from the United States.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Sean Gunn is an American poet.\n","\tRephrased Claim: Sean Gunn is a poet from the United States.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Sean Gunn is an American poet.\n","\tRephrased Claim: Sean Gunn is a poet from the United States.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Sean Gunn is an American poet.\n","\tRephrased Claim: Sean Gunn is a poet from the United States.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Sean Gunn is an American poet.\n","\tRephrased Claim: Sean Gunn is a poet from the United States.\n","-_--_--_--_--_-\n","DEBUG 2.1 (module_2_controls):\n","\tClaim: Sean Gunn is an American poet.\n","\tEntities: ['American', 'Sean', 'Gunn']\n","\tKeywords: ['sean', 'poet', 'gunn', 'american']\n","\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n","\tMax Evidence Target: 8, Max Iterations: 10\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 10\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 1: Sending 10 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","1. Sean Gunn (born May 22, 1974) is an American actor.\n","-_--_--_--_--_-\n","M2 Iter 1: LLM selected 1 sentences.\n","M2 Iter 1: LLM selected few items (1). Lowering sBERT threshold to 0.350\n","DEBUG 2. End Iter 1: Total evidence found: 1\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 14\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 2: Sending 13 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 2: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","   Lowering threshold to 0.300 for next try.\n","DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 18\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 3: Sending 17 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 3: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","M2 Iter 3: Stopping because LLM found no evidence after two tries.\n","M2: Finished. Selected 1 evidence items.\n","-------------------------------------------------------------------\n","\n","###### M3: Starting Classification ######\n","DEBUG 3.1 (module_3_classification):\n","\tClaim: Sean Gunn is an American poet.\n","\tEvidence Texts Sent (1):\n","\tPrompt (partial): Based ONLY on the following evidence sentences, classify the claim as SUPPORTS, REFUTES, or NOT ENOUGH INFO.\n","\n","Claim: 'Sean Gunn is an American poet.'\n","\n","Evidence:\n","- Sean Gunn (born May 22, 1974) is an A...\n","-_--_--_--_--_--_--_--_--_--_-\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing Claims:  80%|████████  | 24/30 [09:02<02:31, 25.25s/it]"]},{"output_type":"stream","name":"stdout","text":["DEBUG 3.2/3.3 (module_3_classification):\n","\tLLM Classification Result: REFUTES\n","\tExit Status: OK\n","-_--_--_--_--_--_--_--_--_--_-\n","Adding gold label/evidence to prediction.\n","##########################################################################\n","\n","##########################################################################\n","\n","Time to process claim: 20.49 seconds.\n","------------------------------------------------\n","\n","\n","--- Processing Claim ID: 1933 ---\n","DEBUG 1.0 (query_generator):\n","\tClaim: Dissociative identity disorder is known as multiple personality disorder.\n","\tEntities: []\n","\tLLM Output: ['dissociative_identity_disorder', 'multiple_personality_disorder', 'mental_disorder', 'psychological_conditions', 'dissociation']\n","-_--_--_--_--_-\n","DEBUG 1.1 (query_generator):\n","\tEntities: []\n","\tGenerated Potential Titles: ['dissociation', 'mental_disorder', 'psychological_conditions', 'multiple_personality_disorder', 'dissociative_identity_disorder']\n","\tSelected Titles for Retrieval: ['dissociation', 'mental_disorder', 'psychological_conditions', 'multiple_personality_disorder', 'dissociative_identity_disorder']\n","-_--_--_--_--_--_--_--_--_--_-\n","###### M1: Retrieving Documents ######\n","DEBUG 1.2: Successfully retrieved intro from 'Dissociative identity disorder' (searched for 'dissociation')\n","DEBUG 1.2: Successfully retrieved intro from 'Dissociative identity disorder' (searched for 'mental_disorder')\n","DEBUG 1.2: Successfully retrieved intro from 'Dissociative identity disorder' (searched for 'psychological_conditions')\n","DEBUG 1.2: Successfully retrieved intro from 'Dissociative identity disorder' (searched for 'multiple_personality_disorder')\n","DEBUG 1.2: Successfully retrieved intro from 'Dissociative identity disorder' (searched for 'dissociative_identity_disorder')\n","DEBUG 1.2: Retrieved content for 5 pages out of 5 potential titles.\n","-_--_--_--_--_--_--_--_--_--_-\n","-------------------------------------------------------------------\n","\n","###### M2: Starting Evidence Extraction ######\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Dissociative identity disorder is known as multiple personality disorder.\n","\tRephrased Claim: Dissociative identity disorder is referred to as multiple personality disorder.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Dissociative identity disorder is known as multiple personality disorder.\n","\tRephrased Claim: Dissociative identity disorder is also referred to as multiple personality disorder.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Dissociative identity disorder is known as multiple personality disorder.\n","\tRephrased Claim: Dissociative identity disorder is referred to as multiple personality disorder.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Dissociative identity disorder is known as multiple personality disorder.\n","\tRephrased Claim: Dissociative identity disorder is also referred to as multiple personality disorder.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Dissociative identity disorder is known as multiple personality disorder.\n","\tRephrased Claim: Dissociative identity disorder is referred to as multiple personality disorder.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Dissociative identity disorder is known as multiple personality disorder.\n","\tRephrased Claim: Dissociative identity disorder is referred to as multiple personality disorder.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Dissociative identity disorder is known as multiple personality disorder.\n","\tRephrased Claim: Dissociative identity disorder is also referred to as multiple personality disorder.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Dissociative identity disorder is known as multiple personality disorder.\n","\tRephrased Claim: Dissociative identity disorder is referred to as multiple personality disorder.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Dissociative identity disorder is known as multiple personality disorder.\n","\tRephrased Claim: Dissociative identity disorder is also referred to as multiple personality disorder.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Dissociative identity disorder is known as multiple personality disorder.\n","\tRephrased Claim: Dissociative identity disorder is also referred to as multiple personality disorder.\n","-_--_--_--_--_-\n","DEBUG 2.1 (module_2_controls):\n","\tClaim: Dissociative identity disorder is known as multiple personality disorder.\n","\tEntities: []\n","\tKeywords: ['disorder', 'personality', 'multiple', 'known', 'identity']\n","\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n","\tMax Evidence Target: 8, Max Iterations: 10\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 15\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 1: Sending 15 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","Dissociative identity disorder (DID), previously known as multiple personality disorder (MPD), is characterized by the presence of at least two personality states or \"alters\".  \n","Dissociative identity disorder (DID), previously known as multiple personality disorder (MPD), is characterized by the presence of at least two personality states or \"alters\".  \n","Dissociative identity disorder (DID), previously known as multiple personality disorder (MPD), is characterized by the presence of at least two personality states or \"alters\".  \n","Dissociative identity disorder (DID), previously known as multiple personality disorder (MPD), is characterized by the presence of at least two personality states or \"alters\".  \n","Dissociative identity disorder (DID), previously known as multiple personality disorder (MPD), is characterized by the presence of at least two personality states or \"alters\".\n","-_--_--_--_--_-\n","M2 Iter 1: LLM selected 5 sentences.\n","   Warning: Could not re-associate LLM output with retrieved data: 'Dissociative identity disorder (DID), previously known as multiple personality d...'\n","   Warning: Could not re-associate LLM output with retrieved data: 'Dissociative identity disorder (DID), previously known as multiple personality d...'\n","   Warning: Could not re-associate LLM output with retrieved data: 'Dissociative identity disorder (DID), previously known as multiple personality d...'\n","   Warning: Could not re-associate LLM output with retrieved data: 'Dissociative identity disorder (DID), previously known as multiple personality d...'\n","M2 Iter 1: LLM selected few items (1). Lowering sBERT threshold to 0.350\n","DEBUG 2. End Iter 1: Total evidence found: 1\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 20\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 2: Sending 15 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 2: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","   Lowering threshold to 0.300 for next try.\n","DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 45\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 3: Sending 40 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 3: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","M2 Iter 3: Stopping because LLM found no evidence after two tries.\n","M2: Finished. Selected 1 evidence items.\n","-------------------------------------------------------------------\n","\n","###### M3: Starting Classification ######\n","DEBUG 3.1 (module_3_classification):\n","\tClaim: Dissociative identity disorder is known as multiple personality disorder.\n","\tEvidence Texts Sent (1):\n","\tPrompt (partial): Based ONLY on the following evidence sentences, classify the claim as SUPPORTS, REFUTES, or NOT ENOUGH INFO.\n","\n","Claim: 'Dissociative identity disorder is known as multiple personality disorder.'\n","\n","Eviden...\n","-_--_--_--_--_--_--_--_--_--_-\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing Claims:  83%|████████▎ | 25/30 [09:36<02:20, 28.09s/it]"]},{"output_type":"stream","name":"stdout","text":["DEBUG 3.2/3.3 (module_3_classification):\n","\tLLM Classification Result: SUPPORTS\n","\tExit Status: OK\n","-_--_--_--_--_--_--_--_--_--_-\n","Adding gold label/evidence to prediction.\n","##########################################################################\n","\n","##########################################################################\n","\n","Time to process claim: 34.72 seconds.\n","------------------------------------------------\n","\n","\n","--- Processing Claim ID: 88894 ---\n","DEBUG 1.0 (query_generator):\n","\tClaim: Zoe Saldana is a Leo.\n","\tEntities: ['Saldana', 'Zoe']\n","\tLLM Output: ['zoe_saldana', 'leo_(astrology)', 'zodiac_signs', 'astrological_signs']\n","-_--_--_--_--_-\n","DEBUG 1.1 (query_generator):\n","\tEntities: ['Saldana', 'Zoe']\n","\tGenerated Potential Titles: ['zoe_saldana', 'zodiac_signs', 'leo_(astrology)', 'astrological_signs']\n","\tSelected Titles for Retrieval: ['zoe_saldana', 'zodiac_signs', 'leo_(astrology)', 'astrological_signs']\n","-_--_--_--_--_--_--_--_--_--_-\n","###### M1: Retrieving Documents ######\n","DEBUG 1.2: Successfully retrieved intro from 'Zoe Saldaña' (searched for 'zoe_saldana')\n","DEBUG 1.2: Successfully retrieved intro from 'Leo (astrology)' (searched for 'zodiac_signs')\n","DEBUG 1.2: DisambiguationError for title: 'leo_(astrology)'. Options: ['Leo (constellation)', 'Leo (astrology)', 'Lateral epitaxial overgrowth', 'Law enforcement officer', 'Law enforcement organisation', 'Louisville Eccentric Observer', 'Michigan Department of Labor and Economic Opportunity', 'L.E.O. (band)', 'Leo (soundtrack)', 'Leo (2000 film)', 'Leo (2002 film)', 'Josef Fares', 'Leo (2012 film)', 'Leo (2023 American film)', 'Leo (2023 Indian film)', 'Leo the Lion (MGM)', 'Leo Awards', 'episode of Being Erica', 'Animal Crackers', 'Fabien Cloutier', 'Leo Namibia', 'Leo Pharma', 'Leo Records', 'Lioré et Olivier', 'The Leo Group', 'Leo (given name)', 'Léo', 'Leo (surname)', 'Leonid dynasty', 'Arakel Babakhanian', 'Leo (singer)', 'Léo (footballer, born 1975)', 'Leo (footballer, born November 1989)', 'Leo (footballer, born December 1989)', 'Léo (footballer, born 1990)', 'Léo (footballer, born 1992)', 'Leo (wrestler)', 'Luiz Eduardo de Oliveira', 'Léo Department', 'Léo, Burkina Faso', 'Leo-Cedarville, Indiana', 'Leo, Ohio', 'Leo, West Virginia', 'Leo Islands', 'Low Earth orbit', 'Launch and Early Orbit phase', 'LEO (spacecraft)', 'Leo (horse)', 'Leonberger', 'Long-term Ecosystem Observatory', 'Panthera', 'LEO (computer)', 'Leo (text editor)', 'BC Lions', 'The PBA Leo Awards', 'LEO (website)', 'Leo Petroglyph', 'Leo clubs', 'Leotard', 'Leos (disambiguation)', 'Lio (disambiguation)', 'St. Leo (disambiguation)']...\n","DEBUG 1.2: Successfully retrieved intro from 'Leo (astrology)' (disambiguated to 'Leo (astrology)')\n","DEBUG 1.2: Successfully retrieved intro from 'Zodiac' (searched for 'astrological_signs')\n","DEBUG 1.2: Retrieved content for 4 pages out of 4 potential titles.\n","-_--_--_--_--_--_--_--_--_--_-\n","-------------------------------------------------------------------\n","\n","###### M2: Starting Evidence Extraction ######\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Zoe Saldana is a Leo.\n","\tRephrased Claim: Zoe Saldana belongs to the Leo zodiac sign.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Zoe Saldana is a Leo.\n","\tRephrased Claim: Zoe Saldana was born under the Leo zodiac sign.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Zoe Saldana is a Leo.\n","\tRephrased Claim: Zoe Saldana belongs to the Leo zodiac sign.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Zoe Saldana is a Leo.\n","\tRephrased Claim: Zoe Saldana is born under the Leo zodiac sign.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Zoe Saldana is a Leo.\n","\tRephrased Claim: Zoe Saldana was born under the sign of Leo.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Zoe Saldana is a Leo.\n","\tRephrased Claim: Zoe Saldana belongs to the zodiac sign Leo.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Zoe Saldana is a Leo.\n","\tRephrased Claim: Zoe Saldana was born under the sign of Leo.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Zoe Saldana is a Leo.\n","\tRephrased Claim: Zoe Saldana was born under the sign of Leo.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Zoe Saldana is a Leo.\n","\tRephrased Claim: Zoe Saldana belongs to the zodiac sign Leo.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Zoe Saldana is a Leo.\n","\tRephrased Claim: Zoe Saldana belongs to the Leo zodiac sign.\n","-_--_--_--_--_-\n","DEBUG 2.1 (module_2_controls):\n","\tClaim: Zoe Saldana is a Leo.\n","\tEntities: ['Saldana', 'Zoe']\n","\tKeywords: ['zoe', 'saldana', 'leo']\n","\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n","\tMax Evidence Target: 8, Max Iterations: 10\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 11\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 1: Sending 11 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 1: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","   Lowering threshold to 0.350 for next try.\n","DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 22\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 2: Sending 22 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 2: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","   Lowering threshold to 0.300 for next try.\n","DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 25\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 3: Sending 25 new candidates to LLM for selection.\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing Claims:  87%|████████▋ | 26/30 [09:54<01:39, 24.96s/it]"]},{"output_type":"stream","name":"stdout","text":["DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 3: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","M2 Iter 3: Stopping because LLM found no evidence after two tries.\n","M2: Finished iterations. No evidence selected.\n","-------------------------------------------------------------------\n","\n","###### M3: Starting Classification ######\n","M3: No evidence text provided. Classifying as NOT ENOUGH INFO.\n","Adding gold label/evidence to prediction.\n","##########################################################################\n","\n","##########################################################################\n","\n","Time to process claim: 17.64 seconds.\n","------------------------------------------------\n","\n","\n","--- Processing Claim ID: 17915 ---\n","DEBUG 1.0 (query_generator):\n","\tClaim: Fred Seibert has produced comedy programs.\n","\tEntities: ['Seibert', 'Fred']\n","\tLLM Output: ['fred_seibert', 'comedy_television', 'animated_television_series', 'list_of_animated_television_series', 'nickelodeon', 'cartoon_network']\n","-_--_--_--_--_-\n","DEBUG 1.1 (query_generator):\n","\tEntities: ['Seibert', 'Fred']\n","\tGenerated Potential Titles: ['nickelodeon', 'fred_seibert', 'cartoon_network', 'comedy_television', 'animated_television_series', 'list_of_animated_television_series']\n","\tSelected Titles for Retrieval: ['nickelodeon', 'fred_seibert', 'cartoon_network', 'comedy_television', 'animated_television_series', 'list_of_animated_television_series']\n","-_--_--_--_--_--_--_--_--_--_-\n","###### M1: Retrieving Documents ######\n","DEBUG 1.2: Successfully retrieved intro from 'List of Nickelodeon Animation Studio productions' (searched for 'nickelodeon')\n","DEBUG 1.2: Successfully retrieved intro from 'Fred Seibert' (searched for 'fred_seibert')\n","DEBUG 1.2: Successfully retrieved intro from 'List of programs broadcast by Cartoon Network' (searched for 'cartoon_network')\n","DEBUG 1.2: Successfully retrieved intro from 'List of comedy television series' (searched for 'comedy_television')\n","DEBUG 1.2: Successfully retrieved intro from 'List of computer-animated television series' (searched for 'animated_television_series')\n","DEBUG 1.2: Successfully retrieved intro from 'Lists of animated television series' (searched for 'list_of_animated_television_series')\n","DEBUG 1.2: Retrieved content for 6 pages out of 6 potential titles.\n","-_--_--_--_--_--_--_--_--_--_-\n","-------------------------------------------------------------------\n","\n","###### M2: Starting Evidence Extraction ######\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Fred Seibert has produced comedy programs.\n","\tRephrased Claim: Fred Seibert has created comedy shows.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Fred Seibert has produced comedy programs.\n","\tRephrased Claim: Fred Seibert has created comedy shows.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Fred Seibert has produced comedy programs.\n","\tRephrased Claim: Fred Seibert has created comedy shows.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Fred Seibert has produced comedy programs.\n","\tRephrased Claim: Fred Seibert has been involved in the production of comedic shows.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Fred Seibert has produced comedy programs.\n","\tRephrased Claim: Fred Seibert has created comedy shows.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Fred Seibert has produced comedy programs.\n","\tRephrased Claim: Fred Seibert has created comedic television shows.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Fred Seibert has produced comedy programs.\n","\tRephrased Claim: Fred Seibert has created comedy shows.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Fred Seibert has produced comedy programs.\n","\tRephrased Claim: Fred Seibert has created comedy shows.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Fred Seibert has produced comedy programs.\n","\tRephrased Claim: Fred Seibert has created comedy shows.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Fred Seibert has produced comedy programs.\n","\tRephrased Claim: Fred Seibert has created comedy shows.\n","-_--_--_--_--_-\n","DEBUG 2.1 (module_2_controls):\n","\tClaim: Fred Seibert has produced comedy programs.\n","\tEntities: ['Seibert', 'Fred']\n","\tKeywords: ['seibert', 'programs', 'produced', 'fred', 'comedy']\n","\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n","\tMax Evidence Target: 8, Max Iterations: 10\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 6\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 1: Sending 6 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","6. He created the animation incubator anthology series What a Cartoon!\n","-_--_--_--_--_-\n","M2 Iter 1: LLM selected 1 sentences.\n","M2 Iter 1: LLM selected few items (1). Lowering sBERT threshold to 0.350\n","DEBUG 2. End Iter 1: Total evidence found: 1\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 21\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 2: Sending 20 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","6. He was a seed investor for the website Tumblr, and has executive produced various animated and live-action series.\n","9. He founded the production company Frederator Studios in 1998, as well as its spin-off entities Frederator Networks, Channel Frederator Network, and Cartoon Hangover.\n","13. Cartoons in 2008; all three have spawned successful television programs as spin-offs, including The Fairly OddParents, Johnny Bravo, Dexter's Laboratory, Courage the Cowardly Dog, My Life as a Teenage Robot, The Powerpuff Girls, and Adventure Time—for most of which he has served as executive producer.\n","-_--_--_--_--_-\n","M2 Iter 2: LLM selected 3 sentences.\n","M2 Iter 2: LLM selected few items (3). Lowering sBERT threshold to 0.300\n","DEBUG 2. End Iter 2: Total evidence found: 4\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 22\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 3: Sending 18 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","8. He has since co-founded Next New Networks, Bolder Media, and launched the production company FredFilms.  \n","15. Cartoon Network's first original series was The Moxy Show and the late-night satirical animated talk show Space Ghost Coast to Coast (the latter moving to Adult Swim at launch on September 2, 2001).  \n","16. The network was launched on October 1, 1992, and airs mainly animated programming, ranging from action to animated comedy.\n","-_--_--_--_--_-\n","M2 Iter 3: LLM selected 3 sentences.\n","M2 Iter 3: LLM selected few items (3). Lowering sBERT threshold to 0.250\n","DEBUG 2. End Iter 3: Total evidence found: 7\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 4/10, Current sBERT Thresh: 0.250\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 25\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 4: Sending 18 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 4: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","M2 Iter 4: Stopping because LLM found no evidence after two tries.\n","M2: Finished. Selected 7 evidence items.\n","-------------------------------------------------------------------\n","\n","###### M3: Starting Classification ######\n","DEBUG 3.1 (module_3_classification):\n","\tClaim: Fred Seibert has produced comedy programs.\n","\tEvidence Texts Sent (7):\n","\tPrompt (partial): Based ONLY on the following evidence sentences, classify the claim as SUPPORTS, REFUTES, or NOT ENOUGH INFO.\n","\n","Claim: 'Fred Seibert has produced comedy programs.'\n","\n","Evidence:\n","- He was a seed investor fo...\n","-_--_--_--_--_--_--_--_--_--_-\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing Claims:  90%|█████████ | 27/30 [10:16<01:12, 24.01s/it]"]},{"output_type":"stream","name":"stdout","text":["DEBUG 3.2/3.3 (module_3_classification):\n","\tLLM Classification Result: SUPPORTS\n","\tExit Status: OK\n","-_--_--_--_--_--_--_--_--_--_-\n","Adding gold label/evidence to prediction.\n","##########################################################################\n","\n","##########################################################################\n","\n","Time to process claim: 21.80 seconds.\n","------------------------------------------------\n","\n","\n","--- Processing Claim ID: 58396 ---\n","DEBUG 1.0 (query_generator):\n","\tClaim: Konidela Production Company was established.\n","\tEntities: ['Production Company', 'Konidela']\n","\tLLM Output: ['konidela_production_company', 'film_production_company']\n","-_--_--_--_--_-\n","DEBUG 1.1 (query_generator):\n","\tEntities: ['Production Company', 'Konidela']\n","\tGenerated Potential Titles: ['film_production_company', 'konidela_production_company']\n","\tSelected Titles for Retrieval: ['film_production_company', 'konidela_production_company']\n","-_--_--_--_--_--_--_--_--_--_-\n","###### M1: Retrieving Documents ######\n","DEBUG 1.2: Successfully retrieved intro from 'Konidela Production Company' (searched for 'film_production_company')\n","DEBUG 1.2: Successfully retrieved intro from 'Konidela Production Company' (searched for 'konidela_production_company')\n","DEBUG 1.2: Retrieved content for 2 pages out of 2 potential titles.\n","-_--_--_--_--_--_--_--_--_--_-\n","-------------------------------------------------------------------\n","\n","###### M2: Starting Evidence Extraction ######\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Konidela Production Company was established.\n","\tRephrased Claim: The Konidela Production Company was founded.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Konidela Production Company was established.\n","\tRephrased Claim: The Konidela Production Company was founded.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Konidela Production Company was established.\n","\tRephrased Claim: The Konidela Production Company was founded.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Konidela Production Company was established.\n","\tRephrased Claim: The Konidela Production Company was founded.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Konidela Production Company was established.\n","\tRephrased Claim: The Konidela Production Company was founded.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Konidela Production Company was established.\n","\tRephrased Claim: The Konidela Production Company was founded.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Konidela Production Company was established.\n","\tRephrased Claim: The Konidela Production Company was founded.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Konidela Production Company was established.\n","\tRephrased Claim: The Konidela Production Company was founded.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Konidela Production Company was established.\n","\tRephrased Claim: The Konidela Production Company was founded.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Konidela Production Company was established.\n","\tRephrased Claim: The Konidela Production Company was founded.\n","-_--_--_--_--_-\n","DEBUG 2.1 (module_2_controls):\n","\tClaim: Konidela Production Company was established.\n","\tEntities: ['Production Company', 'Konidela']\n","\tKeywords: ['production', 'konidela', 'established', 'company']\n","\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n","\tMax Evidence Target: 8, Max Iterations: 10\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 2\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 1: Sending 2 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","1. Konidela Production Company is an Indian film production company established by actor Ram Charan.\n","2. Konidela Production Company is an Indian film production company established by actor Ram Charan.\n","-_--_--_--_--_-\n","M2 Iter 1: LLM selected 2 sentences.\n","   Warning: Could not re-associate LLM output with retrieved data: 'Konidela Production Company is an Indian film production company established by ...'\n","DEBUG 2. End Iter 1: Total evidence found: 1\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.400\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 2\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 2: No new candidates found by sBERT at threshold 0.400.\n","   Lowering threshold to 0.300 for next try.\n","DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 2\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 3: No new candidates found by sBERT at threshold 0.300.\n","   Lowering threshold to 0.200 for next try.\n","DEBUG 2. Iteration 4/10, Current sBERT Thresh: 0.200\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 2\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 4: No new candidates found by sBERT at threshold 0.200.\n","   Lowering threshold to 0.100 for next try.\n","DEBUG 2. Iteration 5/10, Current sBERT Thresh: 0.100\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 2\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 5: No new candidates found by sBERT at threshold 0.100.\n","   Lowering threshold to 0.100 for next try.\n","DEBUG 2. Iteration 6/10, Current sBERT Thresh: 0.100\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 2\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 6: No new candidates found by sBERT at threshold 0.100.\n","M2 Iter 6: No new candidates and threshold at minimum (0.100). Stopping.\n","M2: Finished. Selected 1 evidence items.\n","-------------------------------------------------------------------\n","\n","###### M3: Starting Classification ######\n","DEBUG 3.1 (module_3_classification):\n","\tClaim: Konidela Production Company was established.\n","\tEvidence Texts Sent (1):\n","\tPrompt (partial): Based ONLY on the following evidence sentences, classify the claim as SUPPORTS, REFUTES, or NOT ENOUGH INFO.\n","\n","Claim: 'Konidela Production Company was established.'\n","\n","Evidence:\n","- Konidela Production Com...\n","-_--_--_--_--_--_--_--_--_--_-\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing Claims:  93%|█████████▎| 28/30 [10:26<00:39, 19.94s/it]"]},{"output_type":"stream","name":"stdout","text":["DEBUG 3.2/3.3 (module_3_classification):\n","\tLLM Classification Result: SUPPORTS\n","\tExit Status: OK\n","-_--_--_--_--_--_--_--_--_--_-\n","Adding gold label/evidence to prediction.\n","##########################################################################\n","\n","##########################################################################\n","\n","Time to process claim: 10.45 seconds.\n","------------------------------------------------\n","\n","\n","--- Processing Claim ID: 150751 ---\n","DEBUG 1.0 (query_generator):\n","\tClaim: Paul von Hindenburg was a man.\n","\tEntities: ['Paul']\n","\tLLM Output: ['paul_von_hindenburg', 'hindenburg', 'german_field_marshals', 'presidents_of_germany']\n","-_--_--_--_--_-\n","DEBUG 1.1 (query_generator):\n","\tEntities: ['Paul']\n","\tGenerated Potential Titles: ['hindenburg', 'paul_von_hindenburg', 'presidents_of_germany', 'german_field_marshals']\n","\tSelected Titles for Retrieval: ['hindenburg', 'paul_von_hindenburg', 'presidents_of_germany', 'german_field_marshals']\n","-_--_--_--_--_--_--_--_--_--_-\n","###### M1: Retrieving Documents ######\n","DEBUG 1.2: Successfully retrieved intro from 'Paul von Hindenburg' (searched for 'hindenburg')\n","DEBUG 1.2: Successfully retrieved intro from 'Paul von Hindenburg' (searched for 'paul_von_hindenburg')\n","DEBUG 1.2: Successfully retrieved intro from 'President of Germany' (searched for 'presidents_of_germany')\n","DEBUG 1.2: Successfully retrieved intro from 'Alfred von Waldersee' (searched for 'german_field_marshals')\n","DEBUG 1.2: Retrieved content for 4 pages out of 4 potential titles.\n","-_--_--_--_--_--_--_--_--_--_-\n","-------------------------------------------------------------------\n","\n","###### M2: Starting Evidence Extraction ######\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Paul von Hindenburg was a man.\n","\tRephrased Claim: Paul von Hindenburg was an individual.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Paul von Hindenburg was a man.\n","\tRephrased Claim: Paul von Hindenburg was an individual.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Paul von Hindenburg was a man.\n","\tRephrased Claim: Paul von Hindenburg was an individual.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Paul von Hindenburg was a man.\n","\tRephrased Claim: Paul von Hindenburg was a human being.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Paul von Hindenburg was a man.\n","\tRephrased Claim: Paul von Hindenburg was an individual.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Paul von Hindenburg was a man.\n","\tRephrased Claim: Paul von Hindenburg was an individual.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Paul von Hindenburg was a man.\n","\tRephrased Claim: Paul von Hindenburg was an individual.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Paul von Hindenburg was a man.\n","\tRephrased Claim: Paul von Hindenburg was an individual.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Paul von Hindenburg was a man.\n","\tRephrased Claim: Paul von Hindenburg was an individual.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Paul von Hindenburg was a man.\n","\tRephrased Claim: Paul von Hindenburg was an individual.\n","-_--_--_--_--_-\n","DEBUG 2.1 (module_2_controls):\n","\tClaim: Paul von Hindenburg was a man.\n","\tEntities: ['Paul']\n","\tKeywords: ['von', 'paul', 'man', 'hindenburg']\n","\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n","\tMax Evidence Target: 8, Max Iterations: 10\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 18\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 1: Sending 18 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","3. Paul Ludwig Hans Anton von Beneckendorff und von Hindenburg (2 October 1847 – 2 August 1934) was a Prussian-born German military officer and politician who led the Imperial German Army during the First World War and later became president of Germany from 1925 until his death in 1934.\n","4. Paul Ludwig Hans Anton von Beneckendorff und von Hindenburg (2 October 1847 – 2 August 1934) was a Prussian-born German military officer and politician who led the Imperial German Army during the First World War and later became president of Germany from 1925 until his death in 1934.\n","5. Opposed to Hitler and his Nazi Party, Hindenburg nonetheless played a major role in the instability that resulted in their rise to power.\n","6. Opposed to Hitler and his Nazi Party, Hindenburg nonetheless played a major role in the instability that resulted in their rise to power.\n","7. Hindenburg was born to a family of minor Prussian nobility in the Grand Duchy of Posen.\n","8. Hindenburg was born to a family of minor Prussian nobility in the Grand Duchy of Posen.\n","9. After teaching at the War Academy, Hindenburg rose to become a lieutenant general by 1900.\n","10. After teaching at the War Academy, Hindenburg rose to become a lieutenant general by 1900.\n","11. After Hindenburg died the following year, Hitler combined the presidency with the chancellery before declaring himself Führer (lit.\n","12. After Hindenburg died the following year, Hitler combined the presidency with the chancellery before declaring himself Führer (lit.\n","13. After World War I began in 1914, Hindenburg was recalled and achieved fame on the Eastern Front as the victor of Tannenberg.\n","14. After World War I began in 1914, Hindenburg was recalled and achieved fame on the Eastern Front as the victor of Tannenberg.\n","15. Following the armistice, Hindenburg stepped down as the German Army's Chief of Staff before retiring again in 1919.\n","16. Following the armistice, Hindenburg stepped down as the German Army's Chief of Staff before retiring again in 1919.\n","17. In 1925, Hindenburg returned to public life to become the second elected president of the Weimar Republic.\n","18. In 1925, Hindenburg returned to public life\n","-_--_--_--_--_-\n","M2 Iter 1: LLM selected 16 sentences.\n","   Warning: Could not re-associate LLM output with retrieved data: 'Paul Ludwig Hans Anton von Beneckendorff und von Hindenburg (2 October 1847 – 2 ...'\n","   Warning: Could not re-associate LLM output with retrieved data: 'Opposed to Hitler and his Nazi Party, Hindenburg nonetheless played a major role...'\n","   Warning: Could not re-associate LLM output with retrieved data: 'Hindenburg was born to a family of minor Prussian nobility in the Grand Duchy of...'\n","   Warning: Could not re-associate LLM output with retrieved data: 'After teaching at the War Academy, Hindenburg rose to become a lieutenant genera...'\n","   Warning: Could not re-associate LLM output with retrieved data: 'After Hindenburg died the following year, Hitler combined the presidency with th...'\n","   Warning: Could not re-associate LLM output with retrieved data: 'After World War I began in 1914, Hindenburg was recalled and achieved fame on th...'\n","   Warning: Could not re-associate LLM output with retrieved data: 'Following the armistice, Hindenburg stepped down as the German Army's Chief of S...'\n","DEBUG 2. End Iter 1: Total evidence found: 8\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 1: Reached target evidence count (8).\n","M2: Finished. Selected 8 evidence items.\n","-------------------------------------------------------------------\n","\n","###### M3: Starting Classification ######\n","DEBUG 3.1 (module_3_classification):\n","\tClaim: Paul von Hindenburg was a man.\n","\tEvidence Texts Sent (8):\n","\tPrompt (partial): Based ONLY on the following evidence sentences, classify the claim as SUPPORTS, REFUTES, or NOT ENOUGH INFO.\n","\n","Claim: 'Paul von Hindenburg was a man.'\n","\n","Evidence:\n","- In 1925, Hindenburg returned to publi...\n","-_--_--_--_--_--_--_--_--_--_-\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing Claims:  97%|█████████▋| 29/30 [10:46<00:19, 19.73s/it]"]},{"output_type":"stream","name":"stdout","text":["DEBUG 3.2/3.3 (module_3_classification):\n","\tLLM Classification Result: SUPPORTS\n","\tExit Status: OK\n","-_--_--_--_--_--_--_--_--_--_-\n","Adding gold label/evidence to prediction.\n","##########################################################################\n","\n","##########################################################################\n","\n","Time to process claim: 19.23 seconds.\n","------------------------------------------------\n","\n","\n","--- Processing Claim ID: 179831 ---\n","DEBUG 1.0 (query_generator):\n","\tClaim: Vic Mensa was born June 12, 1993.\n","\tEntities: ['Mensa', 'Vic']\n","\tLLM Output: ['vic_mensa', 'vic_mensa_discography', 'vic_mensa_discography', 'vic_mensa_personal_life']\n","-_--_--_--_--_-\n","DEBUG 1.1 (query_generator):\n","\tEntities: ['Mensa', 'Vic']\n","\tGenerated Potential Titles: ['vic_mensa', 'vic_mensa_discography', 'vic_mensa_personal_life']\n","\tSelected Titles for Retrieval: ['vic_mensa', 'vic_mensa_discography', 'vic_mensa_personal_life']\n","-_--_--_--_--_--_--_--_--_--_-\n","###### M1: Retrieving Documents ######\n","DEBUG 1.2: Successfully retrieved intro from 'Vic Mensa' (searched for 'vic_mensa')\n","DEBUG 1.2: Successfully retrieved intro from 'Vic Mensa' (searched for 'vic_mensa_discography')\n","DEBUG 1.2: Successfully retrieved intro from 'Nile Rodgers' (searched for 'vic_mensa_personal_life')\n","DEBUG 1.2: Retrieved content for 3 pages out of 3 potential titles.\n","-_--_--_--_--_--_--_--_--_--_-\n","-------------------------------------------------------------------\n","\n","###### M2: Starting Evidence Extraction ######\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Vic Mensa was born June 12, 1993.\n","\tRephrased Claim: Vic Mensa was born on June 12, 1993.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Vic Mensa was born June 12, 1993.\n","\tRephrased Claim: Vic Mensa was born on June 12, 1993.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Vic Mensa was born June 12, 1993.\n","\tRephrased Claim: Vic Mensa was born on June 12, 1993.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Vic Mensa was born June 12, 1993.\n","\tRephrased Claim: Vic Mensa was born on June 12, 1993.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Vic Mensa was born June 12, 1993.\n","\tRephrased Claim: Vic Mensa's birth date is June 12, 1993.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Vic Mensa was born June 12, 1993.\n","\tRephrased Claim: Vic Mensa's birth date is June 12, 1993.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Vic Mensa was born June 12, 1993.\n","\tRephrased Claim: Vic Mensa was born on June 12, 1993.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Vic Mensa was born June 12, 1993.\n","\tRephrased Claim: Vic Mensa was born on June 12, 1993.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Vic Mensa was born June 12, 1993.\n","\tRephrased Claim: Vic Mensa's birth date is June 12, 1993.\n","-_--_--_--_--_-\n","DEBUG 1.3 (rephrase_claim):\n","\tOriginal Claim: Vic Mensa was born June 12, 1993.\n","\tRephrased Claim: Vic Mensa was born on June 12, 1993.\n","-_--_--_--_--_-\n","DEBUG 2.1 (module_2_controls):\n","\tClaim: Vic Mensa was born June 12, 1993.\n","\tEntities: ['Mensa', 'Vic']\n","\tKeywords: ['vic', 'mensa', 'june', 'born', '1993']\n","\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n","\tMax Evidence Target: 8, Max Iterations: 10\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 6\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 1: Sending 6 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","1. Victor Kwesi Mensah (born June 6, 1993), known professionally as Vic Mensa, is an American rapper.\n","2. Victor Kwesi Mensah (born June 6, 1993), known professionally as Vic Mensa, is an American rapper.\n","-_--_--_--_--_-\n","M2 Iter 1: LLM selected 2 sentences.\n","   Warning: Could not re-associate LLM output with retrieved data: 'Victor Kwesi Mensah (born June 6, 1993), known professionally as Vic Mensa, is a...'\n","M2 Iter 1: LLM selected few items (1). Lowering sBERT threshold to 0.350\n","DEBUG 2. End Iter 1: Total evidence found: 1\n","-_--_--_--_--_--_--_--_--_--_-\n","DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 6\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 2: Sending 4 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 2: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","   Lowering threshold to 0.300 for next try.\n","DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n","DEBUG 2.2.2 (sbert_filter):\n","\tTotal candidates found across all docs: 6\n","-_--_--_--_--_--_--_--_--_--_-\n","M2 Iter 3: Sending 4 new candidates to LLM for selection.\n","DEBUG 2.3.3 (LLM Selection):\n","\tLLM Raw Output:\n","NOT ENOUGH INFO\n","-_--_--_--_--_-\n","M2 Iter 3: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n","M2 Iter 3: Stopping because LLM found no evidence after two tries.\n","M2: Finished. Selected 1 evidence items.\n","-------------------------------------------------------------------\n","\n","###### M3: Starting Classification ######\n","DEBUG 3.1 (module_3_classification):\n","\tClaim: Vic Mensa was born June 12, 1993.\n","\tEvidence Texts Sent (1):\n","\tPrompt (partial): Based ONLY on the following evidence sentences, classify the claim as SUPPORTS, REFUTES, or NOT ENOUGH INFO.\n","\n","Claim: 'Vic Mensa was born June 12, 1993.'\n","\n","Evidence:\n","- Victor Kwesi Mensah (born June 6, ...\n","-_--_--_--_--_--_--_--_--_--_-\n"]},{"output_type":"stream","name":"stderr","text":["Processing Claims: 100%|██████████| 30/30 [11:02<00:00, 22.07s/it]"]},{"output_type":"stream","name":"stdout","text":["DEBUG 3.2/3.3 (module_3_classification):\n","\tLLM Classification Result: REFUTES\n","\tExit Status: OK\n","-_--_--_--_--_--_--_--_--_--_-\n","Adding gold label/evidence to prediction.\n","##########################################################################\n","\n","##########################################################################\n","\n","Time to process claim: 16.03 seconds.\n","------------------------------------------------\n","\n","\n","Finished processing 30 claims.\n","\n","--- FEVER Scoring Results ---\n","Strict Score (Exact Match): 36.67%\n","Label Accuracy: 56.67%\n","Evidence Precision: 52.67%\n","Evidence Recall: 40.00%\n","Evidence F1 Score: 45.47%\n","Number of test cases scored: 30\n","\n","--- Sample Predictions (Output Format) ---\n","{\n","  \"id\": 113501,\n","  \"predicted_label\": \"SUPPORTS\",\n","  \"predicted_evidence\": [\n","    [\n","      \"Grease_2\",\n","      4\n","    ]\n","  ],\n","  \"label\": \"NOT ENOUGH INFO\",\n","  \"evidence\": []\n","}\n","{\n","  \"id\": 163803,\n","  \"predicted_label\": \"SUPPORTS\",\n","  \"predicted_evidence\": [\n","    [\n","      \"Ukrainian_Soviet_Socialist_Republic\",\n","      6\n","    ],\n","    [\n","      \"Ukrainian_Soviet_Socialist_Republic\",\n","      0\n","    ],\n","    [\n","      \"Ukrainian_Soviet_Socialist_Republic\",\n","      5\n","    ]\n","  ],\n","  \"label\": \"SUPPORTS\",\n","  \"evidence\": [\n","    [\n","      [\n","        null,\n","        null,\n","        \"Ukrainian_Soviet_Socialist_Republic\",\n","        7\n","      ]\n","    ],\n","    [\n","      [\n","        null,\n","        null,\n","        \"United_Nations\",\n","        0\n","      ],\n","      [\n","        null,\n","        null,\n","        \"Ukrainian_Soviet_Socialist_Republic\",\n","        7\n","      ]\n","    ]\n","  ]\n","}\n","{\n","  \"id\": 70041,\n","  \"predicted_label\": \"NOT ENOUGH INFO\",\n","  \"predicted_evidence\": [],\n","  \"label\": \"SUPPORTS\",\n","  \"evidence\": [\n","    [\n","      [\n","        null,\n","        null,\n","        \"2_Hearts_-LRB-Kylie_Minogue_song-RRB-\",\n","        0\n","      ]\n","    ]\n","  ]\n","}\n","{\n","  \"id\": 202314,\n","  \"predicted_label\": \"NOT ENOUGH INFO\",\n","  \"predicted_evidence\": [],\n","  \"label\": \"REFUTES\",\n","  \"evidence\": [\n","    [\n","      [\n","        null,\n","        null,\n","        \"New_Jersey_Turnpike\",\n","        15\n","      ]\n","    ]\n","  ]\n","}\n","{\n","  \"id\": 57085,\n","  \"predicted_label\": \"REFUTES\",\n","  \"predicted_evidence\": [\n","    [\n","      \"Legendary_Entertainment\",\n","      3\n","    ],\n","    [\n","      \"Legendary_Entertainment\",\n","      2\n","    ],\n","    [\n","      \"Wanda_Film\",\n","      1\n","    ]\n","  ],\n","  \"label\": \"NOT ENOUGH INFO\",\n","  \"evidence\": []\n","}\n","{\n","  \"id\": 6032,\n","  \"predicted_label\": \"REFUTES\",\n","  \"predicted_evidence\": [\n","    [\n","      \"Aruba\",\n","      5\n","    ],\n","    [\n","      \"Aruba\",\n","      6\n","    ]\n","  ],\n","  \"label\": \"REFUTES\",\n","  \"evidence\": [\n","    [\n","      [\n","        null,\n","        null,\n","        \"ABC_islands_-LRB-Lesser_Antilles-RRB-\",\n","        0\n","      ]\n","    ],\n","    [\n","      [\n","        null,\n","        null,\n","        \"ABC_islands_-LRB-Lesser_Antilles-RRB-\",\n","        1\n","      ]\n","    ]\n","  ]\n","}\n","{\n","  \"id\": 176630,\n","  \"predicted_label\": \"REFUTES\",\n","  \"predicted_evidence\": [\n","    [\n","      \"Great_white_shark\",\n","      7\n","    ],\n","    [\n","      \"Marine_mammal\",\n","      8\n","    ]\n","  ],\n","  \"label\": \"NOT ENOUGH INFO\",\n","  \"evidence\": []\n","}\n","{\n","  \"id\": 130048,\n","  \"predicted_label\": \"REFUTES\",\n","  \"predicted_evidence\": [\n","    [\n","      \"Burbank,_California\",\n","      4\n","    ]\n","  ],\n","  \"label\": \"REFUTES\",\n","  \"evidence\": [\n","    [\n","      [\n","        null,\n","        null,\n","        \"Burbank,_California\",\n","        7\n","      ]\n","    ]\n","  ]\n","}\n","{\n","  \"id\": 100046,\n","  \"predicted_label\": \"NOT ENOUGH INFO\",\n","  \"predicted_evidence\": [],\n","  \"label\": \"NOT ENOUGH INFO\",\n","  \"evidence\": []\n","}\n","{\n","  \"id\": 204575,\n","  \"predicted_label\": \"REFUTES\",\n","  \"predicted_evidence\": [\n","    [\n","      \"Commodore_-LRB-rank-RRB-\",\n","      7\n","    ],\n","    [\n","      \"Commodore_-LRB-rank-RRB-\",\n","      1\n","    ],\n","    [\n","      \"Commodore_-LRB-rank-RRB-\",\n","      0\n","    ],\n","    [\n","      \"Commodore_-LRB-rank-RRB-\",\n","      5\n","    ],\n","    [\n","      \"Commodore_-LRB-rank-RRB-\",\n","      4\n","    ],\n","    [\n","      \"Commodore_-LRB-rank-RRB-\",\n","      6\n","    ]\n","  ],\n","  \"label\": \"REFUTES\",\n","  \"evidence\": [\n","    [\n","      [\n","        null,\n","        null,\n","        \"Commodore_-LRB-rank-RRB-\",\n","        0\n","      ]\n","    ],\n","    [\n","      [\n","        null,\n","        null,\n","        \"Commodore_-LRB-rank-RRB-\",\n","        9\n","      ],\n","      [\n","        null,\n","        null,\n","        \"Rear_admiral\",\n","        0\n","      ]\n","    ]\n","  ]\n","}\n","{\n","  \"id\": 107539,\n","  \"predicted_label\": \"NOT ENOUGH INFO\",\n","  \"predicted_evidence\": [],\n","  \"label\": \"NOT ENOUGH INFO\",\n","  \"evidence\": []\n","}\n","{\n","  \"id\": 164883,\n","  \"predicted_label\": \"SUPPORTS\",\n","  \"predicted_evidence\": [\n","    [\n","      \"Hezbollah\\u2013Iran_relations\",\n","      8\n","    ],\n","    [\n","      \"Hezbollah\\u2013Iran_relations\",\n","      1\n","    ],\n","    [\n","      \"Hezbollah\\u2013Iran_relations\",\n","      4\n","    ],\n","    [\n","      \"Hezbollah\\u2013Iran_relations\",\n","      5\n","    ],\n","    [\n","      \"Hezbollah\\u2013Iran_relations\",\n","      3\n","    ],\n","    [\n","      \"Hezbollah\\u2013Iran_relations\",\n","      0\n","    ],\n","    [\n","      \"Hezbollah\\u2013Iran_relations\",\n","      7\n","    ],\n","    [\n","      \"Hezbollah\\u2013Iran_relations\",\n","      2\n","    ]\n","  ],\n","  \"label\": \"SUPPORTS\",\n","  \"evidence\": [\n","    [\n","      [\n","        null,\n","        null,\n","        \"Hezbollah\",\n","        7\n","      ]\n","    ]\n","  ]\n","}\n","{\n","  \"id\": 54298,\n","  \"predicted_label\": \"SUPPORTS\",\n","  \"predicted_evidence\": [\n","    [\n","      \"Electric_chair\",\n","      10\n","    ],\n","    [\n","      \"Electric_chair\",\n","      15\n","    ],\n","    [\n","      \"Capital_punishment_in_the_United_States\",\n","      4\n","    ],\n","    [\n","      \"Electric_chair\",\n","      13\n","    ],\n","    [\n","      \"Electric_chair\",\n","      14\n","    ],\n","    [\n","      \"Electric_chair\",\n","      8\n","    ],\n","    [\n","      \"Electric_chair\",\n","      11\n","    ],\n","    [\n","      \"Electric_chair\",\n","      9\n","    ]\n","  ],\n","  \"label\": \"SUPPORTS\",\n","  \"evidence\": [\n","    [\n","      [\n","        null,\n","        null,\n","        \"Electric_chair\",\n","        0\n","      ],\n","      [\n","        null,\n","        null,\n","        \"Capital_punishment\",\n","        1\n","      ],\n","      [\n","        null,\n","        null,\n","        \"Electric_chair\",\n","        14\n","      ]\n","    ]\n","  ]\n","}\n","{\n","  \"id\": 222749,\n","  \"predicted_label\": \"NOT ENOUGH INFO\",\n","  \"predicted_evidence\": [],\n","  \"label\": \"NOT ENOUGH INFO\",\n","  \"evidence\": []\n","}\n","{\n","  \"id\": 219675,\n","  \"predicted_label\": \"REFUTES\",\n","  \"predicted_evidence\": [\n","    [\n","      \"Corsica\",\n","      12\n","    ],\n","    [\n","      \"Corsica\",\n","      9\n","    ],\n","    [\n","      \"Roman_Italy\",\n","      16\n","    ],\n","    [\n","      \"Corsica\",\n","      1\n","    ]\n","  ],\n","  \"label\": \"REFUTES\",\n","  \"evidence\": [\n","    [\n","      [\n","        null,\n","        null,\n","        \"Corsica\",\n","        5\n","      ]\n","    ],\n","    [\n","      [\n","        null,\n","        null,\n","        \"Corsica\",\n","        0\n","      ]\n","    ]\n","  ]\n","}\n","{\n","  \"id\": 134850,\n","  \"predicted_label\": \"REFUTES\",\n","  \"predicted_evidence\": [\n","    [\n","      \"Ice-T\",\n","      12\n","    ],\n","    [\n","      \"Ice-T\",\n","      7\n","    ],\n","    [\n","      \"Ice-T\",\n","      1\n","    ]\n","  ],\n","  \"label\": \"REFUTES\",\n","  \"evidence\": [\n","    [\n","      [\n","        null,\n","        null,\n","        \"Ice-T\",\n","        1\n","      ]\n","    ],\n","    [\n","      [\n","        null,\n","        null,\n","        \"Ice-T\",\n","        2\n","      ]\n","    ]\n","  ]\n","}\n","{\n","  \"id\": 124578,\n","  \"predicted_label\": \"SUPPORTS\",\n","  \"predicted_evidence\": [\n","    [\n","      \"Gettysburg_Address\",\n","      0\n","    ],\n","    [\n","      \"Gettysburg_Address\",\n","      9\n","    ],\n","    [\n","      \"Gettysburg_Address\",\n","      1\n","    ],\n","    [\n","      \"Gettysburg_Address\",\n","      5\n","    ],\n","    [\n","      \"Gettysburg_Address\",\n","      4\n","    ],\n","    [\n","      \"Gettysburg_Address\",\n","      2\n","    ],\n","    [\n","      \"Gettysburg_Address\",\n","      14\n","    ],\n","    [\n","      \"Gettysburg_Address\",\n","      11\n","    ]\n","  ],\n","  \"label\": \"SUPPORTS\",\n","  \"evidence\": [\n","    [\n","      [\n","        null,\n","        null,\n","        \"Gettysburg_Address\",\n","        13\n","      ]\n","    ],\n","    [\n","      [\n","        null,\n","        null,\n","        \"Gettysburg_Address\",\n","        14\n","      ]\n","    ],\n","    [\n","      [\n","        null,\n","        null,\n","        \"Gettysburg_Address\",\n","        0\n","      ]\n","    ]\n","  ]\n","}\n","{\n","  \"id\": 134126,\n","  \"predicted_label\": \"NOT ENOUGH INFO\",\n","  \"predicted_evidence\": [],\n","  \"label\": \"REFUTES\",\n","  \"evidence\": [\n","    [\n","      [\n","        null,\n","        null,\n","        \"Jason_Bourne_-LRB-film-RRB-\",\n","        6\n","      ]\n","    ]\n","  ]\n","}\n","{\n","  \"id\": 125577,\n","  \"predicted_label\": \"NOT ENOUGH INFO\",\n","  \"predicted_evidence\": [],\n","  \"label\": \"REFUTES\",\n","  \"evidence\": [\n","    [\n","      [\n","        null,\n","        null,\n","        \"Ron_Dennis\",\n","        0\n","      ]\n","    ],\n","    [\n","      [\n","        null,\n","        null,\n","        \"Ron_Dennis\",\n","        1\n","      ]\n","    ],\n","    [\n","      [\n","        null,\n","        null,\n","        \"Ron_Dennis\",\n","        28\n","      ]\n","    ]\n","  ]\n","}\n","{\n","  \"id\": 132244,\n","  \"predicted_label\": \"SUPPORTS\",\n","  \"predicted_evidence\": [\n","    [\n","      \"Wolfgang_Amadeus_Mozart\",\n","      4\n","    ],\n","    [\n","      \"Wolfgang_Amadeus_Mozart\",\n","      5\n","    ],\n","    [\n","      \"Wolfgang_Amadeus_Mozart\",\n","      6\n","    ],\n","    [\n","      \"Wolfgang_Amadeus_Mozart\",\n","      1\n","    ]\n","  ],\n","  \"label\": \"NOT ENOUGH INFO\",\n","  \"evidence\": []\n","}\n","\n","--- Run Report ---\n","        id                                              claim  time_to_check                                   entities                                           keywords                                    retrieved_pages   module2_status                             predicted_evidence_ids                           predicted_evidence_texts   module3_result   module3_status                                     module3_prompt                             module1_report_details                             module2_report_details  query_client_temp  rephrase_client_temp  sentEx_client_temp  nli_client_temp  disambiguate_client_temp  strict_score  label_accuracy  precision  recall        f1\n","0   113501                            Grease had bad reviews.      30.617169                                     Grease                               reviews, grease, bad  Grease (film), Grease (film), Grease (film), G...               OK                                  [[\"Grease_2\", 4]]  [\"Despite breakthrough roles for Pfeiffer, Adr...         SUPPORTS               OK  Based ONLY on the following evidence sentences...  {\"mod_1_total_documents\": 5, \"total_document_t...  {\"claim\": \"Grease had bad reviews.\", \"final_ev...                0.5                   0.9                 0.2              0.1                       0.2      0.366667        0.566667   0.526667     0.4  0.454676\n","1   163803  Ukrainian Soviet Socialist Republic was a foun...      27.259885  Socialist Republic, Ukrainian, Soviet, UN  ukrainian, soviet, socialist, republic, partic...  Republics of the Soviet Union, Member states o...               OK  [[\"Ukrainian_Soviet_Socialist_Republic\", 6], [...  [\"As a Soviet quasi-state, the Ukrainian SSR b...         SUPPORTS               OK  Based ONLY on the following evidence sentences...  {\"mod_1_total_documents\": 6, \"total_document_t...  {\"claim\": \"Ukrainian Soviet Socialist Republic...                0.5                   0.9                 0.2              0.1                       0.2      0.366667        0.566667   0.526667     0.4  0.454676\n","2    70041      2 Hearts is a musical composition by Minogue.      15.033673                                    Minogue              musical, minogue, hearts, composition  2 Hearts (film), Kylie Minogue (album), Kylie ...  NOT ENOUGH INFO                                                 []                                                 []  NOT ENOUGH INFO  NOT ENOUGH INFO                       No evidence provided to LLM.  {\"mod_1_total_documents\": 3, \"total_document_t...  {\"claim\": \"2 Hearts is a musical composition b...                0.5                   0.9                 0.2              0.1                       0.2      0.366667        0.566667   0.526667     0.4  0.454676\n","3   202314        The New Jersey Turnpike has zero shoulders.      18.286444                        New Jersey Turnpike             zero, turnpike, shoulders, new, jersey  Cycling infrastructure, National Highway Traff...  NOT ENOUGH INFO                                                 []                                                 []  NOT ENOUGH INFO  NOT ENOUGH INFO                       No evidence provided to LLM.  {\"mod_1_total_documents\": 5, \"total_document_t...  {\"claim\": \"The New Jersey Turnpike has zero sh...                0.5                   0.9                 0.2              0.1                       0.2      0.366667        0.566667   0.526667     0.4  0.454676\n","4    57085  Legendary Entertainment is the owner of Wanda ...      13.633611                              Wanda Cinemas    wanda, owner, legendary, entertainment, cinemas   Wanda Media, Wanda Film, Legendary Entertainment               OK  [[\"Legendary_Entertainment\", 3], [\"Legendary_E...  [\"It is a part of the Dalian Wanda Group.\", \"I...          REFUTES               OK  Based ONLY on the following evidence sentences...  {\"mod_1_total_documents\": 3, \"total_document_t...  {\"claim\": \"Legendary Entertainment is the owne...                0.5                   0.9                 0.2              0.1                       0.2      0.366667        0.566667   0.526667     0.4  0.454676\n","..     ...                                                ...            ...                                        ...                                                ...                                                ...              ...                                                ...                                                ...              ...              ...                                                ...                                                ...                                                ...                ...                   ...                 ...              ...                       ...           ...             ...        ...     ...       ...\n","25   88894                              Zoe Saldana is a Leo.      17.635963                               Saldana, Zoe                                  zoe, saldana, leo  Zoe Saldaña, Leo (astrology), Leo (astrology),...  NOT ENOUGH INFO                                                 []                                                 []  NOT ENOUGH INFO  NOT ENOUGH INFO                       No evidence provided to LLM.  {\"mod_1_total_documents\": 4, \"total_document_t...  {\"claim\": \"Zoe Saldana is a Leo.\", \"final_evid...                0.5                   0.9                 0.2              0.1                       0.2      0.366667        0.566667   0.526667     0.4  0.454676\n","26   17915         Fred Seibert has produced comedy programs.      21.804566                              Seibert, Fred          seibert, programs, produced, fred, comedy  List of Nickelodeon Animation Studio productio...               OK  [[\"Fred_Seibert\", 7], [\"Fred_Seibert\", 6], [\"F...  [\"He was a seed investor for the website Tumbl...         SUPPORTS               OK  Based ONLY on the following evidence sentences...  {\"mod_1_total_documents\": 6, \"total_document_t...  {\"claim\": \"Fred Seibert has produced comedy pr...                0.5                   0.9                 0.2              0.1                       0.2      0.366667        0.566667   0.526667     0.4  0.454676\n","27   58396       Konidela Production Company was established.      10.451084               Production Company, Konidela         production, konidela, established, company  Konidela Production Company, Konidela Producti...               OK               [[\"Konidela_Production_Company\", 0]]  [\"Konidela Production Company is an Indian fil...         SUPPORTS               OK  Based ONLY on the following evidence sentences...  {\"mod_1_total_documents\": 2, \"total_document_t...  {\"claim\": \"Konidela Production Company was est...                0.5                   0.9                 0.2              0.1                       0.2      0.366667        0.566667   0.526667     0.4  0.454676\n","28  150751                     Paul von Hindenburg was a man.      19.230744                                       Paul                         von, paul, man, hindenburg  Paul von Hindenburg, Paul von Hindenburg, Pres...               OK  [[\"Paul_von_Hindenburg\", 0], [\"Paul_von_Hinden...  [\"In 1925, Hindenburg returned to public life ...         SUPPORTS               OK  Based ONLY on the following evidence sentences...  {\"mod_1_total_documents\": 4, \"total_document_t...  {\"claim\": \"Paul von Hindenburg was a man.\", \"f...                0.5                   0.9                 0.2              0.1                       0.2      0.366667        0.566667   0.526667     0.4  0.454676\n","29  179831                  Vic Mensa was born June 12, 1993.      16.027972                                 Mensa, Vic                       vic, mensa, june, born, 1993                 Vic Mensa, Vic Mensa, Nile Rodgers               OK                                 [[\"Vic_Mensa\", 0]]  [\"Victor Kwesi Mensah (born June 6, 1993), kno...          REFUTES               OK  Based ONLY on the following evidence sentences...  {\"mod_1_total_documents\": 3, \"total_document_t...  {\"claim\": \"Vic Mensa was born June 12, 1993.\",...                0.5                   0.9                 0.2              0.1                       0.2      0.366667        0.566667   0.526667     0.4  0.454676\n","\n","[30 rows x 24 columns]\n","\n","Report saved to: /content/drive/MyDrive/SUNY_Poly_DSA598/datasets/FEVER/paper_test_results/run_report_test_n30_20250419_0726.csv\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["# --- Execution & Scoring ---\n","\n","# Ensure test_data is loaded\n","if test_data:\n","    # Run the system on a subset of the test data\n","    NUM_TEST_CLAIMS = 30 # <<< Set the number of claims to test\n","    initial_sbert_thresh=0.4 # Slightly higher initial threshold?\n","    min_sbert_thresh=0.1\n","    thresh_decay=0.05\n","    max_evidence=8\n","    max_iterations=10\n","    near_match_thresh=0.8\n","    max_pages_to_fetch=25\n","    num_search_results=20\n","    query_client_temp=0.5\n","    rephrase_client_temp=0.9\n","    sentEx_client_temp=0.2\n","    nli_client_temp=0.1\n","    disambiguate_client_temp=0.2\n","    predictions, report_df = module_0_sys_control(test_data, NUM_TEST_CLAIMS, initial_sbert_thresh, min_sbert_thresh, thresh_decay, max_evidence, max_iterations, near_match_thresh, max_pages_to_fetch, num_search_results, query_client_temp, rephrase_client_temp, sentEx_client_temp, nli_client_temp, disambiguate_client_temp, verbose=1, debug=True)\n","\n","    # --- FEVER Scoring ---\n","    # NOTE: This will run the scorer, but scores are only meaningful if 'predictions'\n","    # includes the GOLD 'label' and 'evidence' fields.\n","    # The 'strict_score' might be somewhat informative if NEI predictions align.\n","    print(\"\\n--- FEVER Scoring Results ---\")\n","    if predictions:\n","        # The scorer expects 'evidence' to be a list of lists of possible evidence sets.\n","        for p in predictions:\n","            if \"predicted_evidence\" not in p or not isinstance(p[\"predicted_evidence\"], list):\n","                 # If gold evidence wasn't loaded or is malformed, provide the expected structure.\n","                 # A list containing one element: a list of gold evidence items [ [ [None, None, title, id], ... ], ...]\n","                 # Or if the label is NEI, it expects evidence: []\n","                 if p[\"predicted_label\"] == \"NOT ENOUGH INFO\":\n","                      p[\"predicted_evidence\"] = []\n","                 else:\n","                     # For SUPPORTS/REFUTES where we lack gold data, technically the scorer\n","                     # expects at least one evidence set. Providing an empty set list\n","                     # signals no provable evidence was found/available in gold data.\n","                     p[\"predicted_evidence\"] = [[]] # Represents verifiable but no specific gold sentences provided\n","            if p[\"predicted_label\"] == \"NOT ENOUGH INFO\":\n","                p[\"predicted_evidence\"] = []\n","\n","        # Ensure predicted_evidence is always a list (even if empty)\n","        for p in predictions:\n","            if \"predicted_evidence\" not in p:\n","                p[\"predicted_evidence\"] = []\n","\n","        try:\n","            strict_score, label_accuracy, precision, recall, f1 = fever_score(predictions)\n","            print(f\"Strict Score (Exact Match): {strict_score*100:.2f}%\")\n","            print(f\"Label Accuracy: {label_accuracy*100:.2f}%\")\n","            print(f\"Evidence Precision: {precision*100:.2f}%\")\n","            print(f\"Evidence Recall: {recall*100:.2f}%\")\n","            print(f\"Evidence F1 Score: {f1*100:.2f}%\")\n","            print(f\"Number of test cases scored: {len(predictions)}\")\n","        except Exception as e:\n","            print(f\"Error during FEVER scoring: {e}\")\n","            print(\"Scoring skipped. Check prediction format and scorer compatibility.\")\n","\n","    else:\n","        print(\"No predictions generated to score.\")\n","\n","    # --- Display Results ---\n","    print(\"\\n--- Sample Predictions (Output Format) ---\")\n","    for i, item in enumerate(predictions[:20]): # Show first 20 predictions\n","        print(json.dumps(item, indent=2))\n","        if i >= 20: break # Limit output\n","\n","    print(\"\\n--- Run Report ---\")\n","    # Configure pandas display options\n","    pd.set_option('display.max_rows', 10)\n","    pd.set_option('display.max_columns', 25)\n","    pd.set_option('display.width', 1000)\n","    pd.set_option('display.max_colwidth', 50) # Limit column width\n","\n","    if not report_df.empty:\n","        # Add the strict_score, label_accuracy, precision, recall, f1\n","        # Create new columns\n","        report_df['query_client_temp'] = [query_client_temp] * len(report_df)\n","        report_df['rephrase_client_temp'] = [rephrase_client_temp] * len(report_df)\n","        report_df['sentEx_client_temp'] = [sentEx_client_temp] * len(report_df)\n","        report_df['nli_client_temp'] = [nli_client_temp] * len(report_df)\n","        report_df['disambiguate_client_temp'] = [disambiguate_client_temp] * len(report_df)\n","        report_df['strict_score'] = [strict_score] * len(report_df)\n","        report_df['label_accuracy'] = [label_accuracy] * len(report_df)\n","        report_df['precision'] = [precision] * len(report_df)\n","        report_df['recall'] = [recall] * len(report_df)\n","        report_df['f1'] = [f1] * len(report_df)\n","        print(report_df)\n","        # Save the report\n","        try:\n","             report_filename = f'/content/drive/MyDrive/SUNY_Poly_DSA598/datasets/FEVER/paper_test_results/run_report_test_n{len(predictions)}_{time.strftime(\"%Y%m%d_%H%M\")}.csv'\n","             report_df.to_csv(report_filename, index=False)\n","             print(f\"\\nReport saved to: {report_filename}\")\n","        except Exception as e:\n","            print(f\"Error saving report: {e}\")\n","    else:\n","        print(\"Report DataFrame is empty.\")\n","\n","else:\n","    print(\"Test data not loaded. Cannot run system.\")"]},{"cell_type":"code","source":["latest_results = pd.read_csv('/content/drive/MyDrive/SUNY_Poly_DSA598/datasets/FEVER/paper_test_results/run_report_test_n30_20250419_0726.csv', index_col=0)"],"metadata":{"id":"hXDF3Bcb81Jv","executionInfo":{"status":"ok","timestamp":1745047796102,"user_tz":240,"elapsed":82,"user":{"displayName":"Henry Zelenak","userId":"01809909909045225068"}}},"execution_count":172,"outputs":[]},{"cell_type":"code","source":["results_df = pd.DataFrame(latest_results)"],"metadata":{"id":"9F4Ojvk18-DF","executionInfo":{"status":"ok","timestamp":1745047821861,"user_tz":240,"elapsed":0,"user":{"displayName":"Henry Zelenak","userId":"01809909909045225068"}}},"execution_count":173,"outputs":[]},{"cell_type":"code","source":["results_df.info()"],"metadata":{"id":"oLIGoSMN9J1p","executionInfo":{"status":"ok","timestamp":1745047840386,"user_tz":240,"elapsed":44,"user":{"displayName":"Henry Zelenak","userId":"01809909909045225068"}},"outputId":"0fa9e1a0-80a5-4602-ec40-e0a178b77b54","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":175,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","Index: 30 entries, 113501 to 179831\n","Data columns (total 23 columns):\n"," #   Column                    Non-Null Count  Dtype  \n","---  ------                    --------------  -----  \n"," 0   claim                     30 non-null     object \n"," 1   time_to_check             30 non-null     float64\n"," 2   entities                  27 non-null     object \n"," 3   keywords                  30 non-null     object \n"," 4   retrieved_pages           30 non-null     object \n"," 5   module2_status            30 non-null     object \n"," 6   predicted_evidence_ids    30 non-null     object \n"," 7   predicted_evidence_texts  30 non-null     object \n"," 8   module3_result            30 non-null     object \n"," 9   module3_status            30 non-null     object \n"," 10  module3_prompt            30 non-null     object \n"," 11  module1_report_details    30 non-null     object \n"," 12  module2_report_details    30 non-null     object \n"," 13  query_client_temp         30 non-null     float64\n"," 14  rephrase_client_temp      30 non-null     float64\n"," 15  sentEx_client_temp        30 non-null     float64\n"," 16  nli_client_temp           30 non-null     float64\n"," 17  disambiguate_client_temp  30 non-null     float64\n"," 18  strict_score              30 non-null     float64\n"," 19  label_accuracy            30 non-null     float64\n"," 20  precision                 30 non-null     float64\n"," 21  recall                    30 non-null     float64\n"," 22  f1                        30 non-null     float64\n","dtypes: float64(11), object(12)\n","memory usage: 5.6+ KB\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"iq1SvLGi9NEb"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}