{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 783,
     "status": "ok",
     "timestamp": 1743788258280,
     "user": {
      "displayName": "Henry Zelenak",
      "userId": "01809909909045225068"
     },
     "user_tz": 240
    },
    "id": "6go_d9Ittpoe",
    "outputId": "6bfce39d-b49b-4543-fa83-f4a4db4164f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# Mount google drive\n",
    "from google.colab import drive\n",
    "import gc\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 3692,
     "status": "ok",
     "timestamp": 1743788261971,
     "user": {
      "displayName": "Henry Zelenak",
      "userId": "01809909909045225068"
     },
     "user_tz": 240
    },
    "id": "05EjrBx7ENj6"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "import json\n",
    "import os\n",
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "from random import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1743788262018,
     "user": {
      "displayName": "Henry Zelenak",
      "userId": "01809909909045225068"
     },
     "user_tz": 240
    },
    "id": "OjYHaKKQ6dqR",
    "outputId": "dc943e52-3bee-4690-93c7-c7baf44cd4f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/SUNY_Poly_DSA598\n"
     ]
    }
   ],
   "source": [
    "%cd ./drive/My Drive/SUNY_Poly_DSA598/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 71,
     "status": "ok",
     "timestamp": 1743788262090,
     "user": {
      "displayName": "Henry Zelenak",
      "userId": "01809909909045225068"
     },
     "user_tz": 240
    },
    "id": "p5P6aCEs6oL0",
    "outputId": "f4b06f52-420d-40f1-a269-43389b7f6299"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "archive\t\t\t  .git\t\t\t\t  presentation\n",
      "datasets\t\t  .gitignore\t\t\t  transcribe_voice_notes.ipynb\n",
      "FEVER_set_creation.ipynb  liar_gpt4omini_base_eval.ipynb  work_documents\n",
      "FEVER_set_update.ipynb\t  Module_2_dev.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1743788262109,
     "user": {
      "displayName": "Henry Zelenak",
      "userId": "01809909909045225068"
     },
     "user_tz": 240
    },
    "id": "YjqMXr8E5Oze"
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_jsonl(file_path, encoding='utf-8'):\n",
    "    \"\"\"Loads a JSON Lines file into a list of Python objects.\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding=encoding) as f:  # Specify encoding for safety\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))  # Parse each line individually\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 440,
     "status": "ok",
     "timestamp": 1743788262582,
     "user": {
      "displayName": "Henry Zelenak",
      "userId": "01809909909045225068"
     },
     "user_tz": 240
    },
    "id": "g26lnjt-3AHO"
   },
   "outputs": [],
   "source": [
    "# Data paths (replace with your actual paths if different)\n",
    "#train_clf_path = f\"{fever_path}tabular_clf_paper_dev_train/v1_segmented_n3461_03-29_001.csv\"\n",
    "#valid_clf_path = f\"{fever_path}tabular_clf_paper_dev_valid/v1_segmented_n1482_03-29_001.csv\"\n",
    "fever_path = './datasets/FEVER/'\n",
    "train_sentEx_path = f\"/content/drive/MyDrive/SUNY_Poly_DSA598/datasets/FEVER/tabular_sets/tabular_sentEx_paper_dev_train/v1_segmented_n3461_03-29_001.csv\"\n",
    "valid_sentEx_path = f\"/content/drive/MyDrive/SUNY_Poly_DSA598/datasets/FEVER/tabular_sets/tabular_sentEx_paper_dev_valid/v1_segmented_n1482_03-29_001.csv\"\n",
    "test_path = f\"{fever_path}paper_test.jsonl\"\n",
    "train_path = f\"{fever_path}paper_dev.jsonl\"\n",
    "\n",
    "# Load datasets\n",
    "#train_clf = pd.read_csv(train_clf_path)\n",
    "#valid_clf = pd.read_csv(valid_clf_path)\n",
    "train_sentEx = pd.read_csv(train_sentEx_path)\n",
    "valid_sentEx = pd.read_csv(valid_sentEx_path)\n",
    "test_jsonl = load_jsonl(test_path)\n",
    "train_jsonl = load_jsonl(train_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bYJip1V2L_UA"
   },
   "source": [
    "## FEVER Fine-tuning Dataset Update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "huzxXq7GMDBp"
   },
   "source": [
    "### Sentence extraction from Wikipedia text, now with sentence text, page titles, sentence IDs, and entities in a list of tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UNjlqYKmCfy7",
    "outputId": "efb34050-3933-4d69-e095-34952221cef3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load 109 Wikipedia pages from ./datasets/FEVER/wiki-pages...\n",
      "Loading file: ./datasets/FEVER/wiki-pages/wiki-001.jsonl\n",
      "Loading file: ./datasets/FEVER/wiki-pages/wiki-002.jsonl\n",
      "Loading file: ./datasets/FEVER/wiki-pages/wiki-003.jsonl\n",
      "Loading file: ./datasets/FEVER/wiki-pages/wiki-004.jsonl\n",
      "Loading file: ./datasets/FEVER/wiki-pages/wiki-005.jsonl\n",
      "Loading file: ./datasets/FEVER/wiki-pages/wiki-006.jsonl\n",
      "Loading file: ./datasets/FEVER/wiki-pages/wiki-007.jsonl\n",
      "Loading file: ./datasets/FEVER/wiki-pages/wiki-008.jsonl\n",
      "Loading file: ./datasets/FEVER/wiki-pages/wiki-009.jsonl\n",
      "Loading file: ./datasets/FEVER/wiki-pages/wiki-010.jsonl\n",
      "Loading file: ./datasets/FEVER/wiki-pages/wiki-011.jsonl\n",
      "Loading file: ./datasets/FEVER/wiki-pages/wiki-012.jsonl\n"
     ]
    }
   ],
   "source": [
    "def load_wiki_pages_minimal_memory(wiki_dir, num_files_to_load=50):\n",
    "  \"\"\"\n",
    "  Loads Wikipedia page text from JSONL files into a dictionary (minimal memory usage).\n",
    "  Loads only the 'text' field and only the first 'num_files_to_load' files.\n",
    "\n",
    "  Args:\n",
    "      wiki_dir (str): Path to the directory containing wiki-*.jsonl files.\n",
    "      num_files_to_load (int): Number of wiki files to load (default: 3).\n",
    "\n",
    "  Returns:\n",
    "      dict: A dictionary where keys are page titles and values are the page text.\n",
    "  \"\"\"\n",
    "  print(f\"Attempting to load {num_files_to_load} Wikipedia pages from {wiki_dir}...\")\n",
    "  wiki_pages = {}\n",
    "  loaded_files_count = 0\n",
    "  for filename in sorted(os.listdir(wiki_dir)):\n",
    "      if filename.startswith('wiki-') and filename.endswith('.jsonl'):\n",
    "        filepath = os.path.join(wiki_dir, filename)\n",
    "        print(f\"Loading file: {filepath}\")\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:  # Specify encoding for safety\n",
    "          for line in f:\n",
    "            data = json.loads(line)\n",
    "            # Create a object in the dictionary for the data\n",
    "            wiki_pages[data['id']] = {\n",
    "              'text': data['text'],\n",
    "              'lines': data['lines']\n",
    "            }\n",
    "\n",
    "        loaded_files_count += 1\n",
    "      if loaded_files_count >= num_files_to_load:\n",
    "            break\n",
    "  return wiki_pages\n",
    "\n",
    "\n",
    "wiki_pages_dir = './datasets/FEVER/wiki-pages'\n",
    "num_to_load = len(os.listdir(wiki_pages_dir))\n",
    "wiki_page_list_dicts = load_wiki_pages_minimal_memory(wiki_pages_dir, num_to_load)\n",
    "\n",
    "print(f\"Loaded texts from {len(wiki_page_list_dicts)} Wikipedia pages (minimal memory).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1743287636318,
     "user": {
      "displayName": "Henry Zelenak",
      "userId": "01809909909045225068"
     },
     "user_tz": 240
    },
    "id": "MXhyBLp0TyC7",
    "outputId": "731e04e2-6309-44b0-c9cc-37a6b6c2d0a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1: \n",
      "Page 2: 1928_in_association_football\n",
      "Page 3: 1986_NBA_Finals\n",
      "Page 4: 1901_Villanova_Wildcats_football_team\n",
      "Page 5: 1992_Northwestern_Wildcats_football_team\n",
      "Page 6: 1897_Princeton_Tigers_football_team\n",
      "Page 7: 1536_in_philosophy\n",
      "Page 8: ...Di_terra\n",
      "Page 9: 1967â€“68_MJHL_season\n"
     ]
    }
   ],
   "source": [
    "for page, i in zip(wiki_page_list_dicts, range(1, 10)):\n",
    "  print(f\"Page {i}: {page}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LTJeOVlTbJLh"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import requests\n",
    "import re\n",
    "\n",
    "def extract_evidence_text_debug(fever_item, wiki_page_dict, verbose=False, debug=False):\n",
    "    \"\"\"\n",
    "    Extracts evidence sentences with debugging prints.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "      print(f\"Starting evidence extraction for claim: {fever_item['claim']}\")\n",
    "\n",
    "    if fever_item['label'] == 'NOT ENOUGH INFO':\n",
    "      if verbose:\n",
    "        print(f\"NOT ENOUGH INFO found for claim: {fever_item['claim']}, STOP PROCESSING AND REMOVE NOT ENOUGH INFO CLAIMS\")\n",
    "\n",
    "      return [], ''\n",
    "\n",
    "    evidence_sentences = []\n",
    "    text_str = ''\n",
    "\n",
    "    # flatten the evidence set\n",
    "    evidence_set = fever_item['evidence'] # This is a list of lists of lists, where each sublist contains evidence pieces that an annotator has selected for a claim\n",
    "    evidence_set = [item for sublist in evidence_set for item in sublist] # Flatten the evidence set\n",
    "    # Trim the first two elements of each evidence piece (annotation_id, evidence_id)\n",
    "    evidence_set = [evidence_piece[2:] for evidence_piece in evidence_set] # Trim the first two elements of each evidence piece\n",
    "    if debug:\n",
    "      print(f\"DEBUG 0: Evidence set: {evidence_set}\") # DEBUG 0\n",
    "    # Convert inner lists to tuples before creating a set\n",
    "    evidence_set = list(set(tuple(item) for item in evidence_set))\n",
    "    # Convert back to list of lists (optional, based on your later usage)\n",
    "    evidence_set = [list(item) for item in evidence_set]\n",
    "    if debug:\n",
    "      print(f\"DEBUG 1: Length of evidence_set: {len(evidence_set)}\\nSet: {evidence_set}\") # DEBUG 0\n",
    "\n",
    "    pages_loaded = set() # Set to keep track of loaded pages\n",
    "    for evidence_piece in evidence_set: # Looping over evidence pieces (there will be 1 or more, these are the evidence sentences from different pages, for the same claim)\n",
    "        if debug:\n",
    "          print(f\"DEBUG 2: Evidence piece: {evidence_piece}\") # DEBUG 1\n",
    "        if len(evidence_piece) == 2:  # Check if the evidence piece has 4 elements (annotation_id, evidence_id, page_title, sentence_id)\n",
    "            page_title, sentence_id = evidence_piece # Unpack the evidence piece\n",
    "            if debug:\n",
    "              print(f\"DEBUG 2.1: Processing evidence: page_title={page_title}, sentence_id={sentence_id}\") # DEBUG 2\n",
    "            if page_title is not None and sentence_id is not None: # Check if page_title and sentence_id are not None\n",
    "                wiki_page = wiki_page_dict.get(page_title) # Retrieve the wiki page from the dictionary\n",
    "                if debug:\n",
    "                  print(f\"DEBUG 3: Wiki page retrieved: {wiki_page is not None}\") # DEBUG 3\n",
    "                if wiki_page: # Check if the wiki page for these sentences\n",
    "                    if debug:\n",
    "                      print(f\"DEBUG 4: Wiki page keys: {wiki_page.keys()}\") # DEBUG 4\n",
    "                    if 'lines' in wiki_page and isinstance(wiki_page['lines'], str): # Check if 'lines' key exists and is a string\n",
    "                        lines_str = wiki_page['lines']\n",
    "                        # Check if page has already been loaded and added to the text_str\n",
    "                        if page_title not in pages_loaded:\n",
    "                            pages_loaded.add(page_title)\n",
    "                            text_str += \"\\n\" + wiki_page['text']\n",
    "                        elif debug:\n",
    "                            print(f\"DEBUG 4.1: Page {page_title} already loaded, skipping text addition.\") # DEBUG 4.1\n",
    "                        sentences = lines_str.strip().split('\\n')\n",
    "                        if debug:\n",
    "                          print(f\"DEBUG 5: Number of sentences found: {len(sentences)}\") # DEBUG 5\n",
    "                        if sentence_id < len(sentences):  # Check if sentence_id is within the range of sentences\n",
    "                          for sentence, _ in zip(sentences, range(len(sentences))): # Loop over the sentences and their indices\n",
    "                            if len(sentence.split('\\t')) < 2 or sentence.split('\\t')[1].strip() == '': #\n",
    "                              if debug:\n",
    "                                print(f\"DEBUG 6: Skipping blank line at index {_}\") # DEBUG 6\n",
    "                              continue\n",
    "                            if debug:\n",
    "                              print(f\"DEBUG 7: Line retrieved: {sentence} with ID: \" +  sentence.split('\\t')[0].strip()) # DEBUG 7\n",
    "                            if int(sentence.split('\\t')[0].strip()) == sentence_id:\n",
    "                              sentence_text = sentence.split('\\t')[1].strip() # Extract the text after the tab character (To skip the index at the beginning and the entities after: \"0\tThe 1905 Tempe Normal Owls football team was an American football team that represented Tempe Normal School -LRB- later renamed Arizona State University -RRB- as an independent during the 1905 college football season .\tAmerican football\tAmerican football\tArizona State University\tArizona State University\t1905 college football season\t1905 college football season\")\n",
    "                              # Entities are everything after the second item when splitting by tab\n",
    "                              entities = sentence.split('\\t')[2:] # Extract the entities\n",
    "                              # Remove any leading or trailing whitespace from the entities\n",
    "                              entities = [entity.strip() for entity in entities if entity.strip()] # Remove empty entities\n",
    "                              if debug:\n",
    "                                print(f\"DEBUG 8: Sentence text for index {sentence_id} / \" + sentence.split('\\t')[0].strip() + f\" extracted: {sentence_text}\") # DEBUG 8\n",
    "                                print(f\"DEBUG 9: Entities extracted: {entities}\") # DEBUG 9\n",
    "                              # Append a tuple of (sentence_text, page_title, sentence_id, entities) to the evidence_sentences list\n",
    "                              evidence_sentences.append((sentence_text, page_title, sentence_id, entities))\n",
    "                            #else:\n",
    "                                #if verbose:\n",
    "                                  #print(f\"\\tWarning: Sentence index does not match for page: {page_title}, sentence_id: {sentence_id}\")\n",
    "                        else:\n",
    "                          if verbose:\n",
    "                            print(f\"\\tWarning: Sentence index out of range for page: {page_title}, sentence_id: {sentence_id} (Number of sentences: {len(sentences)})\")\n",
    "                    else:\n",
    "                      if verbose:\n",
    "                        print(f\"\\tWarning: Could not retrieve page or lines for title: {page_title}\")\n",
    "                else:\n",
    "                  if verbose:\n",
    "                    print(f\"\\tWarning: Could not retrieve wiki page for title: {page_title}\")\n",
    "        else:\n",
    "          if verbose:\n",
    "            print(f\"\\tWarning: Unexpected evidence format: {evidence_piece}\")\n",
    "        if verbose:\n",
    "          print(f\"DEBUG 10: Evidence sentences for page {page_title} have been extracted.\")\n",
    "          print(\"_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\")\n",
    "    if verbose:\n",
    "      print(f\"Evidence extraction completed for claim: {fever_item['claim']}\")\n",
    "      print(f\"Number of evidence sentences found: {len(evidence_sentences)}\")\n",
    "      print('--------------------------------------------------------------')\n",
    "    return evidence_sentences, text_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1743303491985,
     "user": {
      "displayName": "Henry Zelenak",
      "userId": "01809909909045225068"
     },
     "user_tz": 240
    },
    "id": "E7OpFaA2LRzW",
    "outputId": "881fb2c7-66b4-41ee-d728-324dbcbac930"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evidence extraction for claim: Janet Leigh was incapable of writing.\n",
      "DEBUG 0: Evidence set: [['Janet_Leigh', 0], ['Janet_Leigh', 12], ['Janet_Leigh', 0], ['Janet_Leigh', 12], ['Janet_Leigh', 0], ['Janet_Leigh', 0], ['Janet_Leigh', 0]]\n",
      "DEBUG 1: Length of evidence_set: 2\n",
      "Set: [['Janet_Leigh', 0], ['Janet_Leigh', 12]]\n",
      "DEBUG 2: Evidence piece: ['Janet_Leigh', 0]\n",
      "DEBUG 2.1: Processing evidence: page_title=Janet_Leigh, sentence_id=0\n",
      "DEBUG 3: Wiki page retrieved: True\n",
      "DEBUG 4: Wiki page keys: dict_keys(['text', 'lines'])\n",
      "DEBUG 5: Number of sentences found: 18\n",
      "DEBUG 7: Line retrieved: 0\tJanet Leigh -LRB- born Jeanette Helen Morrison ; July 6 , 1927 -- October 3 , 2004 -RRB- was an American actress , singer , dancer and author . with ID: 0\n",
      "DEBUG 8: Sentence text for index 0 / 0 extracted: Janet Leigh -LRB- born Jeanette Helen Morrison ; July 6 , 1927 -- October 3 , 2004 -RRB- was an American actress , singer , dancer and author .\n",
      "DEBUG 7: Line retrieved: 1\tShe is best remembered for her performance in Psycho , for which she was awarded the Golden Globe Award for Best Supporting Actress and received an Academy Award nomination .\tPsycho\tPsycho (1960 film)\tGolden Globe Award for Best Supporting Actress\tGolden Globe Award for Best Supporting Actress - Motion Picture\tAcademy Award\tAcademy Award for Best Supporting Actress with ID: 1\n",
      "\tWarning: Sentence index does not match for page: Janet_Leigh, sentence_id: 0\n",
      "DEBUG 7: Line retrieved: 2\tShe was the first wife of actor Tony Curtis and the mother of Kelly Curtis and Jamie Lee Curtis .\tTony Curtis\tTony Curtis\tKelly Curtis\tKelly Curtis\tJamie Lee Curtis\tJamie Lee Curtis with ID: 2\n",
      "\tWarning: Sentence index does not match for page: Janet_Leigh, sentence_id: 0\n",
      "DEBUG 6: Skipping blank line at index 3\n",
      "DEBUG 6: Skipping blank line at index 4\n",
      "DEBUG 7: Line retrieved: 5\tDiscovered by actress Norma Shearer , Leigh made her acting debut on radio in 1946 and secured a contract with MGM the following year .\tNorma Shearer\tNorma Shearer\tMGM\tMetro-Goldwyn-Mayer with ID: 5\n",
      "\tWarning: Sentence index does not match for page: Janet_Leigh, sentence_id: 0\n",
      "DEBUG 7: Line retrieved: 6\tEarly in her career , she appeared in popular films spanning a wide variety of genres , including Act of Violence -LRB- 1948 -RRB- , Little Women -LRB- 1949 -RRB- , Angels in the Outfield -LRB- 1951 -RRB- , Scaramouche -LRB- 1952 -RRB- , The Naked Spur -LRB- 1953 -RRB- and Living It Up -LRB- 1954 -RRB- .\tThe Naked Spur\tThe Naked Spur\tAct of Violence\tAct of Violence\tLittle Women\tLittle Women (1949 film)\tAngels in the Outfield\tAngels in the Outfield (1951 film)\tScaramouche\tScaramouche (1952 film)\tLiving It Up\tLiving It Up with ID: 6\n",
      "\tWarning: Sentence index does not match for page: Janet_Leigh, sentence_id: 0\n",
      "DEBUG 7: Line retrieved: 7\tShe played mostly dramatic roles during the latter half of the 1950s , in such films as Safari -LRB- 1956 -RRB- and Touch of Evil -LRB- 1958 -RRB- , but achieved her most lasting recognition as the doomed Marion Crane in Alfred Hitchcock 's Psycho -LRB- 1960 -RRB- .\tPsycho\tPsycho (1960 film)\tSafari\tSafari (1956 film)\tTouch of Evil\tTouch of Evil\tMarion Crane\tMarion Crane\tAlfred Hitchcock\tAlfred Hitchcock with ID: 7\n",
      "\tWarning: Sentence index does not match for page: Janet_Leigh, sentence_id: 0\n",
      "DEBUG 6: Skipping blank line at index 8\n",
      "DEBUG 6: Skipping blank line at index 9\n",
      "DEBUG 7: Line retrieved: 10\tHer highly publicized marriage to Curtis ended in divorce in 1962 , and after starring in The Manchurian Candidate that same year , Leigh scaled back her career .\tThe Manchurian Candidate\tThe Manchurian Candidate (1962 film) with ID: 10\n",
      "\tWarning: Sentence index does not match for page: Janet_Leigh, sentence_id: 0\n",
      "DEBUG 7: Line retrieved: 11\tIntermittently , she continued to appear in notable films , including Bye Bye Birdie -LRB- 1963 -RRB- , Harper -LRB- 1966 -RRB- and Night of the Lepus -LRB- 1972 -RRB- as well as two films with her daughter , Jamie Lee : The Fog -LRB- 1980 -RRB- and Halloween H20 : 20 Years Later -LRB- 1998 -RRB- .\tBye Bye Birdie\tBye Bye Birdie (film)\tHarper\tHarper (film)\tNight of the Lepus\tNight of the Lepus\tThe Fog\tThe Fog with ID: 11\n",
      "\tWarning: Sentence index does not match for page: Janet_Leigh, sentence_id: 0\n",
      "DEBUG 7: Line retrieved: 12\tShe also wrote four books between 1984 and 2002 , including two novels . with ID: 12\n",
      "\tWarning: Sentence index does not match for page: Janet_Leigh, sentence_id: 0\n",
      "DEBUG 6: Skipping blank line at index 13\n",
      "DEBUG 6: Skipping blank line at index 14\n",
      "DEBUG 7: Line retrieved: 15\tLeigh died in 2004 at age 77 , following a year-long battle with vasculitis , an inflammation of the blood vessels .\tvasculitis\tvasculitis with ID: 15\n",
      "\tWarning: Sentence index does not match for page: Janet_Leigh, sentence_id: 0\n",
      "DEBUG 7: Line retrieved: 16\tAmong her survivors was her husband of 42 years , Robert Brandt . with ID: 16\n",
      "\tWarning: Sentence index does not match for page: Janet_Leigh, sentence_id: 0\n",
      "DEBUG 6: Skipping blank line at index 17\n",
      "DEBUG 9: Evidence sentences for page Janet_Leigh have been extracted.\n",
      "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "DEBUG 2: Evidence piece: ['Janet_Leigh', 12]\n",
      "DEBUG 2.1: Processing evidence: page_title=Janet_Leigh, sentence_id=12\n",
      "DEBUG 3: Wiki page retrieved: True\n",
      "DEBUG 4: Wiki page keys: dict_keys(['text', 'lines'])\n",
      "DEBUG 4.1: Page Janet_Leigh already loaded, skipping text addition.\n",
      "DEBUG 5: Number of sentences found: 18\n",
      "DEBUG 7: Line retrieved: 0\tJanet Leigh -LRB- born Jeanette Helen Morrison ; July 6 , 1927 -- October 3 , 2004 -RRB- was an American actress , singer , dancer and author . with ID: 0\n",
      "\tWarning: Sentence index does not match for page: Janet_Leigh, sentence_id: 12\n",
      "DEBUG 7: Line retrieved: 1\tShe is best remembered for her performance in Psycho , for which she was awarded the Golden Globe Award for Best Supporting Actress and received an Academy Award nomination .\tPsycho\tPsycho (1960 film)\tGolden Globe Award for Best Supporting Actress\tGolden Globe Award for Best Supporting Actress - Motion Picture\tAcademy Award\tAcademy Award for Best Supporting Actress with ID: 1\n",
      "\tWarning: Sentence index does not match for page: Janet_Leigh, sentence_id: 12\n",
      "DEBUG 7: Line retrieved: 2\tShe was the first wife of actor Tony Curtis and the mother of Kelly Curtis and Jamie Lee Curtis .\tTony Curtis\tTony Curtis\tKelly Curtis\tKelly Curtis\tJamie Lee Curtis\tJamie Lee Curtis with ID: 2\n",
      "\tWarning: Sentence index does not match for page: Janet_Leigh, sentence_id: 12\n",
      "DEBUG 6: Skipping blank line at index 3\n",
      "DEBUG 6: Skipping blank line at index 4\n",
      "DEBUG 7: Line retrieved: 5\tDiscovered by actress Norma Shearer , Leigh made her acting debut on radio in 1946 and secured a contract with MGM the following year .\tNorma Shearer\tNorma Shearer\tMGM\tMetro-Goldwyn-Mayer with ID: 5\n",
      "\tWarning: Sentence index does not match for page: Janet_Leigh, sentence_id: 12\n",
      "DEBUG 7: Line retrieved: 6\tEarly in her career , she appeared in popular films spanning a wide variety of genres , including Act of Violence -LRB- 1948 -RRB- , Little Women -LRB- 1949 -RRB- , Angels in the Outfield -LRB- 1951 -RRB- , Scaramouche -LRB- 1952 -RRB- , The Naked Spur -LRB- 1953 -RRB- and Living It Up -LRB- 1954 -RRB- .\tThe Naked Spur\tThe Naked Spur\tAct of Violence\tAct of Violence\tLittle Women\tLittle Women (1949 film)\tAngels in the Outfield\tAngels in the Outfield (1951 film)\tScaramouche\tScaramouche (1952 film)\tLiving It Up\tLiving It Up with ID: 6\n",
      "\tWarning: Sentence index does not match for page: Janet_Leigh, sentence_id: 12\n",
      "DEBUG 7: Line retrieved: 7\tShe played mostly dramatic roles during the latter half of the 1950s , in such films as Safari -LRB- 1956 -RRB- and Touch of Evil -LRB- 1958 -RRB- , but achieved her most lasting recognition as the doomed Marion Crane in Alfred Hitchcock 's Psycho -LRB- 1960 -RRB- .\tPsycho\tPsycho (1960 film)\tSafari\tSafari (1956 film)\tTouch of Evil\tTouch of Evil\tMarion Crane\tMarion Crane\tAlfred Hitchcock\tAlfred Hitchcock with ID: 7\n",
      "\tWarning: Sentence index does not match for page: Janet_Leigh, sentence_id: 12\n",
      "DEBUG 6: Skipping blank line at index 8\n",
      "DEBUG 6: Skipping blank line at index 9\n",
      "DEBUG 7: Line retrieved: 10\tHer highly publicized marriage to Curtis ended in divorce in 1962 , and after starring in The Manchurian Candidate that same year , Leigh scaled back her career .\tThe Manchurian Candidate\tThe Manchurian Candidate (1962 film) with ID: 10\n",
      "\tWarning: Sentence index does not match for page: Janet_Leigh, sentence_id: 12\n",
      "DEBUG 7: Line retrieved: 11\tIntermittently , she continued to appear in notable films , including Bye Bye Birdie -LRB- 1963 -RRB- , Harper -LRB- 1966 -RRB- and Night of the Lepus -LRB- 1972 -RRB- as well as two films with her daughter , Jamie Lee : The Fog -LRB- 1980 -RRB- and Halloween H20 : 20 Years Later -LRB- 1998 -RRB- .\tBye Bye Birdie\tBye Bye Birdie (film)\tHarper\tHarper (film)\tNight of the Lepus\tNight of the Lepus\tThe Fog\tThe Fog with ID: 11\n",
      "\tWarning: Sentence index does not match for page: Janet_Leigh, sentence_id: 12\n",
      "DEBUG 7: Line retrieved: 12\tShe also wrote four books between 1984 and 2002 , including two novels . with ID: 12\n",
      "DEBUG 8: Sentence text for index 12 / 12 extracted: She also wrote four books between 1984 and 2002 , including two novels .\n",
      "DEBUG 6: Skipping blank line at index 13\n",
      "DEBUG 6: Skipping blank line at index 14\n",
      "DEBUG 7: Line retrieved: 15\tLeigh died in 2004 at age 77 , following a year-long battle with vasculitis , an inflammation of the blood vessels .\tvasculitis\tvasculitis with ID: 15\n",
      "\tWarning: Sentence index does not match for page: Janet_Leigh, sentence_id: 12\n",
      "DEBUG 7: Line retrieved: 16\tAmong her survivors was her husband of 42 years , Robert Brandt . with ID: 16\n",
      "\tWarning: Sentence index does not match for page: Janet_Leigh, sentence_id: 12\n",
      "DEBUG 6: Skipping blank line at index 17\n",
      "DEBUG 9: Evidence sentences for page Janet_Leigh have been extracted.\n",
      "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "Evidence extraction completed for claim: Janet Leigh was incapable of writing.\n",
      "Number of evidence sentences found: 2\n",
      "--------------------------------------------------------------\n",
      "\n",
      "Claim: Janet Leigh was incapable of writing.\n",
      "\n",
      "Evidence Sentences:\n",
      "- Janet Leigh -LRB- born Jeanette Helen Morrison ; July 6 , 1927 -- October 3 , 2004 -RRB- was an American actress , singer , dancer and author .\n",
      "- She also wrote four books between 1984 and 2002 , including two novels .\n",
      "\n",
      "Full Text:\n",
      "\n",
      "Janet Leigh -LRB- born Jeanette Helen Morrison ; July 6 , 1927 -- October 3 , 2004 -RRB- was an American actress , singer , dancer and author . She is best remembered for her performance in Psycho , for which she was awarded the Golden Globe Award for Best Supporting Actress and received an Academy Award nomination . She was the first wife of actor Tony Curtis and the mother of Kelly Curtis and Jamie Lee Curtis .   Discovered by actress Norma Shearer , Leigh made her acting debut on radio in 1946 and secured a contract with MGM the following year . Early in her career , she appeared in popular films spanning a wide variety of genres , including Act of Violence -LRB- 1948 -RRB- , Little Women -LRB- 1949 -RRB- , Angels in the Outfield -LRB- 1951 -RRB- , Scaramouche -LRB- 1952 -RRB- , The Naked Spur -LRB- 1953 -RRB- and Living It Up -LRB- 1954 -RRB- . She played mostly dramatic roles during the latter half of the 1950s , in such films as Safari -LRB- 1956 -RRB- and Touch of Evil -LRB- 1958 -RRB- , but achieved her most lasting recognition as the doomed Marion Crane in Alfred Hitchcock 's Psycho -LRB- 1960 -RRB- .   Her highly publicized marriage to Curtis ended in divorce in 1962 , and after starring in The Manchurian Candidate that same year , Leigh scaled back her career . Intermittently , she continued to appear in notable films , including Bye Bye Birdie -LRB- 1963 -RRB- , Harper -LRB- 1966 -RRB- and Night of the Lepus -LRB- 1972 -RRB- as well as two films with her daughter , Jamie Lee : The Fog -LRB- 1980 -RRB- and Halloween H20 : 20 Years Later -LRB- 1998 -RRB- . She also wrote four books between 1984 and 2002 , including two novels .   Leigh died in 2004 at age 77 , following a year-long battle with vasculitis , an inflammation of the blood vessels . Among her survivors was her husband of 42 years , Robert Brandt . \n"
     ]
    }
   ],
   "source": [
    "# Test run evidence extraction\n",
    "claim_item = train_jsonl[0]\n",
    "evidence, full_text = extract_evidence_text_debug(claim_item, wiki_page_list_dicts, verbose=True, debug=True)\n",
    "\n",
    "print(f\"\\nClaim: {claim_item['claim']}\")\n",
    "print(\"\\nEvidence Sentences:\")\n",
    "for item in evidence:\n",
    "    sentence, page_title, sentence_id, entities = item\n",
    "    print(f\"- {sentence}\\n\")\n",
    "    print(f\"  (Page: {page_title}, Sentence ID: {sentence_id}, Entities: {entities})\")\n",
    "print(\"\\nFull Text:\")\n",
    "print(full_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mc-HCMvNAwBm"
   },
   "source": [
    "### Update tabular training files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J9uoMBl03AHT"
   },
   "outputs": [],
   "source": [
    "# Get the claims that are SUPPORTS and REFUTES from the jsonl and the tabular data\n",
    "jsonl_supports = [item for item in train_jsonl if item['label'] == 'SUPPORTS']\n",
    "jsonl_refutes = [item for item in train_jsonl if item['label'] == 'REFUTES']\n",
    "\n",
    "# Iterate through the rows of each dataframe, matching the claim text with the claim in the jsonl file, and extract the evidence sentences for that item\n",
    "def update_evidence_sentences(df, jsonl_supports, jsonl_refutes, wiki_page_dict, verbose=0):\n",
    "    \"\"\"\n",
    "    Updates the evidence sentences in the dataframe based on the JSONL data.\n",
    "    \"\"\"\n",
    "    new_df = df.copy()  # Create a copy of the dataframe to avoid modifying the original\n",
    "    for index, row in new_df.iterrows():\n",
    "        claim = row['claim']\n",
    "        label = row['label']\n",
    "        if label == 'SUPPORTS':\n",
    "            # Find the corresponding JSONL item\n",
    "            jsonl_item = next((item for item in jsonl_supports if item['claim'] == claim), None)\n",
    "        elif label == 'REFUTES':\n",
    "            # Find the corresponding JSONL item\n",
    "            jsonl_item = next((item for item in jsonl_refutes if item['claim'] == claim), None)\n",
    "        else:\n",
    "            # Add none if the label is not SUPPORTS or REFUTES\n",
    "            if verbose:\n",
    "                print(f\"Adding nothing for claim: {claim} with label: {label}\")\n",
    "            jsonl_item = None\n",
    "        if jsonl_item:\n",
    "            # Extract evidence sentences and full text\n",
    "            evidence_sentences, _ = extract_evidence_text_debug(jsonl_item, wiki_page_dict, verbose=verbose)\n",
    "            # Update the dataframe with the evidence sentences, directly in the new_df\n",
    "            new_df.at[index, 'evidence_sentences'] = evidence_sentences\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(f\"NOT ENOUGH INFO found for claim: {claim}, not adding anything\")\n",
    "       \n",
    "    return new_df\n",
    "\n",
    "# Update the evidence sentences in the train and valid dataframes\n",
    "train_sentEx_up = update_evidence_sentences(train_sentEx, jsonl_supports=jsonl_supports, jsonl_refutes=jsonl_refutes, wiki_page_dict=wiki_page_list_dicts, verbose=1)\n",
    "valid_sentEx_up = update_evidence_sentences(valid_sentEx, jsonl_supports=jsonl_supports, jsonl_refutes=jsonl_refutes, wiki_page_dict=wiki_page_list_dicts, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1743346822834,
     "user": {
      "displayName": "Henry Zelenak",
      "userId": "01809909909045225068"
     },
     "user_tz": 240
    },
    "id": "QFCvyEkIRTR7",
    "outputId": "7e429cb1-4a6a-4e9d-dc13-53eba0768483"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "395613294602193653092643591633490623395613304603\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "# Get the current time in 24 hour time, hours:minutes:seconds\n",
    "current_time = datetime.datetime.now().strftime(\"%H:%M:%S\")\n",
    "# Print the seconds only\n",
    "seconds = current_time.split(':')[-1]\n",
    "print(\"22395613294602193653092643591633490623395613304603\")\n",
    "print(seconds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 625,
     "status": "ok",
     "timestamp": 1742179737484,
     "user": {
      "displayName": "Henry Zelenak",
      "userId": "01809909909045225068"
     },
     "user_tz": 240
    },
    "id": "N99yAn0_kIdX",
    "outputId": "62051e2e-2cc3-4f4f-9a2e-9632e58a93e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: prompt_v1_segmented_n200_03-16_001.jsonl\n",
      "Metadata saved to ./datasets/FEVER/GPT_meta/sentEx/train_001.jsonl_metadata.csv\n",
      "Processing file: prompt_v1_segmented_n60_03-16_001.jsonl\n",
      "Metadata saved to ./datasets/FEVER/GPT_meta/sentEx/valid_001.jsonl_metadata.csv\n",
      "Processing file: prompt_v1_segmented_n200_03-16_001.jsonl\n",
      "Metadata saved to ./datasets/FEVER/GPT_meta/clf/train_001.jsonl_metadata.csv\n",
      "Processing file: prompt_v1_segmented_n60_03-16_001.jsonl\n",
      "Metadata saved to ./datasets/FEVER/GPT_meta/clf/valid_001.jsonl_metadata.csv\n"
     ]
    }
   ],
   "source": [
    "# Collect metadata about the files (dataset_name  avg_evdc_len  max_evdc_len  min_evdc_len  sys_msg user_msg  sys_msg_num_tokens  sys_msg_avg_wrd_len usr_msg_num_tokens\tusr_msg_avg_wrd_len)\n",
    "\n",
    "# Helper function to tokenize text and return number of tokens and semantic complexity\n",
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    Tokenizes the input text and returns the number of tokens and semantic complexity.\n",
    "    \"\"\"\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Calculate the number of tokens\n",
    "    num_tokens = len(tokens)\n",
    "\n",
    "    # Calculate semantic complexity using a simple heuristic (e.g., average word length)\n",
    "    avg_word_length = sum(len(word) for word in tokens) / num_tokens if num_tokens > 0 else 0\n",
    "\n",
    "    return num_tokens, avg_word_length\n",
    "\n",
    "\n",
    "# Create a function to collect metadata about the files\n",
    "def collect_metadata(in_path, out_path, mode):\n",
    "    \"\"\"\n",
    "    Collects metadata about the file in the input path and saves it to a CSV file in the output path.\n",
    "    \"\"\"\n",
    "    metadata = pd.DataFrame(columns=['dataset_name', 'size', 'avg_evdc_len', 'max_evdc_len', 'min_evdc_len', 'avg_evdc_tkns', 'avg_claim_tkns', 'max_claim_tkns', 'min_claim_tkns', 'sys_msg', 'user_msg', 'sys_msg_num_tokens', 'sys_msg_avg_wrd_len', 'usr_msg_num_tokens', 'usr_msg_avg_wrd_len'])\n",
    "    filename = os.path.basename(in_path)\n",
    "    print(f\"Processing file: {filename}\")\n",
    "    set_id = filename.split('_')[-1]\n",
    "    if filename.endswith('.jsonl'):\n",
    "        with open(in_path, 'r', encoding='utf-8') as f:\n",
    "            data = [json.loads(line) for line in f]\n",
    "            ds_size = len(data)\n",
    "            # Calculate average and max evidence sentence length\n",
    "            evdc_counts = []\n",
    "            evdc_tokens = []\n",
    "            user_stub_tokens = []\n",
    "            claim_tokens = []\n",
    "            if mode == 'clf':\n",
    "              for item in data:\n",
    "                  evidence = item['messages'][1]['content'].split('\\n\\n')[2]\n",
    "                  evdc_counts.append(len(evidence.split('\\n')))\n",
    "                  evdc_tokens.append(len(nltk.word_tokenize(evidence)))\n",
    "                  user_stub_tokens.append(len(nltk.word_tokenize(item['messages'][1]['content'].split('\\n\\n')[0])))\n",
    "                  claim_tokens.append(len(nltk.word_tokenize(item['messages'][1]['content'].split('\\n\\n')[1])))\n",
    "            elif mode == 'sentEx':\n",
    "              for item in data:\n",
    "                  evdc_counts.append(len(item['messages'][2]['content'].split('\\n')))\n",
    "                  evdc_tokens.append(len(nltk.word_tokenize(item['messages'][2]['content'])))\n",
    "                  user_stub_tokens.append(len(nltk.word_tokenize(item['messages'][1]['content'].split('\\n\\n')[0])))\n",
    "                  claim_tokens.append(len(nltk.word_tokenize(item['messages'][1]['content'].split('\\n\\n')[1])))\n",
    "\n",
    "            # Calculate average, max, and min evidence sentence length\n",
    "            avg_evdc_len = np.mean(evdc_counts)\n",
    "            max_evdc_len = np.max(evdc_counts)\n",
    "            min_evdc_len = np.min(evdc_counts)\n",
    "            avg_evdc_tkns = np.mean(evdc_tokens)\n",
    "\n",
    "            # Calculate average, max, and min claim token count\n",
    "            avg_claim_len = np.mean(claim_tokens)\n",
    "            max_claim_len = np.max(claim_tokens)\n",
    "            min_claim_len = np.min(claim_tokens)\n",
    "\n",
    "            item = data[0]\n",
    "            sys_msg_tks, sys_msg_avg_wrd_len = tokenize_text(item['messages'][0]['content'])\n",
    "            usr_msg_tks, usr_msg_avg_wrd_len = tokenize_text(item['messages'][1]['content'].split('\\n\\n')[0])\n",
    "            # Append metadata to the DataFrame\n",
    "            metadata.loc[len(metadata)] = [filename, ds_size, avg_evdc_len, max_evdc_len, min_evdc_len, avg_evdc_tkns, avg_claim_len, max_claim_len, min_claim_len, item['messages'][0]['content'], item['messages'][1]['content'].split('\\n\\n')[0], sys_msg_tks, sys_msg_avg_wrd_len, usr_msg_tks, usr_msg_avg_wrd_len]\n",
    "    else:\n",
    "        print(f\"Error: {filename} is not a JSONL file.\")\n",
    "    # Save metadata to CSV\n",
    "    save_path = out_path + f\"{set_id}_metadata.csv\"\n",
    "    metadata.to_csv(save_path, index=False)\n",
    "    print(f\"Metadata saved to {save_path}\")\n",
    "\n",
    "# Collect metadata for the evidence extraction training data\n",
    "sentEx_meta_paths = {\n",
    "    \"in_paths\": [f\"{fever_path}GPT_sentEx_paper_dev_train/prompt_v1_segmented_n200_03-16_001.jsonl\", f\"{fever_path}GPT_sentEx_paper_dev_valid/prompt_v1_segmented_n60_03-16_001.jsonl\"],\n",
    "    \"out_paths\": [f\"{fever_path}GPT_meta/sentEx/train_\", f\"{fever_path}GPT_meta/sentEx/valid_\"]\n",
    "}\n",
    "clf_meta_paths = {\n",
    "    \"in_paths\": [f\"{fever_path}GPT_clf_paper_dev_train/prompt_v1_segmented_n200_03-16_001.jsonl\", f\"{fever_path}GPT_clf_paper_dev_valid/prompt_v1_segmented_n60_03-16_001.jsonl\"],\n",
    "    \"out_paths\": [f\"{fever_path}GPT_meta/clf/train_\", f\"{fever_path}GPT_meta/clf/valid_\"]\n",
    "}\n",
    "\n",
    "for in_path, out_path in zip(sentEx_meta_paths['in_paths'], sentEx_meta_paths['out_paths']):\n",
    "    collect_metadata(in_path, out_path, 'sentEx')\n",
    "for in_path, out_path in zip(clf_meta_paths['in_paths'], clf_meta_paths['out_paths']):\n",
    "    collect_metadata(in_path, out_path, 'clf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1149,
     "status": "ok",
     "timestamp": 1743348042373,
     "user": {
      "displayName": "Henry Zelenak",
      "userId": "01809909909045225068"
     },
     "user_tz": 240
    },
    "id": "5KFcxiHKgTvK",
    "outputId": "803b8f8d-cdc7-433f-f3ef-855d0ed06689"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data for classification:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3461 entries, 0 to 3460\n",
      "Data columns (total 3 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   claim               3461 non-null   object\n",
      " 1   evidence_sentences  3461 non-null   object\n",
      " 2   label               3461 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 81.2+ KB\n",
      "None\n",
      "Training data for sentence extraction:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3461 entries, 0 to 3460\n",
      "Data columns (total 4 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   claim               3461 non-null   object\n",
      " 1   evidence_sentences  3461 non-null   object\n",
      " 2   full_text           3461 non-null   object\n",
      " 3   label               3461 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 108.3+ KB\n",
      "None\n",
      "Validation data for classification:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1478 entries, 0 to 1477\n",
      "Data columns (total 3 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   claim               1478 non-null   object\n",
      " 1   evidence_sentences  1478 non-null   object\n",
      " 2   label               1478 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 34.8+ KB\n",
      "None\n",
      "Validation data for sentence extraction:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1482 entries, 0 to 1481\n",
      "Data columns (total 4 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   claim               1482 non-null   object\n",
      " 1   evidence_sentences  1482 non-null   object\n",
      " 2   full_text           1482 non-null   object\n",
      " 3   label               1482 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 46.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV training files and convert tokenized characters back\n",
    "def replace_bracket_tokens(text):\n",
    "    text = text.replace('-LRB-', '(')\n",
    "    text = text.replace('-RRB-', ')')\n",
    "    text = text.replace('-LSB-', '[')\n",
    "    text = text.replace('-RSB-', ']')\n",
    "    return text\n",
    "\n",
    "def load_csv(filepath):\n",
    "    \"\"\"\n",
    "    Load a CSV file and convert tokenized characters back to their original form.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    # If there are three columns, replace the tokens in the first two columns\n",
    "    if len(df.columns) == 3:\n",
    "        df.iloc[:, 0] = df.iloc[:, 0].apply(replace_bracket_tokens)\n",
    "        df.iloc[:, 1] = df.iloc[:, 1].apply(replace_bracket_tokens)\n",
    "    # If there are four columns, replace the tokens in the first three columns\n",
    "    elif len(df.columns) == 4:\n",
    "        df.iloc[:, 0] = df.iloc[:, 0].apply(replace_bracket_tokens)\n",
    "        df.iloc[:, 1] = df.iloc[:, 1].apply(replace_bracket_tokens)\n",
    "        df.iloc[:, 2] = df.iloc[:, 2].apply(replace_bracket_tokens)\n",
    "\n",
    "    return df\n",
    "\n",
    "train_clf_path = f\"{fever_path}tabular_clf_paper_dev_train/v1_segmented_n3468_03-29_001.csv\"\n",
    "valid_clf_path = f\"{fever_path}tabular_clf_paper_dev_valid/v1_segmented_n1488_03-29_001.csv\"\n",
    "train_sentEx_path = f\"{fever_path}tabular_sentEx_paper_dev_train/v1_segmented_n3468_03-29_001.csv\"\n",
    "valid_sentEx_path = f\"{fever_path}tabular_sentEx_paper_dev_valid/v1_segmented_n1488_03-29_001.csv\"\n",
    "\n",
    "train_clf_df = load_csv(train_clf_path)\n",
    "valid_clf_df = load_csv(valid_clf_path)\n",
    "train_sentEx_df = load_csv(train_sentEx_path)\n",
    "valid_sentEx_df = load_csv(valid_sentEx_path)\n",
    "\n",
    "# Print info for each dataframe\n",
    "print(f\"Training data for classification:\")\n",
    "print(train_clf_df.info())\n",
    "print(f\"Training data for sentence extraction:\")\n",
    "print(train_sentEx_df.info())\n",
    "\n",
    "print(f\"Validation data for classification:\")\n",
    "print(valid_clf_df.info())\n",
    "print(f\"Validation data for sentence extraction:\")\n",
    "print(valid_sentEx_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1743348096498,
     "user": {
      "displayName": "Henry Zelenak",
      "userId": "01809909909045225068"
     },
     "user_tz": 240
    },
    "id": "1a8oRcSAnr6Y",
    "outputId": "7e8d707a-86ef-4457-8bfa-5b7fdd1bca80"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"valid_clf_df\",\n  \"rows\": 1478,\n  \"fields\": [\n    {\n      \"column\": \"claim\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1472,\n        \"samples\": [\n          \"Tylenol is advertised for reducing rejection rate.\",\n          \"Poseidon grossed money.\",\n          \"First Motion Picture Unit produced zero films.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"evidence_sentences\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1195,\n        \"samples\": [\n          \"Starrcade was an annual professional wrestling event , originally broadcast via closed-circuit television and eventually broadcast via pay-per-view television , held from 1983 to 2000 by the National Wrestling Alliance ( NWA ) and later World Championship Wrestling ( WCW ) .\",\n          \"Girl was Williams ' first studio album since his 2006 debut , In My Mind .\\nGirl ( stylized as G I R L ) is the second studio album by American singer and record producer Pharrell Williams .\\nThe album was released on March 3 , 2014 , through Williams ' label i Am Other and Columbia Records .\",\n          \"The sixth season of Objetivo Fama ran from February to May 2009.\\nHe replaced Fernando Allende.\\nIt was hosted by Puerto Rican singer Gisselle.\\nAt that time, it was the final season, being dubbed as Objetivo Fama: La Despedida.\\nThe season featured a panel of judges that included returning judges Roberto Sueiro and Hilda Ramos, along with newcomer judge, singer Abraham.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"SUPPORTS\",\n          \"REFUTES\",\n          \"NOT ENOUGH INFO\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "valid_clf_df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-5d99a55c-2f35-43b6-935c-0bf2629c2537\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>evidence_sentences</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Late Show with Stephen Colbert is hosted by Stephen Colbert.</td>\n",
       "      <td>Colbert has hosted The Late Show with Stephen Colbert , a late-night television talk show on CBS , since September 8 , 2015 .</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Annette Badland played Bad Horse in The Sparticle Mystery.</td>\n",
       "      <td>She has played Margaret Blaine in the BBC science fiction series Doctor Who , Doomsday Dora in The Sparticle Mystery , Birdie Henshall in the drama series Cutting It , Mavis in season 6 of Skins , Ursula Crowe in children 's science fiction/fantasy series Wizards vs Aliens , and Babe Smith in soap opera EastEnders .</td>\n",
       "      <td>REFUTES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Good over evil is signified spiritually through Diwali.</td>\n",
       "      <td>It is currently held every two years.\\nUntil 1989, the tournament was known as CONCACAF Championship.\\nThe CONCACAF Gold Cup is North America's major tournament in senior men's soccer and determines the continental champion.\\nFrom 1 to 2005, nations from other confederations have regularly joined the tournament as invitees.\\nIn earlier editions, the continental championship was held in different countries, but since the inception of the Gold Cup in 1991, the United States are constant hosts or co-hosts.</td>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Danger UXB is a British television series.</td>\n",
       "      <td>Danger UXB is a 1979 British ITV television series set during the Second World War developed by John Hawkesworth and starring Anthony Andrews as Lieutenant Brian Ash , an officer in the Royal Engineers .</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shadowhunters was not renewed for a third season in April 2017.</td>\n",
       "      <td>In April 2017 , it was announced that the series had been renewed for a third season of 20 episodes .</td>\n",
       "      <td>REFUTES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Nestor Carbonell played Richard Alpert in Lost during his presidency.</td>\n",
       "      <td>In 2 he starred in Sparsha, which gave him breakthrough.\\nSudeepa is an Indian actor and director who primarily works as leading actor role in the Kannada film industry.\\nSudeep made his debut in Thayavva (1997).\\nHe has also worked in Hindi, Telugu and Tamil films.\\nHe is also popularly known as Kiccha Sudeepa.</td>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>There is a film that is about an ex-convict titled Swordfish (film).</td>\n",
       "      <td>The film centers on Stanley Jobson , an ex-con and computer hacker who is targeted for recruitment into a bank robbery conspiracy because of his formidable hacking skills .</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Match Point is a brand of radios.</td>\n",
       "      <td>Match Point is a 2005 British-Luxembourgish psychological thriller film written and directed by Woody Allen and starring Jonathan Rhys Meyers , Scarlett Johansson , Emily Mortimer , Matthew Goode , Brian Cox , and Penelope Wilton .</td>\n",
       "      <td>REFUTES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Girl is related to G I R L.</td>\n",
       "      <td>Underground music is music with practices perceived as outside, or somehow opposed to, mainstream popular music culture.\\nThe term has been applied to artists in styles such as psychedelia, punk, alternative rock, electronica, industrial music, and wider strains of experimental music.\\nUnderground music may be perceived as expressing sincerity and creative freedom in opposition to those practices deemed formulaic or market-driven.\\nUnderground styles lack the commercial success of popular music movements, and may involve the use of avant-garde or abrasive approaches.\\nNotions of individuality and non-conformity are also commonly deployed.</td>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sidse Babett Knudsen does television work.</td>\n",
       "      <td>Knudsen achieved international recognition for her leading role as fictional Danish Prime Minister Birgitte Nyborg in the Danish TV series Borgen .\\nSidse Babett Knudsen ( [ ËˆsisÉ™ bÌ¥abÌ¥É›dÌ¥ ËˆkÊ°nusnÌ© ] ; born 22 November 1968 ) is a Danish actress who works in theatre , television , and film .</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>T2 Trainspotting is an Australian film.</td>\n",
       "      <td>T2 Trainspotting is a 2017 British comedy drama film , set in and around Edinburgh , Scotland .</td>\n",
       "      <td>REFUTES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Eric Church is a communist.</td>\n",
       "      <td>Vai has also appeared as a guest musician on forty-four albums, as diverse as MotÃ¶rhead, M83, and most recently for the second time with Jacob Collier.\\nVai has been awarded three Grammy Awards and forty other awards.\\nHis discography consists of eleven studio albums, two EPs, two special albums, eight live albums, twelve soundtracks, twenty compilation albums and seven videos.\\nSince 1 Vai also released his own studio albums.\\nHe started his career in 1 playing with Frank Zappa and has since recorded and toured with Alcatrazz, Whitesnake, David Lee Roth, and Public Image Ltd.</td>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Eric Church is a person.</td>\n",
       "      <td>Kenneth Eric Church ( born May 3 , 1977 ) is an American country music singer and songwriter .</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>In the End was the only song released in 2000.</td>\n",
       "      <td>It is the eighth track on their debut album Hybrid Theory ( 2000 ) and was released as the album 's fourth single October 9 , 2001 .</td>\n",
       "      <td>REFUTES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Pearl Jam plays loud music.</td>\n",
       "      <td>Rafael Rebecchi was an architect in Rio de Janeiro.\\nThe renovation was completed in 1906.3\\nHe designed several buildings for the Brazilian National Exposition of 1 in Rio de Janeiro.1 He was also involved with a rebuilding project at the Old Cathedral of Rio de Janeiro.\\nHe was Italian.2 Rebecchi was also the architect in charge of renovating the Brazilian National Archives.\\nHe won a facades competition for the design of 1 buildings along a street.</td>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>The White House Press Secretary's primary responsibility is to be a spokesperson.</td>\n",
       "      <td>The White House Press Secretary is a senior White House official whose primary responsibility is to act as spokesperson for the executive branch of the United States government administration , especially with regard to the President , senior executives , and policies .</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Wish Upon was not directed by John R. Leonetti.</td>\n",
       "      <td>Wish Upon is a 2017 supernatural horror thriller film directed by John R. Leonetti and starring Joey King , Ryan Phillipe , Ki Hong Lee , Shannon Purser , Sydney Park and Sherilyn Fenn .</td>\n",
       "      <td>REFUTES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Girl is written by Pharrell Williams.</td>\n",
       "      <td>These elements include the front portico and frieze.\\nIt was built the 1850s, and is a two-storey three-bay brick house with a hipped roof and pattern-book Greek Revival style elements.\\nAmissâ€“Palmer House, also known as the Palmer House, is a historic home located at Blacksburg, Montgomery County, Virginia.\\nAlso on the property are the contributing kitchen, a two-storey three-bay log house and a smokehouse.3\\nIt has a traditional double-pile center-passage form.</td>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Riddick is in a series.</td>\n",
       "      <td>Actor Vin Diesel has played the title role in all of the Riddick-based films and video games so far .\\nRiddick is a 2013 American science fiction thriller film , the third installment in the Riddick film series .\\nWithin the canon of the series , Riddick is shown to be a highly skilled predator -- he is extremely mobile and stealthy - especially for someone of his size , has a vast knowledge of how to kill almost any humanoid in a variety of ways , is an extreme survivalist , and is notoriously hard to contain .\\nRichard B. Riddick , more commonly known as Riddick , is a fictional character and the antihero of four films in the Riddick series ( Pitch Black , The Chronicles of Riddick , the animated movie The Chronicles of Riddick : Dark Fury , and Riddick ) , as well as the two video games The Chronicles of Riddick : Escape from Butcher Bay and The Chronicles of Riddick : Assault on Dark Athena .\\nRiddick was once a mercenary , then part of a security force , and later a soldier .\\nRiddick is a Furyan , a member of a warrior race obliterated by a military campaign that left Furya desolate , and is one of the last of his kind .</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>L.A. Reid hasn't served as a CEO.</td>\n",
       "      <td>Reid was also the founder and CEO of Hitco Music Publishing and the co-founder of LaFace Records .\\nHe has served as the chairman and CEO of Epic Records , a division of Sony Music Entertainment , the president and CEO of Arista Records , and the chairman and CEO of the Island Def Jam Music Group .</td>\n",
       "      <td>REFUTES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5d99a55c-2f35-43b6-935c-0bf2629c2537')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-5d99a55c-2f35-43b6-935c-0bf2629c2537 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-5d99a55c-2f35-43b6-935c-0bf2629c2537');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-d233b7ed-953a-4428-a982-ae6ce33f298e\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d233b7ed-953a-4428-a982-ae6ce33f298e')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-d233b7ed-953a-4428-a982-ae6ce33f298e button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                                                                claim  \\\n",
       "0                    The Late Show with Stephen Colbert is hosted by Stephen Colbert.   \n",
       "1                          Annette Badland played Bad Horse in The Sparticle Mystery.   \n",
       "2                             Good over evil is signified spiritually through Diwali.   \n",
       "3                                          Danger UXB is a British television series.   \n",
       "4                     Shadowhunters was not renewed for a third season in April 2017.   \n",
       "5               Nestor Carbonell played Richard Alpert in Lost during his presidency.   \n",
       "6                There is a film that is about an ex-convict titled Swordfish (film).   \n",
       "7                                                   Match Point is a brand of radios.   \n",
       "8                                                         Girl is related to G I R L.   \n",
       "9                                          Sidse Babett Knudsen does television work.   \n",
       "10                                            T2 Trainspotting is an Australian film.   \n",
       "11                                                        Eric Church is a communist.   \n",
       "12                                                           Eric Church is a person.   \n",
       "13                                     In the End was the only song released in 2000.   \n",
       "14                                                        Pearl Jam plays loud music.   \n",
       "15  The White House Press Secretary's primary responsibility is to be a spokesperson.   \n",
       "16                                    Wish Upon was not directed by John R. Leonetti.   \n",
       "17                                              Girl is written by Pharrell Williams.   \n",
       "18                                                            Riddick is in a series.   \n",
       "19                                                  L.A. Reid hasn't served as a CEO.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          evidence_sentences  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Colbert has hosted The Late Show with Stephen Colbert , a late-night television talk show on CBS , since September 8 , 2015 .   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              She has played Margaret Blaine in the BBC science fiction series Doctor Who , Doomsday Dora in The Sparticle Mystery , Birdie Henshall in the drama series Cutting It , Mavis in season 6 of Skins , Ursula Crowe in children 's science fiction/fantasy series Wizards vs Aliens , and Babe Smith in soap opera EastEnders .   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               It is currently held every two years.\\nUntil 1989, the tournament was known as CONCACAF Championship.\\nThe CONCACAF Gold Cup is North America's major tournament in senior men's soccer and determines the continental champion.\\nFrom 1 to 2005, nations from other confederations have regularly joined the tournament as invitees.\\nIn earlier editions, the continental championship was held in different countries, but since the inception of the Gold Cup in 1991, the United States are constant hosts or co-hosts.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Danger UXB is a 1979 British ITV television series set during the Second World War developed by John Hawkesworth and starring Anthony Andrews as Lieutenant Brian Ash , an officer in the Royal Engineers .   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      In April 2017 , it was announced that the series had been renewed for a third season of 20 episodes .   \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  In 2 he starred in Sparsha, which gave him breakthrough.\\nSudeepa is an Indian actor and director who primarily works as leading actor role in the Kannada film industry.\\nSudeep made his debut in Thayavva (1997).\\nHe has also worked in Hindi, Telugu and Tamil films.\\nHe is also popularly known as Kiccha Sudeepa.   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               The film centers on Stanley Jobson , an ex-con and computer hacker who is targeted for recruitment into a bank robbery conspiracy because of his formidable hacking skills .   \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Match Point is a 2005 British-Luxembourgish psychological thriller film written and directed by Woody Allen and starring Jonathan Rhys Meyers , Scarlett Johansson , Emily Mortimer , Matthew Goode , Brian Cox , and Penelope Wilton .   \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Underground music is music with practices perceived as outside, or somehow opposed to, mainstream popular music culture.\\nThe term has been applied to artists in styles such as psychedelia, punk, alternative rock, electronica, industrial music, and wider strains of experimental music.\\nUnderground music may be perceived as expressing sincerity and creative freedom in opposition to those practices deemed formulaic or market-driven.\\nUnderground styles lack the commercial success of popular music movements, and may involve the use of avant-garde or abrasive approaches.\\nNotions of individuality and non-conformity are also commonly deployed.   \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Knudsen achieved international recognition for her leading role as fictional Danish Prime Minister Birgitte Nyborg in the Danish TV series Borgen .\\nSidse Babett Knudsen ( [ ËˆsisÉ™ bÌ¥abÌ¥É›dÌ¥ ËˆkÊ°nusnÌ© ] ; born 22 November 1968 ) is a Danish actress who works in theatre , television , and film .   \n",
       "10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           T2 Trainspotting is a 2017 British comedy drama film , set in and around Edinburgh , Scotland .   \n",
       "11                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Vai has also appeared as a guest musician on forty-four albums, as diverse as MotÃ¶rhead, M83, and most recently for the second time with Jacob Collier.\\nVai has been awarded three Grammy Awards and forty other awards.\\nHis discography consists of eleven studio albums, two EPs, two special albums, eight live albums, twelve soundtracks, twenty compilation albums and seven videos.\\nSince 1 Vai also released his own studio albums.\\nHe started his career in 1 playing with Frank Zappa and has since recorded and toured with Alcatrazz, Whitesnake, David Lee Roth, and Public Image Ltd.   \n",
       "12                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Kenneth Eric Church ( born May 3 , 1977 ) is an American country music singer and songwriter .   \n",
       "13                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      It is the eighth track on their debut album Hybrid Theory ( 2000 ) and was released as the album 's fourth single October 9 , 2001 .   \n",
       "14                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Rafael Rebecchi was an architect in Rio de Janeiro.\\nThe renovation was completed in 1906.3\\nHe designed several buildings for the Brazilian National Exposition of 1 in Rio de Janeiro.1 He was also involved with a rebuilding project at the Old Cathedral of Rio de Janeiro.\\nHe was Italian.2 Rebecchi was also the architect in charge of renovating the Brazilian National Archives.\\nHe won a facades competition for the design of 1 buildings along a street.   \n",
       "15                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            The White House Press Secretary is a senior White House official whose primary responsibility is to act as spokesperson for the executive branch of the United States government administration , especially with regard to the President , senior executives , and policies .   \n",
       "16                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Wish Upon is a 2017 supernatural horror thriller film directed by John R. Leonetti and starring Joey King , Ryan Phillipe , Ki Hong Lee , Shannon Purser , Sydney Park and Sherilyn Fenn .   \n",
       "17                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      These elements include the front portico and frieze.\\nIt was built the 1850s, and is a two-storey three-bay brick house with a hipped roof and pattern-book Greek Revival style elements.\\nAmissâ€“Palmer House, also known as the Palmer House, is a historic home located at Blacksburg, Montgomery County, Virginia.\\nAlso on the property are the contributing kitchen, a two-storey three-bay log house and a smokehouse.3\\nIt has a traditional double-pile center-passage form.   \n",
       "18  Actor Vin Diesel has played the title role in all of the Riddick-based films and video games so far .\\nRiddick is a 2013 American science fiction thriller film , the third installment in the Riddick film series .\\nWithin the canon of the series , Riddick is shown to be a highly skilled predator -- he is extremely mobile and stealthy - especially for someone of his size , has a vast knowledge of how to kill almost any humanoid in a variety of ways , is an extreme survivalist , and is notoriously hard to contain .\\nRichard B. Riddick , more commonly known as Riddick , is a fictional character and the antihero of four films in the Riddick series ( Pitch Black , The Chronicles of Riddick , the animated movie The Chronicles of Riddick : Dark Fury , and Riddick ) , as well as the two video games The Chronicles of Riddick : Escape from Butcher Bay and The Chronicles of Riddick : Assault on Dark Athena .\\nRiddick was once a mercenary , then part of a security force , and later a soldier .\\nRiddick is a Furyan , a member of a warrior race obliterated by a military campaign that left Furya desolate , and is one of the last of his kind .   \n",
       "19                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Reid was also the founder and CEO of Hitco Music Publishing and the co-founder of LaFace Records .\\nHe has served as the chairman and CEO of Epic Records , a division of Sony Music Entertainment , the president and CEO of Arista Records , and the chairman and CEO of the Island Def Jam Music Group .   \n",
       "\n",
       "              label  \n",
       "0          SUPPORTS  \n",
       "1           REFUTES  \n",
       "2   NOT ENOUGH INFO  \n",
       "3          SUPPORTS  \n",
       "4           REFUTES  \n",
       "5   NOT ENOUGH INFO  \n",
       "6          SUPPORTS  \n",
       "7           REFUTES  \n",
       "8   NOT ENOUGH INFO  \n",
       "9          SUPPORTS  \n",
       "10          REFUTES  \n",
       "11  NOT ENOUGH INFO  \n",
       "12         SUPPORTS  \n",
       "13          REFUTES  \n",
       "14  NOT ENOUGH INFO  \n",
       "15         SUPPORTS  \n",
       "16          REFUTES  \n",
       "17  NOT ENOUGH INFO  \n",
       "18         SUPPORTS  \n",
       "19          REFUTES  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "valid_clf_df.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EpVwkzHtnvr2"
   },
   "outputs": [],
   "source": [
    "# Save all four dataframes back to the same directories\n",
    "train_clf_df.to_csv(f\"{fever_path}tabular_clf_paper_dev_train/v1_segmented_n3468_03-29_001.csv\", index=False)\n",
    "valid_clf_df.to_csv(f\"{fever_path}tabular_clf_paper_dev_valid/v1_segmented_n1488_03-29_001.csv\", index=False)\n",
    "train_sentEx_df.to_csv(f\"{fever_path}tabular_sentEx_paper_dev_train/v1_segmented_n3468_03-29_001.csv\", index=False)\n",
    "valid_sentEx_df.to_csv(f\"{fever_path}tabular_sentEx_paper_dev_valid/v1_segmented_n1488_03-29_001.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52231,
     "status": "ok",
     "timestamp": 1743349250834,
     "user": {
      "displayName": "Henry Zelenak",
      "userId": "01809909909045225068"
     },
     "user_tz": 240
    },
    "id": "pfYd0vtkrYu-",
    "outputId": "eec92aba-362c-45c7-c8bb-519c8a9e7abf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating syntactic complexity: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1478/1478 [00:11<00:00, 125.87it/s]\n",
      "Calculating syntactic complexity: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3461/3461 [00:26<00:00, 129.78it/s]\n",
      "Calculating syntactic complexity: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1482/1482 [00:11<00:00, 126.35it/s]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "# Import progress bar\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "def calculate_syntactic_complexity(df, verbose=False):\n",
    "    \"\"\"\n",
    "    Calculates syntactic complexity of text in the first column of a Pandas DataFrame, per the formula from Salman et al., 2023\n",
    "\n",
    "    Args:\n",
    "        df: Pandas DataFrame with text in the first column.\n",
    "\n",
    "    Returns:\n",
    "        Pandas DataFrame with an additional 'syntactic_complexity' column.\n",
    "    \"\"\"\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")  # Load a suitable spaCy model\n",
    "\n",
    "    tw = 0.07  # Token weight\n",
    "    vw = 0.3   # Verb weight\n",
    "    cw = 0.4   # Conjunction weight\n",
    "\n",
    "    sc_scores = []\n",
    "\n",
    "    # Wrap the iterable with tqdm for progress bar\n",
    "    for text in tqdm(df.iloc[:, 0], desc=\"Calculating syntactic complexity\"):\n",
    "        doc = nlp(text)\n",
    "        tkn = len(doc)\n",
    "        vrb = 0\n",
    "        cnj = 0\n",
    "\n",
    "        for token in doc:\n",
    "            if token.pos_ == \"VERB\":\n",
    "                vrb += 1\n",
    "            elif token.pos_ in {\"CONJ\", \"CCONJ\", \"SCONJ\"}:  # Use a set for conjunction categories\n",
    "                cnj += 1\n",
    "\n",
    "        sc = tkn * tw + vrb * vw + cnj * cw\n",
    "        sc_scores.append(sc)\n",
    "\n",
    "    df['syntactic_complexity'] = sc_scores  # Add results to DataFrame\n",
    "    return df\n",
    "\n",
    "#train_clf_df = calculate_syntactic_complexity(train_clf_df, True)\n",
    "valid_clf_df = calculate_syntactic_complexity(valid_clf_df)\n",
    "train_sentEx_df = calculate_syntactic_complexity(train_sentEx_df)\n",
    "valid_sentEx_df = calculate_syntactic_complexity(valid_sentEx_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 50,
     "status": "ok",
     "timestamp": 1743350653362,
     "user": {
      "displayName": "Henry Zelenak",
      "userId": "01809909909045225068"
     },
     "user_tz": 240
    },
    "id": "9dF8bTQ-tWiL",
    "outputId": "f28303f0-64d8-47f4-dd99-2377b9a60cbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data for classification:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3461 entries, 0 to 3460\n",
      "Data columns (total 4 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   claim                 3461 non-null   object \n",
      " 1   evidence_sentences    3461 non-null   object \n",
      " 2   label                 3461 non-null   object \n",
      " 3   syntactic_complexity  3461 non-null   float64\n",
      "dtypes: float64(1), object(3)\n",
      "memory usage: 108.3+ KB\n",
      "None\n",
      "Training data for sentence extraction:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3461 entries, 0 to 3460\n",
      "Data columns (total 5 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   claim                 3461 non-null   object \n",
      " 1   evidence_sentences    3461 non-null   object \n",
      " 2   full_text             3461 non-null   object \n",
      " 3   label                 3461 non-null   object \n",
      " 4   syntactic_complexity  3461 non-null   float64\n",
      "dtypes: float64(1), object(4)\n",
      "memory usage: 135.3+ KB\n",
      "None\n",
      "Validation data for classification:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1478 entries, 0 to 1477\n",
      "Data columns (total 4 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   claim                 1478 non-null   object \n",
      " 1   evidence_sentences    1478 non-null   object \n",
      " 2   label                 1478 non-null   object \n",
      " 3   syntactic_complexity  1478 non-null   float64\n",
      "dtypes: float64(1), object(3)\n",
      "memory usage: 46.3+ KB\n",
      "None\n",
      "Validation data for sentence extraction:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1478 entries, 0 to 1477\n",
      "Data columns (total 5 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   claim                 1478 non-null   object \n",
      " 1   evidence_sentences    1478 non-null   object \n",
      " 2   full_text             1478 non-null   object \n",
      " 3   label                 1478 non-null   object \n",
      " 4   syntactic_complexity  1478 non-null   float64\n",
      "dtypes: float64(1), object(4)\n",
      "memory usage: 57.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Print info for each dataframe\n",
    "print(f\"Training data for classification:\")\n",
    "print(train_clf_df.info())\n",
    "print(f\"Training data for sentence extraction:\")\n",
    "print(train_sentEx_df.info())\n",
    "\n",
    "print(f\"Validation data for classification:\")\n",
    "print(valid_clf_df.info())\n",
    "print(f\"Validation data for sentence extraction:\")\n",
    "print(valid_sentEx_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QAcGnkR_ytzD"
   },
   "outputs": [],
   "source": [
    "# Drop 4 samples from valid_sentEx_df randomly\n",
    "valid_sentEx_df = valid_sentEx_df.drop(valid_sentEx_df.sample(n=4).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vhTP257Wy2K2"
   },
   "outputs": [],
   "source": [
    "valid_sentEx_df.reset_index(drop=True, inplace=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1BS8ppvnMAXYKJZjuR6u2OPh3aDBdA3Mb",
     "timestamp": 1741372446020
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
