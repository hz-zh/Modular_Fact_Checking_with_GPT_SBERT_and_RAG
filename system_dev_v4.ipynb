{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modular Fact-Checking System Implementation\n",
    "\n",
    "**Henry Zelenak | Last updated: 05/12/2025**\n",
    "\n",
    "This code implements a modular fact-checking system using Python.\n",
    "\n",
    "**Note that the system is currently configured for \"tuned_GPT-sBERTn1024-sentEx-rephsHist\" runs, where fine-tuned GPT-sentEx and sBERTn1024-sentEx models are used and rephrasing history is provided to GPT-rephrase on each iteration of Module 2.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQREkhWQcldO"
   },
   "source": [
    "## 0. Imports and Introduction\n",
    "<a href=\"https://drive.google.com/file/d/1TYzbcR3QkzQNZR0IX34vhDwzFcJcukHJ/view?usp=sharing\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZaSgZIbtsLP"
   },
   "source": [
    "Version Notes:\n",
    "v4: Added claim rephrasing history for GPT_rephrase (submodule 2.1) to ensure new claims are not repeated. Limit the number of rephrasings to 5, reverting to the original claim after 5 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 176203,
     "status": "ok",
     "timestamp": 1746373527670,
     "user": {
      "displayName": "Henry Zelenak",
      "userId": "01809909909045225068"
     },
     "user_tz": 240
    },
    "id": "TM9fLtjScldR",
    "outputId": "a997f59e-6a5e-4b58-ff37-4b40a0c1082c"
   },
   "outputs": [],
   "source": [
    "# Ensure fever-scorer is installed correctly (assuming previous steps worked)\n",
    "!git clone -b release-v2.0 https://github.com/sheffieldnlp/fever-scorer.git\n",
    "%cd fever-scorer\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "# Open /setup.py and add 'license=\"MIT\"' on line 12, then overwrite the file\n",
    "import os\n",
    "with open('setup.py', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    lines[11] = 'license=\"MIT\"\\n'\n",
    "with open('setup.py', 'w') as f:\n",
    "    f.writelines(lines)\n",
    "    f.close()\n",
    "    print(\"setup.py updated\")\n",
    "!pip install .\n",
    "%cd ..\n",
    "\n",
    "# Install necessary libraries\n",
    "!pip install rouge-score sentence-transformers wikipedia\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rouge_score import rouge_scorer\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "from nltk import Tree, pos_tag, word_tokenize, ne_chunk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from fever.scorer import fever_score # Import the FEVER scorer\n",
    "from nltk import RegexpParser\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import ast\n",
    "import time # For logging\n",
    "import wikipedia # For fetching wikipedia content\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import gc\n",
    "from google.colab import userdata\n",
    "import datetime\n",
    "\n",
    "# Download necessary NLTK data files (ensure they are downloaded)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger_eng', quiet=True) # Added _eng suffix, common naming\n",
    "nltk.download('maxent_ne_chunker', quiet=True)\n",
    "nltk.download('words', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('treebank', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('maxent_ne_chunker_tab', quiet=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBFL9QEacldT"
   },
   "source": [
    "## 1. Load Data and Models\n",
    "<a id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32868,
     "status": "ok",
     "timestamp": 1746373722906,
     "user": {
      "displayName": "Henry Zelenak",
      "userId": "01809909909045225068"
     },
     "user_tz": 240
    },
    "id": "kepg2KTQcldT",
    "outputId": "e6fa7d51-ff75-48c4-b586-57adde34f55e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "Loaded 9999 items from /content/drive/My Drive/SUNY_Poly_DSA598/datasets/FEVER/paper_test.jsonl\n",
      "OpenAI API key found. Client initialized.\n"
     ]
    }
   ],
   "source": [
    "# Mount google drive (if using Colab)\n",
    "try:\n",
    "    from google.colab import drive, userdata\n",
    "    drive.mount('/content/drive')\n",
    "    # Adjust path as needed\n",
    "    BASE_DIR = '/content/drive/My Drive/SUNY_Poly_DSA598/'\n",
    "    DATA_DIR = os.path.join(BASE_DIR, 'datasets/FEVER/')\n",
    "    # Ensure the directory exists\n",
    "    # os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "    # Set API Key securely\n",
    "    api_key = userdata.get('openaikey')\n",
    "\n",
    "    # Assuming data files are in DATA_DIR after mounting\n",
    "    test_path = \"/content/drive/My Drive/SUNY_Poly_DSA598/datasets/FEVER/paper_test.jsonl\" # Explicit path example\n",
    "\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Not running in Colab or google libraries not found. Assuming local setup.\")\n",
    "\n",
    "    # Fallback path if not in Colab drive structure\n",
    "    if not os.path.exists(\"./datasets/FEVER/paper_test.jsonl\"):\n",
    "\n",
    "         test_path = \"paper_test.jsonl\" \n",
    "    else:\n",
    "        test_path = \"./datasets/FEVER/paper_test.jsonl\"\n",
    "\n",
    "def load_jsonl(file_path, encoding='utf-8'):\n",
    "    \"\"\"Loads a JSON Lines file into a list of Python objects.\"\"\"\n",
    "    data = []\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding=encoding) as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    data.append(json.loads(line))\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Warning: Skipping invalid JSON line in {file_path}: {line.strip()}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: File not found at {file_path}\")\n",
    "        return None # Return None or empty list on error\n",
    "    return data\n",
    "\n",
    "# Load test dataset\n",
    "test_data = load_jsonl(test_path)\n",
    "if test_data is None:\n",
    "    print(\"Exiting due to missing test data file.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Loaded {len(test_data)} items from {test_path}\")\n",
    "\n",
    "# Initialize OpenAI client\n",
    "if api_key:\n",
    "    query_client = OpenAI(api_key=api_key)\n",
    "    sentEx_client = OpenAI(api_key=api_key)\n",
    "    rephrase_client = OpenAI(api_key=api_key)\n",
    "    nli_client = OpenAI(api_key=api_key)\n",
    "    print(\"OpenAI API key found. Client initialized.\")\n",
    "else:\n",
    "    print(\"ERROR: OpenAI API key not found. Please set it up.\")\n",
    "    exit()\n",
    "\n",
    "#### INITIALIZE SBERT ####\n",
    "# sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "# More performant model\n",
    "#sbert_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "# FINE TUNED MODEL\n",
    "model_path = f\"{BASE_DIR}models/sBERT/all-mpnet-base-v2_n1024_04-20_12:22_(ORCL_TEST)\"\n",
    "sbert_model = SentenceTransformer(model_path)\n",
    "\n",
    "#### GET GPT-4o-mini MODEL NAMES (GPT_clf and GPT_sentEx)\n",
    "sentEx_ft_model = 'ft:gpt-4o-2024-08-06:personal::BOUBY8Au'\n",
    "#clf_ft_model = 'ft:gpt-4o-2024-08-06:personal::BOUBZKQy'\n",
    "#query_ft_model = 'ft:gpt-4o-2024-08-06:personal::BR9osbAZ:ckpt-step-150'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zpdfYw0fcldV"
   },
   "source": [
    "## 2. Module 1: Document Retrieval\n",
    "<a id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYkfA_GJcldU"
   },
   "source": [
    "## 2. Helper Functions (Entity/Keyword Extraction, Near Match) (Submodule 1.1)\n",
    "<a id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1746373722915,
     "user": {
      "displayName": "Henry Zelenak",
      "userId": "01809909909045225068"
     },
     "user_tz": 240
    },
    "id": "EJNdmjuicldU"
   },
   "outputs": [],
   "source": [
    "# --- Set up Wikipedia API ---\n",
    "# --- Entity and Keyword Extraction (Submodule 1.1) ---\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def extract_entities(text):\n",
    "    \"\"\"Extracts named entities using NLTK.\"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    named_entities = ne_chunk(tagged_tokens)\n",
    "    entities = []\n",
    "    for subtree in named_entities:\n",
    "        if isinstance(subtree, Tree):\n",
    "            # Improve entity extraction: filter by common NE types if needed\n",
    "            # if hasattr(subtree, 'label') and subtree.label() in ['PERSON', 'ORGANIZATION', 'GPE', 'LOCATION']:\n",
    "            entity = \" \".join([word for word, tag in subtree.leaves()])\n",
    "            entities.append(entity)\n",
    "    # Simple post-processing: remove duplicates and potentially filter short/generic entities\n",
    "    entities = sorted(list(set(entities)), key=len, reverse=True) # Prioritize longer entities\n",
    "    return entities\n",
    "\n",
    "def extract_keywords(text, max_keywords=5):\n",
    "    \"\"\"Extracts keywords using TF-IDF.\"\"\"\n",
    "    try:\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', max_features=50) # Use more features initially\n",
    "        tfidf_matrix = vectorizer.fit_transform([text])\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        # Get scores for the single document\n",
    "        scores = tfidf_matrix.toarray().flatten()\n",
    "        # Get indices of top N scores\n",
    "        top_indices = scores.argsort()[-max_keywords:][::-1]\n",
    "        keywords = [feature_names[i] for i in top_indices]\n",
    "        return keywords\n",
    "    except ValueError:\n",
    "        # Handle case where text might be too short or only contains stop words\n",
    "        return []\n",
    "\n",
    "\n",
    "# --- Near Match Function ---\n",
    "def near_match(a, b, threshold=0.9, verbose=0):\n",
    "    \"\"\"\n",
    "    Checks if two strings are similar based on Jaccard similarity of words.\n",
    "    Improved robustness for empty strings.\n",
    "    \"\"\"\n",
    "    if not a or not b: # Handle empty strings\n",
    "        return False\n",
    "    set_a = set(a.lower().split())\n",
    "    set_b = set(b.lower().split())\n",
    "    intersection = len(set_a.intersection(set_b))\n",
    "    union = len(set_a.union(set_b))\n",
    "    if union == 0: # Both strings only contained whitespace or were identical empties\n",
    "        return True if a == b else False # Match if identical, else False\n",
    "    sim = intersection / union # Jaccard similarity\n",
    "\n",
    "    if verbose >= 1:\n",
    "        print(f\"Comparing:\\n  A: '{a}'\\n  B: '{b}'\\n  Similarity: {sim:.4f} (Threshold: {threshold}) -> Match: {sim >= threshold}\")\n",
    "    return sim >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 147,
     "status": "ok",
     "timestamp": 1746373723061,
     "user": {
      "displayName": "Henry Zelenak",
      "userId": "01809909909045225068"
     },
     "user_tz": 240
    },
    "id": "NpxmZJQzcldV"
   },
   "outputs": [],
   "source": [
    "# --- Module 1: Document Retrieval ---\n",
    "query_client = OpenAI(api_key=api_key)\n",
    "def query_generator(claim, keywords, entities, max_pages_to_fetch, temp=0.3, debug=False):\n",
    "    \"\"\"\n",
    "    **UPDATED:** Simulates an entity->URL model.\n",
    "    Generates potential Wikipedia page titles based on extracted entities.\n",
    "    Currently uses the entities directly, formatted as potential titles.\n",
    "    A more advanced simulation could involve an LLM call.\n",
    "\n",
    "    Args:\n",
    "        claim (str): The input claim (context).\n",
    "        keywords (list of str): Keywords (less emphasis now).\n",
    "        entities (list of str): The primary entities to use for lookup.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of potential Wikipedia page titles (strings).\n",
    "    \"\"\"\n",
    "    # Simple simulation: Use entities as potential page titles\n",
    "    # Replace spaces with underscores, common Wikipedia format\n",
    "    #potential_titles = [entity.replace(\" \", \"_\") for entity in entities]\n",
    "\n",
    "    potential_titles = []\n",
    "    try:\n",
    "        prompt = f\"Given the claim '{claim}', list the most relevant Wikipedia page titles likely to contain evidence. Include key facts about the claim, such as the type of items mentioned. Respond with only a bracketed list of lowercase page titles with spaces as underscores, each title wrapped in single quotes and separated by a comma.\"\n",
    "        response = query_client.chat.completions.create(\n",
    "          model='gpt-4o-mini',\n",
    "            messages=[\n",
    "               {\"role\": \"system\", \"content\": \"You are an assistant that identifies relevant Wikipedia page titles based on a claim and entities.\"},\n",
    "               {\"role\": \"user\", \"content\": prompt},\n",
    "           ],\n",
    "          max_tokens=256,\n",
    "          temperature=temp,\n",
    "        )\n",
    "        llm_titles_str = response.choices[0].message.content.strip()\n",
    "        if debug:\n",
    "            print(f\"DEBUG 1.0 (query_generator):\")\n",
    "            print(f\"\\tClaim: {claim}\")\n",
    "            print(f\"\\tEntities: {entities}\")\n",
    "            print(f\"\\tLLM Output: {llm_titles_str}\")\n",
    "            print(\"-_-\" * 5)\n",
    "        try:\n",
    "            llm_titles = ast.literal_eval(llm_titles_str)\n",
    "            if isinstance(llm_titles, list):\n",
    "                potential_titles.extend(llm_titles)\n",
    "        except (ValueError, SyntaxError):\n",
    "            print(f\"Warning: LLM returned non-list format for titles: {llm_titles_str}\")\n",
    "    except Exception as e:\n",
    "         print(f\"Warning: LLM call for query generation failed: {e}\")\n",
    "\n",
    "    # Remove duplicates and limit the number of titles\n",
    "    unique_titles = sorted(list(set(potential_titles)), key=len) # Keep unique, maybe shorter titles are base articles\n",
    "\n",
    "    # Limit the number of pages to fetch to avoid excessive API calls/cost\n",
    "    selected_titles = unique_titles[:max_pages_to_fetch]\n",
    "\n",
    "\n",
    "    if debug:\n",
    "        print(f\"DEBUG 1.1 (query_generator):\")\n",
    "        print(f\"\\tEntities: {entities}\")\n",
    "        print(f\"\\tGenerated Potential Titles: {unique_titles}\")\n",
    "        print(f\"\\tSelected Titles for Retrieval: {selected_titles}\")\n",
    "        print(\"-_-\" * 10)\n",
    "\n",
    "    return selected_titles\n",
    "\n",
    "\n",
    "disambiguate_options_client = OpenAI(api_key=api_key)\n",
    "def retrieve_documents_from_wikipedia(page_titles, claim, entities, num_search_results=2, temp=0.2, debug=False):\n",
    "    \"\"\"\n",
    "    **UPDATED:** Retrieves document content (introduction) from specific Wikipedia page titles.\n",
    "    Uses the 'wikipedia' library for API access.\n",
    "\n",
    "    Args:\n",
    "        page_titles (list of str): List of Wikipedia page titles to fetch.\n",
    "        max_intro_sentences (int): Max sentences to take from the intro.\n",
    "        debug (bool): Enable debug printing.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (list of str, list of str):\n",
    "                 - documents: List of retrieved document introduction texts.\n",
    "                 - document_sources: List of corresponding page titles from which content was retrieved.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    document_sources = []\n",
    "    wikipedia.set_lang(\"en\") # Ensure English Wikipedia\n",
    "\n",
    "    if not page_titles:\n",
    "        if debug:\n",
    "            print(\"DEBUG 1.2 (retrieve_documents): No page titles provided.\")\n",
    "        return [], []\n",
    "\n",
    "    for title in page_titles:\n",
    "        try:\n",
    "            # Suggestion handling: wikipedia library can sometimes find pages even with slight title variations\n",
    "            search_results = wikipedia.search(title, results=num_search_results)\n",
    "            if not search_results:\n",
    "                 if debug:\n",
    "                     print(f\"DEBUG 1.2: No Wikipedia page found for potential title: '{title}'\")\n",
    "                 continue\n",
    "\n",
    "            # Get the closest match in the search result titles to the claim using sBERT\n",
    "            claim_embedding = sbert_model.encode(claim, convert_to_tensor=True)\n",
    "            search_results_embeddings = sbert_model.encode(search_results, convert_to_tensor=True)\n",
    "            similarities = util.pytorch_cos_sim(claim_embedding, search_results_embeddings)[0]\n",
    "            closest_index = similarities.argmax().item()\n",
    "            actual_title = search_results[closest_index]\n",
    "\n",
    "            # Get the page object (handle disambiguation / page errors)\n",
    "            page = wikipedia.page(actual_title, auto_suggest=False, redirect=True) # Use actual title now\n",
    "\n",
    "            # Extract introduction (summary)\n",
    "            # The library's summary often captures the intro well. Limit sentences.\n",
    "            intro_text = page.summary\n",
    "            sentences = nltk.sent_tokenize(intro_text)\n",
    "            content = \" \".join(sentences)\n",
    "\n",
    "            # Basic cleaning (redundant if summary is clean, but good practice)\n",
    "            content = re.sub(r'\\s+', ' ', content).strip() # Normalize whitespace\n",
    "\n",
    "            if content:\n",
    "              if page.title not in document_sources:\n",
    "                documents.append(content)\n",
    "                document_sources.append(page.title) # Use the canonical title from the page object\n",
    "                if debug:\n",
    "                    print(f\"DEBUG 1.2: Successfully retrieved intro from '{page.title}' (searched for '{title}')\")\n",
    "              else:\n",
    "                if debug:\n",
    "                    print(f\"DEBUG 1.2: Skipping duplicate intro for '{page.title}' (searched for '{title}')\")\n",
    "            else:\n",
    "                 if debug:\n",
    "                    print(f\"DEBUG 1.2: Empty content retrieved for page '{page.title}'\")\n",
    "\n",
    "        except wikipedia.exceptions.PageError:\n",
    "            if debug:\n",
    "                print(f\"DEBUG 1.2: PageError - Wikipedia page not found for title: '{title}' (or '{actual_title}')\")\n",
    "        except wikipedia.exceptions.DisambiguationError as e:\n",
    "            if debug:\n",
    "                print(f\"DEBUG 1.2: DisambiguationError for title: '{title}'. Options: {e.options[:len(e.options)]}...\")\n",
    "            # match the options to the claim and entities with gpt_4o-mini\n",
    "            prompt = f\"Given the claim '{claim}' and the entities '{entities}', choose the most relevant Wikipedia page title from the following options: {e.options}. Respond with only the selected title.\"\n",
    "            response = disambiguate_options_client.chat.completions.create(\n",
    "              model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                   {\"role\": \"system\", \"content\": \"You are an assistant that selects the most relevant Wikipedia page title from a list of options.\"},\n",
    "                   {\"role\": \"user\", \"content\": prompt},\n",
    "               ],\n",
    "              max_tokens=100,\n",
    "              temperature=0.3,\n",
    "            )\n",
    "            selected_title = response.choices[0].message.content.strip()\n",
    "            if selected_title in e.options:\n",
    "                try:\n",
    "                    page = wikipedia.page(selected_title, auto_suggest=False, redirect=True)\n",
    "                    intro_text = page.summary\n",
    "                    sentences = nltk.sent_tokenize(intro_text)\n",
    "                    content = \" \".join(sentences)\n",
    "                    content = re.sub(r'\\s+', ' ', content).strip() # Normalize whitespace\n",
    "\n",
    "                    if content:\n",
    "                        documents.append(content)\n",
    "                        document_sources.append(page.title) # Use the canonical title from the page object\n",
    "                        if debug:\n",
    "                            print(f\"DEBUG 1.2: Successfully retrieved intro from '{page.title}' (disambiguated to '{selected_title}')\")\n",
    "                    else:\n",
    "                         if debug:\n",
    "                            print(f\"DEBUG 1.2: Empty content retrieved for disambiguated page '{page.title}'\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Error retrieving disambiguated page '{selected_title}': {e}\")\n",
    "            else:\n",
    "                print(f\"Warning: Selected title '{selected_title}' not in disambiguation options.\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Warning: Network error retrieving '{title}': {e}\")\n",
    "            # Optional: Implement retry logic\n",
    "            time.sleep(1) # Basic wait on error\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Unexpected error retrieving '{title}': {e}\")\n",
    "\n",
    "    if debug:\n",
    "        print(f\"DEBUG 1.2: Retrieved content for {len(documents)} pages out of {len(page_titles)} potential titles.\")\n",
    "        print(\"-_-\" * 10)\n",
    "        print(\"-------------------------------------------------------------------\\n\")\n",
    "\n",
    "    # Concatenate the documents into one string and tokenize it with nltk\n",
    "    all_text = \" \".join(documents)\n",
    "    total_tokens = nltk.word_tokenize(all_text)\n",
    "\n",
    "    return documents, document_sources, total_tokens, len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOMbJ1uncldW"
   },
   "source": [
    "## 3. Module 2: Evidence Sentence Extraction\n",
    "<a id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 191,
     "status": "ok",
     "timestamp": 1746373723263,
     "user": {
      "displayName": "Henry Zelenak",
      "userId": "01809909909045225068"
     },
     "user_tz": 240
    },
    "id": "TVcIPHt8cldW"
   },
   "outputs": [],
   "source": [
    "# --- Module 2 Helper: GPT-4o-mini claim rephrasing (New) --- #\n",
    "rephrase_client = OpenAI(api_key=api_key)\n",
    "def rephrase_claim(claim, rephrs_history=\"\", rephrs_temp=0.5, debug=False):\n",
    "  \"\"\"\n",
    "  **NEW:** Rephrases the claim using GPT-4o-mini.\n",
    "\n",
    "  Args:\n",
    "      claim (str): The input claim.\n",
    "\n",
    "  Returns:\n",
    "      list of str: Rephrased claims.\n",
    "  \"\"\"\n",
    "\n",
    "  if rephrs_history != \"\":\n",
    "    prompt = f\"Rephrase the claim '{claim}' to encompass the same meaning but with different wording. Do not change the meaning or add any new information. Do not use any of the previous versions:\\n {rephrs_history}\"\n",
    "  else:\n",
    "    prompt = f\"Rephrase the claim '{claim}' to encompass the same meaning but with different wording. Do not change the meaning or add any new information.\"\n",
    "\n",
    "\n",
    "  response = rephrase_client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\", # Use specific model\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": prompt},\n",
    "        {\"role\": \"user\", \"content\": claim},\n",
    "    ],\n",
    "    max_tokens=512, # Adjust based on expected output length\n",
    "    n=1,\n",
    "    stop=None,\n",
    "    temperature=rephrs_temp, # Lower temp for more deterministic output\n",
    "  )\n",
    "  rephrased_claim = response.choices[0].message.content.strip()\n",
    "\n",
    "  if debug:\n",
    "      print(f\"DEBUG 2.1.1 (rephrase_claim):\")\n",
    "      #print(f\"\\tOriginal Claim: {claim}\")\n",
    "      print(f\"\\tRephrased Claim: {rephrased_claim}\")\n",
    "      print(\"-_-\" * 5)\n",
    "\n",
    "  # Return the rephrased claim\n",
    "  return rephrased_claim\n",
    "\n",
    "\n",
    "# --- Module 2 Helper: sBERT Filtering (Updated) ---\n",
    "\n",
    "def sbert_slide_filter(documents, doc_sources, claim, sbert_threshold, debug=False):\n",
    "    \"\"\"\n",
    "    **UPDATED:** Performs sentence filtering using sBERT similarity.\n",
    "    Processes each document individually, assigning sentence IDs.\n",
    "    Returns candidates as [source, id, text, score].\n",
    "\n",
    "    Args:\n",
    "        documents (list of str): List of document texts (introductions).\n",
    "        doc_sources (list of str): Corresponding source identifiers (page titles).\n",
    "        claim (str): The claim text.\n",
    "        sbert_threshold (float): The similarity threshold.\n",
    "        debug (bool): Enable debug printing.\n",
    "\n",
    "    Returns:\n",
    "        list: List of candidate sentences: [[page_title, sentence_id, sentence_text, similarity_score], ...]\n",
    "    \"\"\"\n",
    "    all_candidates = []\n",
    "    claim_embedding = sbert_model.encode(claim, convert_to_tensor=True)\n",
    "\n",
    "    if len(documents) != len(doc_sources):\n",
    "        print(\"Error: Mismatch between documents and sources count in sbert_slide_filter.\")\n",
    "        return []\n",
    "\n",
    "    for doc_text, source_id in zip(documents, doc_sources):\n",
    "        sentences = nltk.sent_tokenize(doc_text)\n",
    "        if not sentences:\n",
    "            continue\n",
    "\n",
    "        # Calculate the total number of tokens\n",
    "        total_tokens = sum(len(sent.split()) for sent in sentences)\n",
    "\n",
    "        # sBERT was trained on the page titles (as appended to the sentences) with bracket encoding from the FEVER dataset\n",
    "        # We need to convert the page title to the same format\n",
    "       ### THIS RESULTED IN A LOWER SCORE FOR THE FINE TUNED MODEL - TESTED 04-25-25 AND REMOVED 04-26-25\n",
    "        ###\"\"\" RESULTS\n",
    "        \"\"\"\n",
    "        --- FEVER Scoring Results ---\n",
    "        Strict Score (Exact Match): 43.33%\n",
    "        Label Accuracy: 70.00%\n",
    "        Evidence Precision: 41.50%\n",
    "        Evidence Recall: 35.00%\n",
    "        Evidence F1 Score: 37.97%\n",
    "        Number of test cases scored: 30\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        bracket_mapping = {\n",
    "            \"(\": \"-LRB-\",\n",
    "            \")\": \"-RRB-\",\n",
    "            \"[\": \"-LSB-\",\n",
    "            \"]\": \"-RSB-\"\n",
    "        }\n",
    "        # Convert the source_id to the same format\n",
    "        for key, value in bracket_mapping.items():\n",
    "            source_id = source_id.replace(key, value)\n",
    "        \"\"\"\n",
    "\n",
    "        ############################################################################\n",
    "        ### FINE-TUNed MODEL MODE—WE NEED TO APPEND THE PAGE TITLE TO THE CANDIDATE ORIGIN SENTENCE\n",
    "        # Append the source_id to the end of each sentences for the fine-tuned model\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            sentences[i] = sentence + \" \" + source_id # Append source_id (URL/title) with a space separator\n",
    "        ############################################################################\n",
    "\n",
    "        # Encode all sentences in the document at once for efficiency\n",
    "        sentence_embeddings = sbert_model.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "        # Calculate cosine similarities between claim and all sentences in this doc\n",
    "        similarities = util.pytorch_cos_sim(claim_embedding, sentence_embeddings)[0] # Shape [1, num_sentences] -> [num_sentences]\n",
    "\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            similarity_score = similarities[i].item() # Get scalar value\n",
    "\n",
    "            if similarity_score >= sbert_threshold:\n",
    "                candidate = [source_id, i, sentence, similarity_score]\n",
    "                all_candidates.append(candidate)\n",
    "                if debug == 2: # More verbose debug\n",
    "                     print(f\"DEBUG 2.2.1 (sbert_filter):\")\n",
    "                     print(f\"\\tClaim: {claim[:50]}...\")\n",
    "                     print(f\"\\tDoc: {source_id}, Sent ID: {i}\")\n",
    "                     print(f\"\\tSentence: {sentence[:100]}...\")\n",
    "                     # print(f\"\\tSentence+Source (optional): {sentence_with_source[:100]}...\")\n",
    "                     print(f\"\\tSimilarity: {similarity_score:.4f} (Threshold: {sbert_threshold}) -> PASSED\")\n",
    "                     print(\"-_-\" * 5)\n",
    "\n",
    "    # Sort candidates by similarity score (descending) - helps LLM prioritize\n",
    "    all_candidates.sort(key=lambda x: x[3], reverse=True)\n",
    "\n",
    "    if debug:\n",
    "       print(f\"DEBUG 2.2.2 (sbert_filter):\")\n",
    "       print(f\"\\tTotal candidates found across all docs: {len(all_candidates)}\")\n",
    "       # print(f\"\\tTop 3 candidates: {all_candidates[:3]}\") # Print top few if needed\n",
    "       print(\"-_-\" * 10)\n",
    "\n",
    "    return all_candidates, total_tokens\n",
    "\n",
    "\n",
    "# --- Module 2 Helper: LLM Sentence Selection (Updated Prompt) ---\n",
    "sentEx_client = OpenAI(api_key=api_key)\n",
    "def extract_sentences_with_llm(claim, candidate_sentences_text, prompt, sentEx_temp, debug=False):\n",
    "    \"\"\"\n",
    "    **UPDATED:** Extracts sentences using an LLM based on provided candidates.\n",
    "    Prompt adjusted for selection task.\n",
    "\n",
    "    Args:\n",
    "        claim (str): The input claim.\n",
    "        candidate_sentences_text (list of str): Candidate sentences provided by sBERT.\n",
    "        prompt (str): The specific prompt for the LLM (should guide selection).\n",
    "        debug (bool): Enable debug printing.\n",
    "\n",
    "    Returns:\n",
    "        list of str: Selected sentences (as strings). Returns [\"NOT ENOUGH INFO\"] on failure or specific LLM response.\n",
    "    \"\"\"\n",
    "\n",
    "    if not candidate_sentences_text:\n",
    "      if debug:\n",
    "        print(\"Warning: No candidates from sBERT to select from.\")\n",
    "      return [\"NOT ENOUGH INFO\"] # No candidates to select from\n",
    "\n",
    "    # Format candidates for the prompt (e.g., numbered list)\n",
    "    formatted_candidates = \"\\n\".join([f\"{i+1}. {s}\" for i, s in enumerate(candidate_sentences_text)])\n",
    "\n",
    "    full_prompt = f\"{prompt}\\n\\nClaim: {claim}\\n\\nSelect from these candidate sentences:\\n{formatted_candidates}\"\n",
    "\n",
    "    if debug > 1:\n",
    "        print(f\"DEBUG 2.3.2 (LLM Selection):\")\n",
    "        print(f\"\\tLLM Prompt (partial):\\n{prompt}\\n...\") # Show base prompt\n",
    "        print(f\"\\tNum Candidates Sent to LLM: {len(candidate_sentences_text)}\")\n",
    "        # print(f\"\\tCandidates: {formatted_candidates}\")\n",
    "        print(\"-_-\" * 5)\n",
    "\n",
    "    try:\n",
    "        response = sentEx_client.chat.completions.create(\n",
    "            model=sentEx_ft_model, # Use the specified model\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant. Select sentences from the provided list that are evidence for the claim. Return ONLY the selected sentences, each on a new line. If none are relevant, respond ONLY with 'NOT ENOUGH INFO'.\"},\n",
    "                {\"role\": \"user\", \"content\": full_prompt},\n",
    "            ],\n",
    "            max_tokens=512, # Adjust based on expected output length\n",
    "            n=1,\n",
    "            stop=None,\n",
    "            temperature=sentEx_temp, # Lower temp for more deterministic selection\n",
    "        )\n",
    "        llm_output = response.choices[0].message.content.strip()\n",
    "\n",
    "        if debug:\n",
    "            print(f\"DEBUG 2.3.3 (LLM Selection):\")\n",
    "            print(f\"\\tLLM Raw Output:\\n{llm_output}\")\n",
    "            print(\"-_-\" * 5)\n",
    "\n",
    "        if \"NOT ENOUGH INFO\" in llm_output:\n",
    "             # Check if it's the *only* response, case-insensitive\n",
    "             if llm_output.upper() == \"NOT ENOUGH INFO\":\n",
    "                 return [\"NOT ENOUGH INFO\"]\n",
    "             else:\n",
    "                 # Handle cases where NEI is mixed with sentences - treat as NEI or try to parse?\n",
    "                 # Safer to treat as NEI if the instruction was to only return NEI when applicable.\n",
    "                 print(f\"Warning: LLM output contained 'NOT ENOUGH INFO' along with other text. Interpreting as NEI.\")\n",
    "                 return [\"NOT ENOUGH INFO\"]\n",
    "\n",
    "\n",
    "        # Split the response into sentences, removing empty lines\n",
    "        selected_sentences = [s.strip() for s in llm_output.split('\\n') if s.strip()]\n",
    "\n",
    "        # Optional: Post-process LLM output - remove potential numbering (e.g., \"1. Sentence text\")\n",
    "        processed_sentences = []\n",
    "        for s in selected_sentences:\n",
    "            match = re.match(r'^\\s*\\d+\\.\\s*(.*)', s) # Matches \"1. \", \" 2. \", etc.\n",
    "            if match:\n",
    "                processed_sentences.append(match.group(1).strip())\n",
    "            else:\n",
    "                processed_sentences.append(s) # Keep as is if no numbering pattern\n",
    "\n",
    "        return processed_sentences\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during LLM call in extract_sentences_with_llm: {e}\")\n",
    "        return [\"NOT ENOUGH INFO\"] # Treat errors as failure to find info\n",
    "\n",
    "\n",
    "# --- Module 2 Main Control Flow (Updated) ---\n",
    "\n",
    "def module_2_2_controls(claim, documents, doc_sources, entities, keywords, initial_sbert_thresh=0.2, min_sbert_thresh=0.1, thresh_decay=0.05, max_evidence=5, max_iterations=5, near_match_thresh=0.9, rephrs_temp=0.3, sentEx_temp=0.3, verbose=0, debug=False):\n",
    "    \"\"\"\n",
    "    **UPDATED:** Module 2 implementing iterative sBERT -> LLM selection with reassociation.\n",
    "\n",
    "    Args:\n",
    "        claim (str): The input claim.\n",
    "        documents (list of str): List of retrieved document texts.\n",
    "        doc_sources (list of str): Corresponding source identifiers (page titles).\n",
    "        entities (list of str): Entities from the claim.\n",
    "        keywords (list of str): Keywords from the claim.\n",
    "        initial_sbert_thresh (float): Starting sBERT similarity threshold.\n",
    "        min_sbert_thresh (float): Minimum sBERT threshold allowed.\n",
    "        thresh_decay (float): Amount to decrease threshold if LLM selects few sentences.\n",
    "        max_evidence (int): Target number of evidence sentences.\n",
    "        verbose (int): Verbosity level.\n",
    "        debug (bool): Enable debug printing.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (list, str, dict):\n",
    "                 - final_evidence_ids: List of selected evidence: [[page_title, sentence_id], ...]\n",
    "                 - status: \"OK\" or \"NOT ENOUGH INFO\".\n",
    "                 - report: Dictionary with run details.\n",
    "    \"\"\"\n",
    "    if verbose: print(\"###### M2: Starting Evidence Extraction ######\")\n",
    "\n",
    "    final_evidence_ids = [] # Stores [[page_title, sentence_id]]\n",
    "    all_sbert_candidates_map = {} # Store all candidates found { (title, id) : [title, id, text, score] } to avoid duplicates and for re-association\n",
    "    selected_candidate_keys = set() # Keep track of (title, id) keys already selected\n",
    "\n",
    "    current_sbert_thresh = initial_sbert_thresh\n",
    "    sbert_total_tokens = 0\n",
    "\n",
    "    llm_total_tokens = 0\n",
    "    llm_total_sentences = 0\n",
    "\n",
    "    # Define rephrs_history for rephrasing\n",
    "    rephrs_history = \"\"\n",
    "\n",
    "    # Define prompts (using entities/keywords)\n",
    "    entity_str = \", \".join(entities) if entities else \"relevant entities\"\n",
    "    keyword_str = \", \".join(keywords) if keywords else \"relevant keywords\"\n",
    "    prompts = {\n",
    "      \"init\": f\"Retrieve sentences from the list that either support or refute the following claim. Specifically, focus on sentences mentioning {entity_str}. Order the sentences by relevance, highest first, and return a list separated by the return character. If there are no relevant sentences, respond with 'NOT ENOUGH INFO'. DO NOT CREATE ANY SENTENCES THAT ARE NOT IN THE PROVIDED LIST, AND DO NOT TRUNCATE THE SENTENCE.\",\n",
    "      \"followup\": f\"You didn’t find enough sentences. Find additional (new) sentences that that are relevant to key points in the claim. Order the sentences by relevance, highest first, and return a list separated by the return character. If there are no relevant sentences, respond with 'NOT ENOUGH INFO'. DO NOT CREATE ANY SENTENCES THAT ARE NOT IN THE PROVIDED LIST, AND DO NOT TRUNCATE THE SENTENCE.\",\n",
    "    }\n",
    "\n",
    "    if debug:\n",
    "        print(f\"DEBUG 2.1 (module_2_controls):\")\n",
    "        print(f\"\\tClaim: {claim}\")\n",
    "        print(f\"\\tEntities: {entities}\")\n",
    "        print(f\"\\tKeywords: {keywords}\")\n",
    "        print(f\"\\tInitial sBERT Thresh: {initial_sbert_thresh}, Min Thresh: {min_sbert_thresh}\")\n",
    "        print(f\"\\tMax Evidence Target: {max_evidence}, Max Iterations: {max_iterations}\")\n",
    "        print(\"-_-\" * 10)\n",
    "\n",
    "    for iteration in range(max_iterations):\n",
    "        if len(final_evidence_ids) >= max_evidence:\n",
    "            if verbose: print(f\"M2 Iter {iteration}: Reached target evidence count ({len(final_evidence_ids)}).\")\n",
    "            break\n",
    "\n",
    "        if debug:\n",
    "            print(f\"DEBUG 2. Iteration {iteration+1}/{max_iterations}, Current sBERT Thresh: {current_sbert_thresh:.3f}\")\n",
    "\n",
    "        ##### GPT_REPHRASE CALL #####\n",
    "        # Rephrase the claim for better context\n",
    "        if iteration == 0:\n",
    "            this_claim = claim # Use original claim for the first iteration\n",
    "        elif iteration < 5:\n",
    "            this_claim = rephrase_claim(claim, rephrs_history, rephrs_temp, debug) #\n",
    "        else:\n",
    "            this_claim = claim # Use original claim again if too many iterations (too expensive)\n",
    "        rephrs_history += f\"\\n{this_claim}\" # Add to history for next iteration\n",
    "\n",
    "        ##### SBERT_SENTEX CALL #####\n",
    "        # 1. Get sBERT Candidates (at current threshold)\n",
    "        # 1.1: Calculate the total number of tokens across all documents\n",
    "        sbert_candidates, iter_tokens = sbert_slide_filter(documents, doc_sources, this_claim, current_sbert_thresh, debug=debug)\n",
    "        sbert_total_tokens += iter_tokens\n",
    "\n",
    "        # Store new candidates and identify *new* ones for this iteration's LLM input\n",
    "        new_candidates_for_llm = []\n",
    "        current_iter_candidate_details = [] # Store details [[title, id, text],...] for re-association\n",
    "        for cand in sbert_candidates:\n",
    "            key = (cand[0], cand[1]) # (title, id)\n",
    "            if key not in all_sbert_candidates_map:\n",
    "                all_sbert_candidates_map[key] = cand # Store full details\n",
    "            # Only consider candidates not already selected for the LLM input\n",
    "            if key not in selected_candidate_keys:\n",
    "                 new_candidates_for_llm.append(cand[2]) # Add sentence text to LLM input list\n",
    "                 current_iter_candidate_details.append([cand[0], cand[1], cand[2]]) # Store [title, id, text] for matching\n",
    "\n",
    "        if not new_candidates_for_llm:\n",
    "            if verbose: print(f\"M2 Iter {iteration+1}: No new candidates found by sBERT at threshold {current_sbert_thresh:.3f}.\")\n",
    "            # Option: Lower threshold aggressively or break if already low\n",
    "            if current_sbert_thresh > min_sbert_thresh:\n",
    "                 current_sbert_thresh = max(min_sbert_thresh, current_sbert_thresh - thresh_decay * 2) # Faster decay if nothing found\n",
    "                 if verbose: print(f\"   Lowering threshold to {current_sbert_thresh:.3f} for next try.\")\n",
    "                 continue # Try again with lower threshold\n",
    "            else:\n",
    "                 if verbose: print(f\"M2 Iter {iteration+1}: No new candidates and threshold at minimum ({current_sbert_thresh:.3f}). Stopping.\")\n",
    "                 break # Stop if threshold is already at minimum\n",
    "\n",
    "        ##### GPT_SENTEX CALL #####\n",
    "        # 2. LLM Selection\n",
    "        # 2.1: Calculate the number of tokens sent to the LLM at this iteration\n",
    "        llm_total_tokens += sum(len(sent.split()) for sent in new_candidates_for_llm)\n",
    "        llm_total_sentences += len(new_candidates_for_llm)\n",
    "        if verbose: print(f\"M2 Iter {iteration+1}: Sending {len(new_candidates_for_llm)} new candidates to LLM for selection.\")\n",
    "        # Use the appropriate prompt based on the iteration\n",
    "        if iteration == 0:\n",
    "            this_prompt = prompts[\"init\"]\n",
    "        else:\n",
    "            this_prompt = prompts[\"followup\"]\n",
    "        selected_sentences_text = extract_sentences_with_llm(this_claim, new_candidates_for_llm, this_prompt, sentEx_temp, debug=debug)\n",
    "\n",
    "        # 3. Process LLM Output & Re-association\n",
    "        num_selected_this_iter = 0\n",
    "        if selected_sentences_text == [\"NOT ENOUGH INFO\"]:\n",
    "            if verbose: print(f\"M2 Iter {iteration+1}: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\")\n",
    "            # Decide how to proceed: lower threshold, stop?\n",
    "            # Lower threshold if LLM found nothing on the first or second tries\n",
    "            if iteration < 2 and current_sbert_thresh > min_sbert_thresh:\n",
    "                current_sbert_thresh = max(min_sbert_thresh, current_sbert_thresh - thresh_decay)\n",
    "                if verbose: print(f\"   Lowering threshold to {current_sbert_thresh:.3f} for next try.\")\n",
    "                continue # Try again with lower threshold\n",
    "            else:\n",
    "                if verbose: print(f\"M2 Iter {iteration+1}: Stopping because LLM found no evidence after two tries.\")\n",
    "                break\n",
    "        else:\n",
    "            if verbose: print(f\"M2 Iter {iteration+1}: LLM selected {len(selected_sentences_text)} sentences.\")\n",
    "            # Re-associate selected text with [title, id] using near_match\n",
    "            for llm_sent in selected_sentences_text:\n",
    "                best_match_key = None\n",
    "                highest_sim = -1.0\n",
    "                # Find the best match among the candidates sent to the LLM this iteration\n",
    "                for title, sent_id, orig_text in current_iter_candidate_details:\n",
    "                    key = (title, sent_id)\n",
    "                    # Skip if this candidate was already selected in this iteration by a previous LLM sentence match\n",
    "                    # Or if it was selected in a *previous* iteration\n",
    "                    if key in selected_candidate_keys:\n",
    "                        continue\n",
    "\n",
    "                    # Use near_match to compare LLM output with original sBERT candidate text\n",
    "                    similarity = len(set(llm_sent.lower().split()).intersection(set(orig_text.lower().split()))) / len(set(llm_sent.lower().split()).union(set(orig_text.lower().split())))\n",
    "\n",
    "                    # Using exact match or near_match for robustness\n",
    "                    if near_match(llm_sent, orig_text, threshold=near_match_thresh, verbose=debug>1): # Use near match\n",
    "                         # Crude way to find the 'best' match if multiple near-matches exist\n",
    "                         if similarity > highest_sim:\n",
    "                              highest_sim = similarity\n",
    "                              best_match_key = key\n",
    "\n",
    "                if best_match_key:\n",
    "                    if best_match_key not in selected_candidate_keys:\n",
    "                        final_evidence_ids.append([best_match_key[0], best_match_key[1]]) # Store [title, id]\n",
    "                        #######################################################################################\n",
    "                        # NOTE: NOT IMPLEMENTED FOR TESTING—THROWAWAY IDEA\n",
    "                        ### NOTE: WE ARE ADDING 1 TO EACH SENTENCE INDEX. This is due to observation alone: A few (<40% of the predicted evidence items have the correct page title but the sentence ID is one fewer)\n",
    "                        ### THIS MAY BE VERY CONSEQUENTIAL\n",
    "\n",
    "                        #######################################################################################\n",
    "                        selected_candidate_keys.add(best_match_key) # Mark as selected\n",
    "                        num_selected_this_iter += 1\n",
    "                        if verbose > 1: print(f\"   Matched: '{llm_sent[:50]}...' -> {best_match_key}\")\n",
    "                        if len(final_evidence_ids) >= max_evidence:\n",
    "                            break # Stop if max evidence reached during re-association\n",
    "                    # else: (already selected) - do nothing\n",
    "                else:\n",
    "                    if verbose: print(f\"   Warning: Could not re-associate LLM output with retrieved data: '{llm_sent[:80]}...'\")\n",
    "\n",
    "\n",
    "        # 4. Dynamic Threshold Adjustment (Based on LLM selection)\n",
    "        if num_selected_this_iter < len(new_candidates_for_llm) / 4 and len(new_candidates_for_llm) > 0: # Example: If LLM selected less than 25% of candidates\n",
    "            if current_sbert_thresh > min_sbert_thresh:\n",
    "                current_sbert_thresh = max(min_sbert_thresh, current_sbert_thresh - thresh_decay)\n",
    "                if verbose: print(f\"M2 Iter {iteration+1}: LLM selected few items ({num_selected_this_iter}). Lowering sBERT threshold to {current_sbert_thresh:.3f}\")\n",
    "        # Optional: Increase threshold slightly if LLM selects almost everything? (Less common)\n",
    "        elif num_selected_this_iter > len(new_candidates_for_llm) * 0.8:\n",
    "            current_sbert_thresh = min(initial_sbert_thresh, current_sbert_thresh + thresh_decay / 2)\n",
    "            if verbose: print(f\"   LLM selected many items. Slightly increasing threshold to {current_sbert_thresh:.3f}\")\n",
    "\n",
    "        if debug:\n",
    "            print(f\"DEBUG 2. End Iter {iteration+1}: Total evidence found: {len(final_evidence_ids)}\")\n",
    "            print(\"-_-\" * 10)\n",
    "\n",
    "\n",
    "    # Final Status and Report\n",
    "    status = \"OK\" if final_evidence_ids else \"NOT ENOUGH INFO\"\n",
    "    if not final_evidence_ids and verbose:\n",
    "        print(\"M2: Finished iterations. No evidence selected.\")\n",
    "        print(\"-------------------------------------------------------------------\\n\")\n",
    "    elif verbose:\n",
    "        print(f\"M2: Finished. Selected {len(final_evidence_ids)} evidence items.\")\n",
    "        print(\"-------------------------------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "    # Store all sentences found by sbert (for analysis) and selected ones\n",
    "    # Need to retrieve text for selected IDs for the report\n",
    "    all_sbert_sentences_text = [details[2] for details in all_sbert_candidates_map.values()]\n",
    "    selected_evidence_texts = [all_sbert_candidates_map[key][2] for key in selected_candidate_keys if key in all_sbert_candidates_map]\n",
    "\n",
    "    report = {\n",
    "        \"claim\": claim,\n",
    "        \"final_evidence_ids\": final_evidence_ids, # [[title, id], ...]\n",
    "        \"selected_evidence_texts\": selected_evidence_texts, # List of text for selected evidence\n",
    "        \"status\": status,\n",
    "        \"iterations_run\": iteration + 1,\n",
    "        \"max_evidence\": max_evidence,\n",
    "        \"max_iterations\": max_iterations,\n",
    "        \"mod_2_total_documents\": len(documents),\n",
    "        \"sbert_total_sentences\": len(all_sbert_candidates_map),\n",
    "        \"sbert_total_tokens\": sbert_total_tokens,\n",
    "        \"initial_sbert_thresh\": initial_sbert_thresh,\n",
    "        \"final_sbert_threshold\": current_sbert_thresh,\n",
    "        \"min_sbert_thresh\": min_sbert_thresh,\n",
    "        \"thresh_decay\": thresh_decay,\n",
    "        # \"all_sbert_candidates_text\": all_sbert_sentences_text, # Can be large\n",
    "        \"llm_total_sentences\": llm_total_sentences,\n",
    "        \"llm_total_tokens\": llm_total_tokens,\n",
    "        \"near_match_thresh\": near_match_thresh,\n",
    "    }\n",
    "\n",
    "    return final_evidence_ids, status, report\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zElKAY14cldY"
   },
   "source": [
    "## 4. Module 3: Claim Classification\n",
    "<a id=\"4\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 88,
     "status": "ok",
     "timestamp": 1746373723360,
     "user": {
      "displayName": "Henry Zelenak",
      "userId": "01809909909045225068"
     },
     "user_tz": 240
    },
    "id": "jyGTtn5kcldY"
   },
   "outputs": [],
   "source": [
    "# --- Module 3: Classification (Updated) ---\n",
    "nli_client = OpenAI(api_key=api_key)\n",
    "def module_3_classification(claim, evidence_texts, nli_client_temp=0.1, verbose=0, debug=False):\n",
    "    \"\"\"\n",
    "    **UPDATED:** Classifies the claim based on the TEXT of the extracted evidence sentences.\n",
    "\n",
    "    Args:\n",
    "        claim (str): The input claim.\n",
    "        evidence_texts (list of str): List of extracted evidence sentence texts.\n",
    "        verbose (int): Verbosity level.\n",
    "        debug (bool): Enable debug printing.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (str, str, str):\n",
    "                 - classification_result: \"SUPPORTS\", \"REFUTES\", or \"NOT ENOUGH INFO\".\n",
    "                 - exit_status: \"OK\" or \"NOT ENOUGH INFO\".\n",
    "                 - prompt: The prompt used for classification.\n",
    "    \"\"\"\n",
    "\n",
    "    if verbose: print(\"###### M3: Starting Classification ######\")\n",
    "\n",
    "    if not evidence_texts:\n",
    "        if verbose: print(\"M3: No evidence text provided. Classifying as NOT ENOUGH INFO.\")\n",
    "        # Return structure consistent with LLM call but without making one\n",
    "        return \"NOT ENOUGH INFO\", \"NOT ENOUGH INFO\", \"No evidence provided to LLM.\"\n",
    "\n",
    "    # Format evidence for the prompt\n",
    "    formatted_evidence = \"\\n\".join([f\"- {e}\" for e in evidence_texts])\n",
    "    if not formatted_evidence: # Handle case where list might contain only empty strings\n",
    "         if verbose: print(\"M3: Evidence text list was empty or contained only whitespace. Classifying as NOT ENOUGH INFO.\")\n",
    "         return \"NOT ENOUGH INFO\", \"NOT ENOUGH INFO\", \"Evidence text was empty.\"\n",
    "\n",
    "\n",
    "    # 3.1 Prompt\n",
    "    base_prompt = f\"Based ONLY on the following evidence sentences, classify the claim as SUPPORTS, REFUTES, or NOT ENOUGH INFO.\\n\\nClaim: '{claim}'\\n\\nEvidence:\\n{formatted_evidence}\\n\\nRespond ONLY with SUPPORTS, REFUTES, or NOT ENOUGH INFO.\"\n",
    "    ft_prompt = f\"Given the claim, classify the stance of the potentially relevant evidence out of the following categories: '1' (if the claim is supported by the evidence), '0' (if the claim is refuted by the evidence), '2' (if you do not have enough info to make a confident decision). Respond with a single digit label. Do not use any other labels.\\n\\nClaim: '{claim}'\\n\\nEvidence:\\n{formatted_evidence}\"\n",
    "\n",
    "    if debug:\n",
    "        print(f\"DEBUG 3.1 (module_3_classification):\")\n",
    "        print(f\"\\tClaim: {claim}\")\n",
    "        print(f\"\\tEvidence Texts Sent ({len(evidence_texts)}):\")\n",
    "        # for i, txt in enumerate(evidence_texts): print(f\"\\t  {i+1}. {txt[:100]}...\") # Print snippet\n",
    "        print(f\"\\tPrompt (partial): {base_prompt[:200]}...\")\n",
    "        print(\"-_-\" * 10)\n",
    "\n",
    "    # 3.2 Classification Call\n",
    "    try:\n",
    "        response = nli_client.chat.completions.create(\n",
    "            model='gpt-4o-mini', # Use specified model\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a claim classification assistant. Respond only with 'SUPPORTS', 'REFUTES', or 'NOT ENOUGH INFO'.\"},\n",
    "                {\"role\": \"user\", \"content\": base_prompt},\n",
    "            ],\n",
    "            max_tokens=10,  # Classification is short\n",
    "            n=1,\n",
    "            stop=None,\n",
    "            temperature=nli_client_temp, # Low temperature for classification\n",
    "        )\n",
    "        classification_result = response.choices[0].message.content.strip().upper() # Normalize output\n",
    "\n",
    "        # For the fine tuned model that outputs encoded labels\n",
    "        \"\"\"\n",
    "        # Convert numerical labels to text\n",
    "        if classification_result == \"1\":\n",
    "            classification_result = \"SUPPORTS\"\n",
    "        elif classification_result == \"0\":\n",
    "            classification_result = \"REFUTES\"\n",
    "        elif classification_result == \"2\":\n",
    "            classification_result = \"NOT ENOUGH INFO\"\n",
    "        else:\n",
    "            print(f\"Warning: Module 3 LLM returned invalid label '{classification_result}'. Defaulting to NOT ENOUGH INFO.\")\n",
    "            classification_result = \"NOT ENOUGH INFO\"\n",
    "        \"\"\"\n",
    "        # Validate output\n",
    "        valid_labels = [\"SUPPORTS\", \"REFUTES\", \"NOT ENOUGH INFO\"]\n",
    "        if classification_result not in valid_labels:\n",
    "             print(f\"Warning: Module 3 LLM returned invalid label '{classification_result}'. Defaulting to NOT ENOUGH INFO.\")\n",
    "             classification_result = \"NOT ENOUGH INFO\"\n",
    "\n",
    "    except Exception as e:\n",
    "         print(f\"Error during Module 3 classification LLM call: {e}\")\n",
    "         classification_result = \"NOT ENOUGH INFO\" # Default on error\n",
    "\n",
    "\n",
    "    # 3.3 Exit Status\n",
    "    exit_status = \"OK\" if classification_result in [\"SUPPORTS\", \"REFUTES\"] else \"NOT ENOUGH INFO\"\n",
    "\n",
    "    if debug:\n",
    "      print(f\"DEBUG 3.2/3.3 (module_3_classification):\")\n",
    "      print(f\"\\tLLM Classification Result: {classification_result}\")\n",
    "      print(f\"\\tExit Status: {exit_status}\")\n",
    "      print(\"-_-\" * 10)\n",
    "\n",
    "    return classification_result, exit_status, base_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X59R90ZCcldY"
   },
   "source": [
    "## 5. Module 0: System Control & Execution\n",
    "<a id=\"5\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 50,
     "status": "ok",
     "timestamp": 1746373723416,
     "user": {
      "displayName": "Henry Zelenak",
      "userId": "01809909909045225068"
     },
     "user_tz": 240
    },
    "id": "1BMqmrX7cldY"
   },
   "outputs": [],
   "source": [
    "# --- Helper to get test claim from paper_test.jsonl ---\n",
    "global_test_data_index = 0\n",
    "\n",
    "def get_test_claim(test_data_list, verbose=0, debug=False):\n",
    "    \"\"\"\n",
    "    **UPDATED:** Gets the next claim from the loaded paper_test.jsonl data.\n",
    "\n",
    "    Args:\n",
    "        test_data_list (list): The list loaded from paper_test.jsonl.\n",
    "        verbose (int): Verbosity level.\n",
    "        debug (bool): Enable debug printing.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (claim_id, claim_text) or (None, None) if index is out of bounds.\n",
    "    \"\"\"\n",
    "    global global_test_data_index\n",
    "    if global_test_data_index >= len(test_data_list):\n",
    "        print(\"Reached end of test data.\")\n",
    "        return None, None # Signal end\n",
    "\n",
    "    item = test_data_list[global_test_data_index]\n",
    "    claim_id = item.get(\"id\")\n",
    "    claim_text = item.get(\"claim\")\n",
    "\n",
    "    if claim_id is None or claim_text is None:\n",
    "        print(f\"Warning: Skipping item at index {global_test_data_index} due to missing 'id' or 'claim'. Item: {item}\")\n",
    "        global_test_data_index += 1\n",
    "        return get_test_claim(test_data_list, verbose, debug) # Recursively get next\n",
    "\n",
    "    if verbose > 1:\n",
    "        print(f\"Getting Test Claim {global_test_data_index + 1}/{len(test_data_list)}: ID={claim_id}, Claim='{claim_text[:100]}...'\")\n",
    "\n",
    "    global_test_data_index += 1 # Increment for next call\n",
    "    return claim_id, claim_text\n",
    "\n",
    "# --- Main System Control Flow (Updated) ---\n",
    "\n",
    "def module_0_sys_control(test_items_list, test_size, initial_sbert_thresh, min_sbert_thresh, thresh_decay, max_evidence, max_iterations, near_match_thresh, max_pages_to_fetch, num_search_results, query_client_temp, rephrase_client_temp, sentEx_client_temp, nli_client_temp, disambiguate_client_temp, verbose=0, debug=False):\n",
    "    \"\"\"\n",
    "    **UPDATED:** Orchestrates the full fact-checking pipeline for test data.\n",
    "\n",
    "    Args:\n",
    "        test_items_list (list): List of dicts loaded from paper_test.jsonl.\n",
    "        test_size (int): Number of items to process from the list.\n",
    "        verbose (int): Verbosity level.\n",
    "        debug (bool): Enable detailed debug printing.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (list, pd.DataFrame):\n",
    "                 - predictions_list: List of prediction dicts for output/scoring.\n",
    "                 - run_report_df: DataFrame containing detailed logs for each claim.\n",
    "    \"\"\"\n",
    "    global global_test_data_index\n",
    "    global_test_data_index = 0 # Reset index at the start of a run\n",
    "\n",
    "    predictions_list = [] # Stores final formatted predictions\n",
    "    report_columns = [\n",
    "        'id', 'claim', 'time_to_check', 'entities', 'keywords', 'retrieved_pages',\n",
    "        'module2_status', 'predicted_evidence_ids', 'predicted_evidence_texts',\n",
    "        'module3_result', 'module3_status', 'module3_prompt', 'module1_report_details',\n",
    "        'module2_report_details' # Store the nested report dict from M2\n",
    "    ]\n",
    "    run_report_list = [] # Collect data for DataFrame\n",
    "\n",
    "    # Store original documents fetched by Module 1 for later lookup\n",
    "    # Useful for getting text for Module 3 without re-fetching/storing large texts repeatedly\n",
    "    document_store = {} # { page_title: text }\n",
    "\n",
    "    # Limit processing to test_size or available data\n",
    "    actual_test_size = min(test_size, len(test_items_list))\n",
    "    if actual_test_size <= 0:\n",
    "         print(\"Error: No test data items to process.\")\n",
    "         return [], pd.DataFrame(columns=report_columns)\n",
    "\n",
    "    print(f\"Starting system control. Processing {actual_test_size} test claims...\")\n",
    "\n",
    "    for i in tqdm(range(actual_test_size), desc=\"Processing Claims\"):\n",
    "        # 0. Start timer\n",
    "        start_time = time.time()\n",
    "\n",
    "        # 1. Get Claim from Test Data\n",
    "        claim_id, claim = get_test_claim(test_items_list, verbose=verbose, debug=debug)\n",
    "        if claim_id is None: # Reached end or error\n",
    "            break\n",
    "\n",
    "        if verbose: print(f\"\\n--- Processing Claim ID: {claim_id} ---\")\n",
    "        if verbose > 1: print(f\"Claim: {claim}\")\n",
    "\n",
    "        document_store.clear() # Clear store for new claim\n",
    "\n",
    "        # 2. Extract Entities & Keywords\n",
    "        entities = extract_entities(claim)\n",
    "        keywords = extract_keywords(claim)\n",
    "        if verbose > 1: print(f\"Entities: {entities}, Keywords: {keywords}\")\n",
    "\n",
    "        # --- Module 1 ---\n",
    "\n",
    "        # 3. Generate Potential Page Titles\n",
    "        potential_titles = query_generator(claim, keywords, entities, max_pages_to_fetch, query_client_temp, debug=debug)\n",
    "\n",
    "        # 4. Retrieve Documents\n",
    "        if verbose: print(\"###### M1: Retrieving Documents ######\")\n",
    "        documents, doc_sources, total_document_tokens, mod_1_total_documents = retrieve_documents_from_wikipedia(potential_titles, claim, entities, num_search_results, disambiguate_client_temp, debug=debug)\n",
    "        retrieved_pages_str = \", \".join(doc_sources) if doc_sources else \"None\"\n",
    "\n",
    "        # Store fetched documents for Module 3 lookup\n",
    "        for title, text in zip(doc_sources, documents):\n",
    "            document_store[title] = text\n",
    "\n",
    "        # Create the Module 1 report\n",
    "        module_1_report = {\n",
    "            \"mod_1_total_pages\": mod_1_total_documents,\n",
    "            \"total_document_tokens\": total_document_tokens,\n",
    "            \"potential_titles\": potential_titles,\n",
    "            \"retrieved_titles\": retrieved_pages_str\n",
    "        }\n",
    "\n",
    "        if not documents:\n",
    "            if verbose: print(\"M1: No documents retrieved. Cannot proceed.\\n-------------------------------------------------------\\n\")\n",
    "            # Handle case with no documents: classify as NEI directly\n",
    "            predicted_evidence_ids = []\n",
    "            predicted_evidence_texts = []\n",
    "            classification_result = \"NOT ENOUGH INFO\"\n",
    "            mod2_status = \"NOT ENOUGH INFO\"\n",
    "            mod3_status = \"NOT ENOUGH INFO\"\n",
    "            mod3_prompt = \"Skipped - No documents from M1\"\n",
    "            mod2_report_details = {\"status\": \"Skipped - No documents from M1\"}\n",
    "        else:\n",
    "            # --- Module 2 ---\n",
    "            # 5. Extract Evidence Sentences ([title, id])\n",
    "            # Use appropriate thresholds\n",
    "            predicted_evidence_ids, mod2_status, mod2_report = module_2_2_controls(\n",
    "                claim, documents, doc_sources, entities, keywords,\n",
    "                initial_sbert_thresh=initial_sbert_thresh, # Slightly higher initial threshold?\n",
    "                min_sbert_thresh=min_sbert_thresh,\n",
    "                thresh_decay=thresh_decay,\n",
    "                max_evidence=max_evidence,\n",
    "                max_iterations=max_iterations,\n",
    "                near_match_thresh=near_match_thresh,\n",
    "                rephrs_temp=rephrase_client_temp,\n",
    "                sentEx_temp=sentEx_client_temp,\n",
    "                verbose=verbose,\n",
    "                debug=debug\n",
    "            )\n",
    "            predicted_evidence_texts = mod2_report.get(\"selected_evidence_texts\", [])\n",
    "            mod2_report_details = mod2_report # Store the whole M2 report\n",
    "\n",
    "            # --- Module 3 ---\n",
    "            # 6. Classify Claim based on evidence TEXT\n",
    "            classification_result, mod3_status, mod3_prompt = module_3_classification(\n",
    "                claim,\n",
    "                predicted_evidence_texts, # Pass the actual text\n",
    "                nli_client_temp=nli_client_temp,\n",
    "                verbose=verbose,\n",
    "                debug=debug\n",
    "            )\n",
    "\n",
    "        # 7. Format Output for FEVER Scorer / Final JSON\n",
    "        # 7.1 Encode the brackets (-LRB-, -RRB-, -LSB-, -RSB-) and replace spaces with underscores for each page title\n",
    "        bracket_mapping = {\n",
    "            \"(\": \"-LRB-\",\n",
    "            \")\": \"-RRB-\",\n",
    "            \"[\": \"-LSB-\",\n",
    "            \"]\": \"-RSB-\"\n",
    "        }\n",
    "        for item in predicted_evidence_ids:\n",
    "            # Encode the page title like the test set (brackets and unerscores)\n",
    "            item[0] = \"\".join(bracket_mapping.get(c, c) for c in item[0])\n",
    "            item[0] = item[0].replace(\" \", \"_\")\n",
    "\n",
    "        prediction_item = {\n",
    "            \"id\": claim_id,\n",
    "            \"predicted_label\": classification_result,\n",
    "            \"predicted_evidence\": predicted_evidence_ids # List of [page_title, sentence_id]\n",
    "        }\n",
    "        # The FEVER scorer needs gold labels/evidence\n",
    "        # We add dummy fields here if there is no gold evidence.\n",
    "        if \"label\" in test_items_list[i]: # Check if gold data exists\n",
    "            if verbose: print(\"Adding gold label/evidence to prediction.\")\n",
    "            prediction_item[\"label\"] = test_items_list[i][\"label\"]\n",
    "            # Set th efirst two items of each inner list of test_items_list to None and exclude duplicates\n",
    "            unique_evdc_items = []\n",
    "            for inner_list in test_items_list[i][\"evidence\"]:\n",
    "                if inner_list not in unique_evdc_items:\n",
    "                    unique_evdc_items.append(inner_list)\n",
    "            for evidence_set in test_items_list[i][\"evidence\"]:\n",
    "                for item in evidence_set:\n",
    "                    item[0] = None\n",
    "                    item[1] = None\n",
    "            if test_items_list[i][\"label\"] == \"NOT ENOUGH INFO\":\n",
    "                unique_evdc_items = []\n",
    "            prediction_item[\"evidence\"] = unique_evdc_items\n",
    "            if verbose: print(\"##########################################################################\\n\")\n",
    "        else:\n",
    "            if verbose: print(\"Adding dummy gold label/evidence to prediction because gold data is missing.\")\n",
    "            # Add placeholder fields if running the scorer function is desired,\n",
    "            # otherwise, these can be omitted if just generating predictions.\n",
    "            prediction_item[\"label\"] = \"NOT ENOUGH INFO\" # Dummy\n",
    "            prediction_item[\"evidence\"] = [] # Dummy\n",
    "        if verbose: print(\"##########################################################################\\n\")\n",
    "\n",
    "        predictions_list.append(prediction_item)\n",
    "\n",
    "        time_to_check = time.time() - start_time\n",
    "        if verbose: print(f\"Time to process claim: {time_to_check:.2f} seconds.\\n------------------------------------------------\\n\")\n",
    "\n",
    "        # 8. Log Run Details\n",
    "        run_report_list.append({\n",
    "            'id': claim_id,\n",
    "            'claim': claim,\n",
    "            'time_to_check': time_to_check,\n",
    "            'entities': \", \".join(entities) if entities else \"\",\n",
    "            'keywords': \", \".join(keywords) if keywords else \"\",\n",
    "            'max_pages_to_fetch': max_pages_to_fetch,\n",
    "            'max_search_results': num_search_results,\n",
    "            'retrieved_page_titles': retrieved_pages_str,\n",
    "            'module2_status': mod2_status,\n",
    "            'predicted_evidence_ids': json.dumps(predicted_evidence_ids), # Store as JSON string\n",
    "            'predicted_evidence_texts': json.dumps(predicted_evidence_texts), # Store as JSON string\n",
    "            'module3_result': classification_result,\n",
    "            'module3_status': mod3_status,\n",
    "            'module3_prompt': mod3_prompt,\n",
    "            'module1_report_details': json.dumps(module_1_report), # Store M1 report dict as JSON string\n",
    "            'module2_report_details': json.dumps(mod2_report_details) # Store M2 report dict as JSON string\n",
    "        })\n",
    "\n",
    "        # Optional: Garbage collect periodically if memory usage is high\n",
    "        if i % 50 == 0:\n",
    "            gc.collect()\n",
    "\n",
    "    print(f\"\\nFinished processing {len(predictions_list)} claims.\")\n",
    "    run_report_df = pd.DataFrame(run_report_list, columns=report_columns)\n",
    "    return predictions_list, run_report_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1331543,
     "status": "ok",
     "timestamp": 1746375054961,
     "user": {
      "displayName": "Henry Zelenak",
      "userId": "01809909909045225068"
     },
     "user_tz": 240
    },
    "id": "31-EqN8iztRa",
    "outputId": "b1a5eb21-443d-4be9-f907-83e77e298569"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting system control. Processing 30 test claims...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing Claims:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Claim ID: 113501 ---\n",
      "DEBUG 1.0 (query_generator):\n",
      "\tClaim: Grease had bad reviews.\n",
      "\tEntities: ['Grease']\n",
      "\tLLM Output: ['grease_(film)', 'critical_reception', 'list_of_films_with_lowest_ratings', 'rotten_tomatoes', 'metacritic']\n",
      "-_--_--_--_--_-\n",
      "DEBUG 1.1 (query_generator):\n",
      "\tEntities: ['Grease']\n",
      "\tGenerated Potential Titles: ['metacritic', 'grease_(film)', 'rotten_tomatoes', 'critical_reception', 'list_of_films_with_lowest_ratings']\n",
      "\tSelected Titles for Retrieval: ['metacritic', 'grease_(film)', 'rotten_tomatoes', 'critical_reception', 'list_of_films_with_lowest_ratings']\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "###### M1: Retrieving Documents ######\n",
      "DEBUG 1.2: Successfully retrieved intro from 'List of video games notable for negative reception' (searched for 'metacritic')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.11/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG 1.2: DisambiguationError for title: 'grease_(film)'. Options: ['Grease (lubricant)', 'petroleum', 'Brown grease', 'Yellow grease', 'Hydrogenated vegetable oil', 'Vegetable shortening', 'bribe', 'killing', 'Pomade', 'Grease (musical)', 'Grease (film)', 'Grease 2', '\"Grease\" (song)', '1971 musical play', 'Grease: The Original Soundtrack from the Motion Picture', 'Grease: The New Broadway Cast Recording (2007 album)', 'Grease: Live', \"Grease: You're the One that I Want!\", 'Grease is the Word', 'Extreme Ghostbusters episode 14', 'The Keith & Paddy Picture Show season 2, episode 1', 'Grease (franchise)', 'Grease (video game)', 'Mud fever', 'Aglossa cuprina', 'Grease (networking)', 'All pages with titles beginning with Grease ', 'All pages with titles containing Grease', 'Greaser (disambiguation)', 'Greasy (disambiguation)', 'Greece (disambiguation)']...\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Grease (film)' (disambiguated to 'Grease (film)')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'List of films with a 0% rating on Rotten Tomatoes' (searched for 'rotten_tomatoes')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Review' (searched for 'critical_reception')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'List of films considered the worst' (searched for 'list_of_films_with_lowest_ratings')\n",
      "DEBUG 1.2: Retrieved content for 5 pages out of 5 potential titles.\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M2: Starting Evidence Extraction ######\n",
      "DEBUG 2.1 (module_2_controls):\n",
      "\tClaim: Grease had bad reviews.\n",
      "\tEntities: ['Grease']\n",
      "\tKeywords: ['reviews', 'grease', 'bad']\n",
      "\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n",
      "\tMax Evidence Target: 8, Max Iterations: 10\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 18\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 1: Sending 18 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "1. The film was successful both critically and commercially, becoming the highest-grossing musical film at the time. Grease (film)\n",
      "2. The film also received five nominations at the 36th Golden Globe Awards, including for Best Motion Picture – Musical or Comedy and two for Best Original Song, for \"Grease\" and \"You're the One that I Want\". Grease (film)\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 1: LLM selected 2 sentences.\n",
      "M2 Iter 1: LLM selected few items (2). Lowering sBERT threshold to 0.350\n",
      "DEBUG 2. End Iter 1: Total evidence found: 2\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: Grease received unfavorable reviews.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 19\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 2: Sending 17 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 2: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "   Lowering threshold to 0.300 for next try.\n",
      "DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: Grease received negative feedback.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 19\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 3: Sending 17 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 3: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "M2 Iter 3: Stopping because LLM found no evidence after two tries.\n",
      "M2: Finished. Selected 2 evidence items.\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M3: Starting Classification ######\n",
      "DEBUG 3.1 (module_3_classification):\n",
      "\tClaim: Grease had bad reviews.\n",
      "\tEvidence Texts Sent (2):\n",
      "\tPrompt (partial): Based ONLY on the following evidence sentences, classify the claim as SUPPORTS, REFUTES, or NOT ENOUGH INFO.\n",
      "\n",
      "Claim: 'Grease had bad reviews.'\n",
      "\n",
      "Evidence:\n",
      "- The film was successful both critically and ...\n",
      "-_--_--_--_--_--_--_--_--_--_-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing Claims:   3%|▎         | 1/30 [00:44<21:22, 44.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG 3.2/3.3 (module_3_classification):\n",
      "\tLLM Classification Result: REFUTES\n",
      "\tExit Status: OK\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "Adding gold label/evidence to prediction.\n",
      "##########################################################################\n",
      "\n",
      "##########################################################################\n",
      "\n",
      "Time to process claim: 43.82 seconds.\n",
      "------------------------------------------------\n",
      "\n",
      "\n",
      "--- Processing Claim ID: 163803 ---\n",
      "DEBUG 1.0 (query_generator):\n",
      "\tClaim: Ukrainian Soviet Socialist Republic was a founding participant of the UN.\n",
      "\tEntities: ['Socialist Republic', 'Ukrainian', 'Soviet', 'UN']\n",
      "\tLLM Output: ['ukrainian_soviet_socialist_republic', 'founding_members_of_the_un', 'united_nations', 'history_of_the_united_nations', 'soviet_union']\n",
      "-_--_--_--_--_-\n",
      "DEBUG 1.1 (query_generator):\n",
      "\tEntities: ['Socialist Republic', 'Ukrainian', 'Soviet', 'UN']\n",
      "\tGenerated Potential Titles: ['soviet_union', 'united_nations', 'founding_members_of_the_un', 'history_of_the_united_nations', 'ukrainian_soviet_socialist_republic']\n",
      "\tSelected Titles for Retrieval: ['soviet_union', 'united_nations', 'founding_members_of_the_un', 'history_of_the_united_nations', 'ukrainian_soviet_socialist_republic']\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "###### M1: Retrieving Documents ######\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Republics of the Soviet Union' (searched for 'soviet_union')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Member states of the United Nations' (searched for 'united_nations')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'International membership of Ukraine' (searched for 'founding_members_of_the_un')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'History of the United Nations' (searched for 'history_of_the_united_nations')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Ukrainian People's Republic of Soviets' (searched for 'ukrainian_soviet_socialist_republic')\n",
      "DEBUG 1.2: Retrieved content for 5 pages out of 5 potential titles.\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M2: Starting Evidence Extraction ######\n",
      "DEBUG 2.1 (module_2_controls):\n",
      "\tClaim: Ukrainian Soviet Socialist Republic was a founding participant of the UN.\n",
      "\tEntities: ['Socialist Republic', 'Ukrainian', 'Soviet', 'UN']\n",
      "\tKeywords: ['ukrainian', 'soviet', 'socialist', 'republic', 'participant']\n",
      "\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n",
      "\tMax Evidence Target: 8, Max Iterations: 10\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 27\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 1: Sending 27 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "3. The Soviet Union was formed in 1922 by a treaty between the Soviet republics of Byelorussia, Russian SFSR (RSFSR), Transcaucasian Federation, and Ukraine, by which they became its constituent republics of the Union of Soviet Socialist Republics (Soviet Union). Republics of the Soviet Union\n",
      "\n",
      "5. As a result of its status as a sovereign state, the Union Republic de jure had the right to enter into relations with foreign states, conclude treaties with them and exchange diplomatic and consular representatives and participate in the activities of international organizations (including membership in international organizations). Republics of the Soviet Union\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 1: LLM selected 2 sentences.\n",
      "M2 Iter 1: LLM selected few items (2). Lowering sBERT threshold to 0.350\n",
      "DEBUG 2. End Iter 1: Total evidence found: 2\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: The Ukrainian Soviet Socialist Republic was one of the original members of the UN.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 27\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 2: Sending 25 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 2: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "   Lowering threshold to 0.300 for next try.\n",
      "DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: The Ukrainian Soviet Socialist Republic played a key role as one of the founding members of the UN.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 27\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 3: Sending 25 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 3: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "M2 Iter 3: Stopping because LLM found no evidence after two tries.\n",
      "M2: Finished. Selected 2 evidence items.\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M3: Starting Classification ######\n",
      "DEBUG 3.1 (module_3_classification):\n",
      "\tClaim: Ukrainian Soviet Socialist Republic was a founding participant of the UN.\n",
      "\tEvidence Texts Sent (2):\n",
      "\tPrompt (partial): Based ONLY on the following evidence sentences, classify the claim as SUPPORTS, REFUTES, or NOT ENOUGH INFO.\n",
      "\n",
      "Claim: 'Ukrainian Soviet Socialist Republic was a founding participant of the UN.'\n",
      "\n",
      "Eviden...\n",
      "-_--_--_--_--_--_--_--_--_--_-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing Claims:   7%|▋         | 2/30 [01:30<21:19, 45.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG 3.2/3.3 (module_3_classification):\n",
      "\tLLM Classification Result: NOT ENOUGH INFO\n",
      "\tExit Status: NOT ENOUGH INFO\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "Adding gold label/evidence to prediction.\n",
      "##########################################################################\n",
      "\n",
      "##########################################################################\n",
      "\n",
      "Time to process claim: 46.74 seconds.\n",
      "------------------------------------------------\n",
      "\n",
      "\n",
      "--- Processing Claim ID: 70041 ---\n",
      "DEBUG 1.0 (query_generator):\n",
      "\tClaim: 2 Hearts is a musical composition by Minogue.\n",
      "\tEntities: ['Minogue']\n",
      "\tLLM Output: ['2_hearts', 'kylie_minogue', 'musical_composition']\n",
      "-_--_--_--_--_-\n",
      "DEBUG 1.1 (query_generator):\n",
      "\tEntities: ['Minogue']\n",
      "\tGenerated Potential Titles: ['2_hearts', 'kylie_minogue', 'musical_composition']\n",
      "\tSelected Titles for Retrieval: ['2_hearts', 'kylie_minogue', 'musical_composition']\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "###### M1: Retrieving Documents ######\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Two Hearts (Kish Mauve song)' (searched for '2_hearts')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Kylie Minogue' (searched for 'kylie_minogue')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Musical composition' (searched for 'musical_composition')\n",
      "DEBUG 1.2: Retrieved content for 3 pages out of 3 potential titles.\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M2: Starting Evidence Extraction ######\n",
      "DEBUG 2.1 (module_2_controls):\n",
      "\tClaim: 2 Hearts is a musical composition by Minogue.\n",
      "\tEntities: ['Minogue']\n",
      "\tKeywords: ['musical', 'minogue', 'hearts', 'composition']\n",
      "\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n",
      "\tMax Evidence Target: 8, Max Iterations: 10\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 39\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 1: Sending 39 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "\"Two Hearts\", alternatively titled \"2 Hearts\", is a song first recorded by British electronic duo Kish Mauve, written for their 2005 self-titled extended play and later re-recorded by Australian singer Kylie Minogue for her tenth studio album, X (2007). Two Hearts (Kish Mauve song)\n",
      "Minogue's version was released on 9 November 2007 by Parlophone as the album's lead single. Two Hearts (Kish Mauve song)\n",
      "She continued reinventing her image and experimenting with a range of genres on her subsequent albums, which spawned successful singles such as \"Slow\", \"I Believe in You\", \"2 Hearts\" and \"All the Lovers\". Kylie Minogue\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 1: LLM selected 3 sentences.\n",
      "M2 Iter 1: LLM selected few items (3). Lowering sBERT threshold to 0.350\n",
      "DEBUG 2. End Iter 1: Total evidence found: 3\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: 2 Hearts is a song created by Minogue.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 41\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 2: Sending 38 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "5. The song was Minogue's first commercial single since \"Giving You Up\" (2005), as she was diagnosed with breast cancer in May 2005. Two Hearts (Kish Mauve song)\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 2: LLM selected 1 sentences.\n",
      "M2 Iter 2: LLM selected few items (1). Lowering sBERT threshold to 0.300\n",
      "DEBUG 2. End Iter 2: Total evidence found: 4\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: 2 Hearts is a piece of music composed by Minogue.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 51\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 3: Sending 47 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 3: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "M2 Iter 3: Stopping because LLM found no evidence after two tries.\n",
      "M2: Finished. Selected 4 evidence items.\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M3: Starting Classification ######\n",
      "DEBUG 3.1 (module_3_classification):\n",
      "\tClaim: 2 Hearts is a musical composition by Minogue.\n",
      "\tEvidence Texts Sent (4):\n",
      "\tPrompt (partial): Based ONLY on the following evidence sentences, classify the claim as SUPPORTS, REFUTES, or NOT ENOUGH INFO.\n",
      "\n",
      "Claim: '2 Hearts is a musical composition by Minogue.'\n",
      "\n",
      "Evidence:\n",
      "- Minogue's version was ...\n",
      "-_--_--_--_--_--_--_--_--_--_-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing Claims:  10%|█         | 3/30 [02:36<24:34, 54.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG 3.2/3.3 (module_3_classification):\n",
      "\tLLM Classification Result: SUPPORTS\n",
      "\tExit Status: OK\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "Adding gold label/evidence to prediction.\n",
      "##########################################################################\n",
      "\n",
      "##########################################################################\n",
      "\n",
      "Time to process claim: 65.18 seconds.\n",
      "------------------------------------------------\n",
      "\n",
      "\n",
      "--- Processing Claim ID: 202314 ---\n",
      "DEBUG 1.0 (query_generator):\n",
      "\tClaim: The New Jersey Turnpike has zero shoulders.\n",
      "\tEntities: ['New Jersey Turnpike']\n",
      "\tLLM Output: ['new_jersey_turnpike', 'highway_safety', 'road_design', 'traffic_engineering', 'roadway_geometry']\n",
      "-_--_--_--_--_-\n",
      "DEBUG 1.1 (query_generator):\n",
      "\tEntities: ['New Jersey Turnpike']\n",
      "\tGenerated Potential Titles: ['road_design', 'highway_safety', 'roadway_geometry', 'traffic_engineering', 'new_jersey_turnpike']\n",
      "\tSelected Titles for Retrieval: ['road_design', 'highway_safety', 'roadway_geometry', 'traffic_engineering', 'new_jersey_turnpike']\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "###### M1: Retrieving Documents ######\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Highway engineering' (searched for 'road_design')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'National Highway Traffic Safety Administration' (searched for 'highway_safety')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'New Jersey Turnpike' (searched for 'roadway_geometry')\n",
      "DEBUG 1.2: Skipping duplicate intro for 'Highway engineering' (searched for 'traffic_engineering')\n",
      "DEBUG 1.2: Skipping duplicate intro for 'New Jersey Turnpike' (searched for 'new_jersey_turnpike')\n",
      "DEBUG 1.2: Retrieved content for 3 pages out of 5 potential titles.\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M2: Starting Evidence Extraction ######\n",
      "DEBUG 2.1 (module_2_controls):\n",
      "\tClaim: The New Jersey Turnpike has zero shoulders.\n",
      "\tEntities: ['New Jersey Turnpike']\n",
      "\tKeywords: ['zero', 'turnpike', 'shoulders', 'new', 'jersey']\n",
      "\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n",
      "\tMax Evidence Target: 8, Max Iterations: 10\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 17\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 1: Sending 17 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "The turnpike has 12-foot-wide (3.7 m) lanes, 10-foot-wide (3.0 m) shoulders, and 13 of the highway's service areas are named after notable New Jersey residents. New Jersey Turnpike\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 1: LLM selected 1 sentences.\n",
      "M2 Iter 1: LLM selected few items (1). Lowering sBERT threshold to 0.350\n",
      "DEBUG 2. End Iter 1: Total evidence found: 1\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: The New Jersey Turnpike lacks any shoulder space.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 20\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 2: Sending 19 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 2: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "   Lowering threshold to 0.300 for next try.\n",
      "DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: There are no shoulders on the New Jersey Turnpike.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 19\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 3: Sending 18 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 3: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "M2 Iter 3: Stopping because LLM found no evidence after two tries.\n",
      "M2: Finished. Selected 1 evidence items.\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M3: Starting Classification ######\n",
      "DEBUG 3.1 (module_3_classification):\n",
      "\tClaim: The New Jersey Turnpike has zero shoulders.\n",
      "\tEvidence Texts Sent (1):\n",
      "\tPrompt (partial): Based ONLY on the following evidence sentences, classify the claim as SUPPORTS, REFUTES, or NOT ENOUGH INFO.\n",
      "\n",
      "Claim: 'The New Jersey Turnpike has zero shoulders.'\n",
      "\n",
      "Evidence:\n",
      "- The turnpike has 12-foot...\n",
      "-_--_--_--_--_--_--_--_--_--_-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing Claims:  13%|█▎        | 4/30 [03:22<22:15, 51.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG 3.2/3.3 (module_3_classification):\n",
      "\tLLM Classification Result: REFUTES\n",
      "\tExit Status: OK\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "Adding gold label/evidence to prediction.\n",
      "##########################################################################\n",
      "\n",
      "##########################################################################\n",
      "\n",
      "Time to process claim: 46.43 seconds.\n",
      "------------------------------------------------\n",
      "\n",
      "\n",
      "--- Processing Claim ID: 57085 ---\n",
      "DEBUG 1.0 (query_generator):\n",
      "\tClaim: Legendary Entertainment is the owner of Wanda Cinemas.\n",
      "\tEntities: ['Wanda Cinemas']\n",
      "\tLLM Output: ['legendary_entertainment', 'wanda_cinemas', 'legendary_entertainment#ownership', 'wanda_group', 'film_industry']\n",
      "-_--_--_--_--_-\n",
      "DEBUG 1.1 (query_generator):\n",
      "\tEntities: ['Wanda Cinemas']\n",
      "\tGenerated Potential Titles: ['wanda_group', 'film_industry', 'wanda_cinemas', 'legendary_entertainment', 'legendary_entertainment#ownership']\n",
      "\tSelected Titles for Retrieval: ['wanda_group', 'film_industry', 'wanda_cinemas', 'legendary_entertainment', 'legendary_entertainment#ownership']\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "###### M1: Retrieving Documents ######\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Wanda Media' (searched for 'wanda_group')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Cinema of China' (searched for 'film_industry')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Wanda Group' (searched for 'wanda_cinemas')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'List of Legendary Pictures films' (searched for 'legendary_entertainment')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Legendary Entertainment' (searched for 'legendary_entertainment#ownership')\n",
      "DEBUG 1.2: Retrieved content for 5 pages out of 5 potential titles.\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M2: Starting Evidence Extraction ######\n",
      "DEBUG 2.1 (module_2_controls):\n",
      "\tClaim: Legendary Entertainment is the owner of Wanda Cinemas.\n",
      "\tEntities: ['Wanda Cinemas']\n",
      "\tKeywords: ['wanda', 'owner', 'legendary', 'entertainment', 'cinemas']\n",
      "\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n",
      "\tMax Evidence Target: 8, Max Iterations: 10\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 19\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 1: Sending 19 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 1: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "   Lowering threshold to 0.350 for next try.\n",
      "DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: Wanda Cinemas is owned by Legendary Entertainment.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 21\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 2: Sending 21 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 2: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "   Lowering threshold to 0.300 for next try.\n",
      "DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: Wanda Cinemas is under the ownership of Legendary Entertainment.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 25\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 3: Sending 25 new candidates to LLM for selection.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing Claims:  17%|█▋        | 5/30 [03:52<18:13, 43.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 3: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "M2 Iter 3: Stopping because LLM found no evidence after two tries.\n",
      "M2: Finished iterations. No evidence selected.\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M3: Starting Classification ######\n",
      "M3: No evidence text provided. Classifying as NOT ENOUGH INFO.\n",
      "Adding gold label/evidence to prediction.\n",
      "##########################################################################\n",
      "\n",
      "##########################################################################\n",
      "\n",
      "Time to process claim: 30.24 seconds.\n",
      "------------------------------------------------\n",
      "\n",
      "\n",
      "--- Processing Claim ID: 6032 ---\n",
      "DEBUG 1.0 (query_generator):\n",
      "\tClaim: Aruba is the only ABC Island.\n",
      "\tEntities: ['ABC Island', 'Aruba']\n",
      "\tLLM Output: ['aruba', 'abc_islands', 'caribbean', 'islands_of_the_caribbean']\n",
      "-_--_--_--_--_-\n",
      "DEBUG 1.1 (query_generator):\n",
      "\tEntities: ['ABC Island', 'Aruba']\n",
      "\tGenerated Potential Titles: ['aruba', 'caribbean', 'abc_islands', 'islands_of_the_caribbean']\n",
      "\tSelected Titles for Retrieval: ['aruba', 'caribbean', 'abc_islands', 'islands_of_the_caribbean']\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "###### M1: Retrieving Documents ######\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Aruba' (searched for 'aruba')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'List of Caribbean islands' (searched for 'caribbean')\n",
      "DEBUG 1.2: DisambiguationError for title: 'abc_islands'. Options: ['ABC Islands (Alaska)', 'ABC islands (Leeward Antilles)']...\n",
      "DEBUG 1.2: Successfully retrieved intro from 'ABC islands (Leeward Antilles)' (disambiguated to 'ABC islands (Leeward Antilles)')\n",
      "DEBUG 1.2: Skipping duplicate intro for 'List of Caribbean islands' (searched for 'islands_of_the_caribbean')\n",
      "DEBUG 1.2: Retrieved content for 3 pages out of 4 potential titles.\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M2: Starting Evidence Extraction ######\n",
      "DEBUG 2.1 (module_2_controls):\n",
      "\tClaim: Aruba is the only ABC Island.\n",
      "\tEntities: ['ABC Island', 'Aruba']\n",
      "\tKeywords: ['island', 'aruba', 'abc']\n",
      "\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n",
      "\tMax Evidence Target: 8, Max Iterations: 10\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 19\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 1: Sending 19 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "1. Alongside Bonaire and Curaçao, Aruba forms a group referred to as the ABC islands. Aruba\n",
      "2. The ABC islands is the physical group of Aruba, Bonaire, and Curaçao, the three westernmost islands of the Leeward Antilles in the Caribbean Sea. ABC islands (Leeward Antilles)\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 1: LLM selected 2 sentences.\n",
      "M2 Iter 1: LLM selected few items (2). Lowering sBERT threshold to 0.350\n",
      "DEBUG 2. End Iter 1: Total evidence found: 2\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: Aruba stands alone as the sole ABC Island.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 22\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 2: Sending 20 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 2: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "   Lowering threshold to 0.300 for next try.\n",
      "DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: Aruba is the singular ABC Island.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 20\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 3: Sending 18 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 3: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "M2 Iter 3: Stopping because LLM found no evidence after two tries.\n",
      "M2: Finished. Selected 2 evidence items.\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M3: Starting Classification ######\n",
      "DEBUG 3.1 (module_3_classification):\n",
      "\tClaim: Aruba is the only ABC Island.\n",
      "\tEvidence Texts Sent (2):\n",
      "\tPrompt (partial): Based ONLY on the following evidence sentences, classify the claim as SUPPORTS, REFUTES, or NOT ENOUGH INFO.\n",
      "\n",
      "Claim: 'Aruba is the only ABC Island.'\n",
      "\n",
      "Evidence:\n",
      "- Alongside Bonaire and Curaçao, Aruba f...\n",
      "-_--_--_--_--_--_--_--_--_--_-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing Claims:  20%|██        | 6/30 [04:29<16:34, 41.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG 3.2/3.3 (module_3_classification):\n",
      "\tLLM Classification Result: REFUTES\n",
      "\tExit Status: OK\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "Adding gold label/evidence to prediction.\n",
      "##########################################################################\n",
      "\n",
      "##########################################################################\n",
      "\n",
      "Time to process claim: 36.92 seconds.\n",
      "------------------------------------------------\n",
      "\n",
      "\n",
      "--- Processing Claim ID: 176630 ---\n",
      "DEBUG 1.0 (query_generator):\n",
      "\tClaim: Great white sharks do not prefer dolphins as prey.\n",
      "\tEntities: ['Great']\n",
      "\tLLM Output: ['great_white_shark', 'dolphin', 'predation', 'marine_ecosystem', 'shark_behavior', 'food_web', 'great_white_shark_behavior', 'shark_diet']\n",
      "-_--_--_--_--_-\n",
      "DEBUG 1.1 (query_generator):\n",
      "\tEntities: ['Great']\n",
      "\tGenerated Potential Titles: ['dolphin', 'food_web', 'predation', 'shark_diet', 'shark_behavior', 'marine_ecosystem', 'great_white_shark', 'great_white_shark_behavior']\n",
      "\tSelected Titles for Retrieval: ['dolphin', 'food_web', 'predation', 'shark_diet', 'shark_behavior', 'marine_ecosystem', 'great_white_shark', 'great_white_shark_behavior']\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "###### M1: Retrieving Documents ######\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Oceanic dolphin' (searched for 'dolphin')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Marine food web' (searched for 'food_web')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Pursuit predation' (searched for 'predation')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Great white shark' (searched for 'shark_diet')\n",
      "DEBUG 1.2: Skipping duplicate intro for 'Great white shark' (searched for 'shark_behavior')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Marine mammal' (searched for 'marine_ecosystem')\n",
      "DEBUG 1.2: Skipping duplicate intro for 'Great white shark' (searched for 'great_white_shark')\n",
      "DEBUG 1.2: Skipping duplicate intro for 'Great white shark' (searched for 'great_white_shark_behavior')\n",
      "DEBUG 1.2: Retrieved content for 5 pages out of 8 potential titles.\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M2: Starting Evidence Extraction ######\n",
      "DEBUG 2.1 (module_2_controls):\n",
      "\tClaim: Great white sharks do not prefer dolphins as prey.\n",
      "\tEntities: ['Great']\n",
      "\tKeywords: ['white', 'sharks', 'prey', 'prefer', 'great']\n",
      "\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n",
      "\tMax Evidence Target: 8, Max Iterations: 10\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 47\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 1: Sending 47 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "1. The great white shark is arguably the world's largest-known extant macropredatory fish, and is one of the primary predators of marine mammals, such as pinnipeds and dolphins. Great white shark\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 1: LLM selected 1 sentences.\n",
      "M2 Iter 1: LLM selected few items (1). Lowering sBERT threshold to 0.350\n",
      "DEBUG 2. End Iter 1: Total evidence found: 1\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: Great white sharks are not inclined to choose dolphins as their food source.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 63\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 2: Sending 62 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 2: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "   Lowering threshold to 0.300 for next try.\n",
      "DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: Great white sharks are not likely to target dolphins for food.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 74\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 3: Sending 73 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 3: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "M2 Iter 3: Stopping because LLM found no evidence after two tries.\n",
      "M2: Finished. Selected 1 evidence items.\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M3: Starting Classification ######\n",
      "DEBUG 3.1 (module_3_classification):\n",
      "\tClaim: Great white sharks do not prefer dolphins as prey.\n",
      "\tEvidence Texts Sent (1):\n",
      "\tPrompt (partial): Based ONLY on the following evidence sentences, classify the claim as SUPPORTS, REFUTES, or NOT ENOUGH INFO.\n",
      "\n",
      "Claim: 'Great white sharks do not prefer dolphins as prey.'\n",
      "\n",
      "Evidence:\n",
      "- The great white s...\n",
      "-_--_--_--_--_--_--_--_--_--_-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing Claims:  23%|██▎       | 7/30 [05:51<20:54, 54.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG 3.2/3.3 (module_3_classification):\n",
      "\tLLM Classification Result: NOT ENOUGH INFO\n",
      "\tExit Status: NOT ENOUGH INFO\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "Adding gold label/evidence to prediction.\n",
      "##########################################################################\n",
      "\n",
      "##########################################################################\n",
      "\n",
      "Time to process claim: 81.50 seconds.\n",
      "------------------------------------------------\n",
      "\n",
      "\n",
      "--- Processing Claim ID: 130048 ---\n",
      "DEBUG 1.0 (query_generator):\n",
      "\tClaim: Burbank, California has always been completely void of industry.\n",
      "\tEntities: ['California', 'Burbank']\n",
      "\tLLM Output: ['burbank,_california', 'history_of_burbank,_california', 'economy_of_burbank,_california', 'list_of_industries_in_burbank,_california', 'burbank_studios', 'media_industry_in_burbank']\n",
      "-_--_--_--_--_-\n",
      "DEBUG 1.1 (query_generator):\n",
      "\tEntities: ['California', 'Burbank']\n",
      "\tGenerated Potential Titles: ['burbank_studios', 'burbank,_california', 'media_industry_in_burbank', 'economy_of_burbank,_california', 'history_of_burbank,_california', 'list_of_industries_in_burbank,_california']\n",
      "\tSelected Titles for Retrieval: ['burbank_studios', 'burbank,_california', 'media_industry_in_burbank', 'economy_of_burbank,_california', 'history_of_burbank,_california', 'list_of_industries_in_burbank,_california']\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "###### M1: Retrieving Documents ######\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Burbank, California' (searched for 'burbank_studios')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Burbank, Santa Clara County, California' (searched for 'burbank,_california')\n",
      "DEBUG 1.2: Skipping duplicate intro for 'Burbank, California' (searched for 'media_industry_in_burbank')\n",
      "DEBUG 1.2: Skipping duplicate intro for 'Burbank, California' (searched for 'economy_of_burbank,_california')\n",
      "DEBUG 1.2: Skipping duplicate intro for 'Burbank, California' (searched for 'history_of_burbank,_california')\n",
      "DEBUG 1.2: Skipping duplicate intro for 'Burbank, California' (searched for 'list_of_industries_in_burbank,_california')\n",
      "DEBUG 1.2: Retrieved content for 2 pages out of 6 potential titles.\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M2: Starting Evidence Extraction ######\n",
      "DEBUG 2.1 (module_2_controls):\n",
      "\tClaim: Burbank, California has always been completely void of industry.\n",
      "\tEntities: ['California', 'Burbank']\n",
      "\tKeywords: ['void', 'industry', 'completely', 'california', 'burbank']\n",
      "\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n",
      "\tMax Evidence Target: 8, Max Iterations: 10\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 17\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 1: Sending 17 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "3. Numerous media and entertainment companies are headquartered or have significant production facilities in Burbank—often called the \"Media Capital of the World\" and only a few miles northeast of Hollywood—including Warner Bros. Entertainment, the Walt Disney Company, Nickelodeon Animation Studio, The Burbank Studios, Cartoon Network Studios with the West Coast branch of Cartoon Network, and Insomniac Games. Burbank, California\n",
      "17. The Hollywood Burbank Airport was the location of Lockheed's Skunk Works, which produced some of the most secret and technologically advanced airplanes, including the U-2 spy planes. Burbank, California\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 1: LLM selected 2 sentences.\n",
      "M2 Iter 1: LLM selected few items (2). Lowering sBERT threshold to 0.350\n",
      "DEBUG 2. End Iter 1: Total evidence found: 2\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: Burbank, California has never had any industrial presence.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 17\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 2: Sending 15 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 2: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "   Lowering threshold to 0.300 for next try.\n",
      "DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: Burbank, California has consistently lacked any form of industry.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 17\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 3: Sending 15 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 3: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "M2 Iter 3: Stopping because LLM found no evidence after two tries.\n",
      "M2: Finished. Selected 2 evidence items.\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M3: Starting Classification ######\n",
      "DEBUG 3.1 (module_3_classification):\n",
      "\tClaim: Burbank, California has always been completely void of industry.\n",
      "\tEvidence Texts Sent (2):\n",
      "\tPrompt (partial): Based ONLY on the following evidence sentences, classify the claim as SUPPORTS, REFUTES, or NOT ENOUGH INFO.\n",
      "\n",
      "Claim: 'Burbank, California has always been completely void of industry.'\n",
      "\n",
      "Evidence:\n",
      "- The...\n",
      "-_--_--_--_--_--_--_--_--_--_-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing Claims:  27%|██▋       | 8/30 [06:24<17:28, 47.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG 3.2/3.3 (module_3_classification):\n",
      "\tLLM Classification Result: REFUTES\n",
      "\tExit Status: OK\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "Adding gold label/evidence to prediction.\n",
      "##########################################################################\n",
      "\n",
      "##########################################################################\n",
      "\n",
      "Time to process claim: 32.97 seconds.\n",
      "------------------------------------------------\n",
      "\n",
      "\n",
      "--- Processing Claim ID: 100046 ---\n",
      "DEBUG 1.0 (query_generator):\n",
      "\tClaim: The Guthrie Theater's second building began operating in 1963.\n",
      "\tEntities: ['Guthrie']\n",
      "\tLLM Output: ['guthrie_theater', 'guthrie_theater#history', 'theater_building', '1963_in_architecture']\n",
      "-_--_--_--_--_-\n",
      "DEBUG 1.1 (query_generator):\n",
      "\tEntities: ['Guthrie']\n",
      "\tGenerated Potential Titles: ['guthrie_theater', 'theater_building', '1963_in_architecture', 'guthrie_theater#history']\n",
      "\tSelected Titles for Retrieval: ['guthrie_theater', 'theater_building', '1963_in_architecture', 'guthrie_theater#history']\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "###### M1: Retrieving Documents ######\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Guthrie Theater' (searched for 'guthrie_theater')\n",
      "DEBUG 1.2: Skipping duplicate intro for 'Guthrie Theater' (searched for 'theater_building')\n",
      "DEBUG 1.2: Successfully retrieved intro from '1963 in architecture' (searched for '1963_in_architecture')\n",
      "DEBUG 1.2: Skipping duplicate intro for 'Guthrie Theater' (searched for 'guthrie_theater#history')\n",
      "DEBUG 1.2: Retrieved content for 2 pages out of 4 potential titles.\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M2: Starting Evidence Extraction ######\n",
      "DEBUG 2.1 (module_2_controls):\n",
      "\tClaim: The Guthrie Theater's second building began operating in 1963.\n",
      "\tEntities: ['Guthrie']\n",
      "\tKeywords: ['theater', 'second', 'operating', 'guthrie', 'building']\n",
      "\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n",
      "\tMax Evidence Target: 8, Max Iterations: 10\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 8\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 1: Sending 8 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 1: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "   Lowering threshold to 0.350 for next try.\n",
      "DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: The second building of the Guthrie Theater commenced operations in 1963.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 8\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 2: Sending 8 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 2: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "   Lowering threshold to 0.300 for next try.\n",
      "DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: In 1963, the second building of the Guthrie Theater started its operations.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 8\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 3: Sending 8 new candidates to LLM for selection.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing Claims:  30%|███       | 9/30 [06:40<13:15, 37.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 3: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "M2 Iter 3: Stopping because LLM found no evidence after two tries.\n",
      "M2: Finished iterations. No evidence selected.\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M3: Starting Classification ######\n",
      "M3: No evidence text provided. Classifying as NOT ENOUGH INFO.\n",
      "Adding gold label/evidence to prediction.\n",
      "##########################################################################\n",
      "\n",
      "##########################################################################\n",
      "\n",
      "Time to process claim: 16.29 seconds.\n",
      "------------------------------------------------\n",
      "\n",
      "\n",
      "--- Processing Claim ID: 204575 ---\n",
      "DEBUG 1.0 (query_generator):\n",
      "\tClaim: Commodore is ranked above a rear admiral.\n",
      "\tEntities: ['Commodore']\n",
      "\tLLM Output: ['commodore', 'rear_admiral', 'naval_ranks', 'military_rank', 'officer_rank']\n",
      "-_--_--_--_--_-\n",
      "DEBUG 1.1 (query_generator):\n",
      "\tEntities: ['Commodore']\n",
      "\tGenerated Potential Titles: ['commodore', 'naval_ranks', 'officer_rank', 'rear_admiral', 'military_rank']\n",
      "\tSelected Titles for Retrieval: ['commodore', 'naval_ranks', 'officer_rank', 'rear_admiral', 'military_rank']\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "###### M1: Retrieving Documents ######\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Commodore (rank)' (searched for 'commodore')\n",
      "DEBUG 1.2: Skipping duplicate intro for 'Commodore (rank)' (searched for 'naval_ranks')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Royal Navy officer rank insignia' (searched for 'officer_rank')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Commodore admiral' (searched for 'rear_admiral')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Ensign (rank)' (searched for 'military_rank')\n",
      "DEBUG 1.2: Retrieved content for 4 pages out of 5 potential titles.\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M2: Starting Evidence Extraction ######\n",
      "DEBUG 2.1 (module_2_controls):\n",
      "\tClaim: Commodore is ranked above a rear admiral.\n",
      "\tEntities: ['Commodore']\n",
      "\tKeywords: ['rear', 'ranked', 'commodore', 'admiral']\n",
      "\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n",
      "\tMax Evidence Target: 8, Max Iterations: 10\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 27\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 1: Sending 27 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "1. It is superior to a navy captain, but below a rear admiral. Commodore (rank)\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 1: LLM selected 1 sentences.\n",
      "M2 Iter 1: LLM selected few items (1). Lowering sBERT threshold to 0.350\n",
      "DEBUG 2. End Iter 1: Total evidence found: 1\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: A rear admiral is ranked below a commodore.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 28\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 2: Sending 27 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 2: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "   Lowering threshold to 0.300 for next try.\n",
      "DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: A rear admiral holds a lower rank than a commodore.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 28\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 3: Sending 27 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 3: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "M2 Iter 3: Stopping because LLM found no evidence after two tries.\n",
      "M2: Finished. Selected 1 evidence items.\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M3: Starting Classification ######\n",
      "DEBUG 3.1 (module_3_classification):\n",
      "\tClaim: Commodore is ranked above a rear admiral.\n",
      "\tEvidence Texts Sent (1):\n",
      "\tPrompt (partial): Based ONLY on the following evidence sentences, classify the claim as SUPPORTS, REFUTES, or NOT ENOUGH INFO.\n",
      "\n",
      "Claim: 'Commodore is ranked above a rear admiral.'\n",
      "\n",
      "Evidence:\n",
      "- It is superior to a navy c...\n",
      "-_--_--_--_--_--_--_--_--_--_-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing Claims:  33%|███▎      | 10/30 [07:20<12:48, 38.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG 3.2/3.3 (module_3_classification):\n",
      "\tLLM Classification Result: REFUTES\n",
      "\tExit Status: OK\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "Adding gold label/evidence to prediction.\n",
      "##########################################################################\n",
      "\n",
      "##########################################################################\n",
      "\n",
      "Time to process claim: 39.65 seconds.\n",
      "------------------------------------------------\n",
      "\n",
      "\n",
      "--- Processing Claim ID: 107539 ---\n",
      "DEBUG 1.0 (query_generator):\n",
      "\tClaim: Moscovium is a halogen.\n",
      "\tEntities: ['Moscovium']\n",
      "\tLLM Output: ['moscovium', 'halogen', 'periodic_table', 'chemical_element', 'list_of_chemical_elements']\n",
      "-_--_--_--_--_-\n",
      "DEBUG 1.1 (query_generator):\n",
      "\tEntities: ['Moscovium']\n",
      "\tGenerated Potential Titles: ['halogen', 'moscovium', 'periodic_table', 'chemical_element', 'list_of_chemical_elements']\n",
      "\tSelected Titles for Retrieval: ['halogen', 'moscovium', 'periodic_table', 'chemical_element', 'list_of_chemical_elements']\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "###### M1: Retrieving Documents ######\n",
      "DEBUG 1.2: DisambiguationError for title: 'halogen'. Options: ['Halogen (album)', 'Halogen (band)', 'Halogen lamp', 'Halogen oven', 'Halogen Foundation', 'Halogen Software', 'Halogen TV']...\n",
      "Warning: Selected title 'None of the provided options are relevant to the claim about Moscovium being a halogen. However, if I must select the most relevant title from the list, I would choose:\n",
      "\n",
      "'Halogen (album)'' not in disambiguation options.\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Isotopes of moscovium' (searched for 'moscovium')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Valence (chemistry)' (searched for 'periodic_table')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Isotope' (searched for 'chemical_element')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Molar ionization energies of the elements' (searched for 'list_of_chemical_elements')\n",
      "DEBUG 1.2: Retrieved content for 4 pages out of 5 potential titles.\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M2: Starting Evidence Extraction ######\n",
      "DEBUG 2.1 (module_2_controls):\n",
      "\tClaim: Moscovium is a halogen.\n",
      "\tEntities: ['Moscovium']\n",
      "\tKeywords: ['moscovium', 'halogen']\n",
      "\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n",
      "\tMax Evidence Target: 8, Max Iterations: 10\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 6\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 1: Sending 6 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 1: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "   Lowering threshold to 0.350 for next try.\n",
      "DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: Moscovium belongs to the halogen group.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 14\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 2: Sending 14 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 2: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "   Lowering threshold to 0.300 for next try.\n",
      "DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: Moscovium is classified as a halogen.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 22\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 3: Sending 22 new candidates to LLM for selection.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing Claims:  37%|███▋      | 11/30 [07:53<11:37, 36.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 3: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "M2 Iter 3: Stopping because LLM found no evidence after two tries.\n",
      "M2: Finished iterations. No evidence selected.\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M3: Starting Classification ######\n",
      "M3: No evidence text provided. Classifying as NOT ENOUGH INFO.\n",
      "Adding gold label/evidence to prediction.\n",
      "##########################################################################\n",
      "\n",
      "##########################################################################\n",
      "\n",
      "Time to process claim: 32.88 seconds.\n",
      "------------------------------------------------\n",
      "\n",
      "\n",
      "--- Processing Claim ID: 164883 ---\n",
      "DEBUG 1.0 (query_generator):\n",
      "\tClaim: Hezbollah received a type of training from Iran.\n",
      "\tEntities: ['Hezbollah', 'Iran']\n",
      "\tLLM Output: ['hezbollah', 'iran', 'hezbollah–iran_relations', 'military_training', 'iranian_revolutionary_guard']\n",
      "-_--_--_--_--_-\n",
      "DEBUG 1.1 (query_generator):\n",
      "\tEntities: ['Hezbollah', 'Iran']\n",
      "\tGenerated Potential Titles: ['iran', 'hezbollah', 'military_training', 'hezbollah–iran_relations', 'iranian_revolutionary_guard']\n",
      "\tSelected Titles for Retrieval: ['iran', 'hezbollah', 'military_training', 'hezbollah–iran_relations', 'iranian_revolutionary_guard']\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "###### M1: Retrieving Documents ######\n",
      "DEBUG 1.2: Successfully retrieved intro from 'History of Iran' (searched for 'iran')\n",
      "DEBUG 1.2: DisambiguationError for title: 'hezbollah'. Options: ['Hizbullah Afghan', 'Hezbollah Afghanistan', 'Liwa Fatemiyoun', 'Liwa Zainebiyoun', 'Islamic Resistance Movement of Azerbaijan', 'Hizbullah (Indonesia)', 'Kurdish Hezbollah of Iran', 'Hezbollah Organization', 'Hezbollah (Iran)', 'Ansar-e Hezbollah', 'Hezbollah fraction', 'Hezbollah Assembly', 'Independent Hezbollah deputies', 'Hezbollah Movement in Iraq', 'Harakat Hezbollah al-Nujaba', \"Kata'ib Hezbollah\", 'Kurdish Revolutionary Hezbollah', 'Mauritian Solidarity Front', 'Hezbollah Al-Hejaz', 'Kurdish Hezbollah', 'Hasbullah', 'The Land of Rape and Honey', 'Hezbollah International Financing Prevention Act of 2014']...\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Hezbollah Organization' (disambiguated to 'Hezbollah Organization')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'International Military Education and Training' (searched for 'military_training')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Hezbollah–Iran relations' (searched for 'hezbollah–iran_relations')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Iran and state-sponsored terrorism' (searched for 'iranian_revolutionary_guard')\n",
      "DEBUG 1.2: Retrieved content for 5 pages out of 5 potential titles.\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M2: Starting Evidence Extraction ######\n",
      "DEBUG 2.1 (module_2_controls):\n",
      "\tClaim: Hezbollah received a type of training from Iran.\n",
      "\tEntities: ['Hezbollah', 'Iran']\n",
      "\tKeywords: ['type', 'training', 'received', 'iran', 'hezbollah']\n",
      "\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n",
      "\tMax Evidence Target: 8, Max Iterations: 10\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 33\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 1: Sending 33 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "Iranian support, including financial aid, deployment of Revolutionary Guards, and training, has played an important role in Hezbollah's formation and development. Hezbollah–Iran relations  \n",
      "Additionally, Iran provides weapons, training, and other forms of assistance to Hezbollah. Hezbollah–Iran relations  \n",
      "The organization's founders adopted the model outlined by Ayatollah Khomeini after the 1979 Iranian Revolution, and its forces were trained by a contingent of Revolutionary Guards from Iran. Hezbollah–Iran relations\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 1: LLM selected 3 sentences.\n",
      "M2 Iter 1: LLM selected few items (3). Lowering sBERT threshold to 0.350\n",
      "DEBUG 2. End Iter 1: Total evidence found: 3\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: Hezbollah was trained by Iran in a certain manner.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 37\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 2: Sending 34 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "1. Since the Iranian Revolution in 1979, the government of the Islamic Republic of Iran has been accused by several countries of training, financing, and providing weapons and safe havens for non-state militant actors, such as Hezbollah in Lebanon, Hamas in Gaza, and other Palestinian groups such as the Islamic Jihad (IJ) and the Popular Front for the Liberation of Palestine (PFLP). Iran and state-sponsored terrorism\n",
      "2. Its special operations unit, the Quds Force, is known to provide arms, training, and financial support to militias and political movements across the Middle East, including Bahrain, Iraq, Lebanon, Palestine, Syria, and Yemen. Iran and state-sponsored terrorism\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 2: LLM selected 2 sentences.\n",
      "M2 Iter 2: LLM selected few items (2). Lowering sBERT threshold to 0.300\n",
      "DEBUG 2. End Iter 2: Total evidence found: 5\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: Hezbollah underwent a specific form of training provided by Iran.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 48\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 3: Sending 43 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 3: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "M2 Iter 3: Stopping because LLM found no evidence after two tries.\n",
      "M2: Finished. Selected 5 evidence items.\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M3: Starting Classification ######\n",
      "DEBUG 3.1 (module_3_classification):\n",
      "\tClaim: Hezbollah received a type of training from Iran.\n",
      "\tEvidence Texts Sent (5):\n",
      "\tPrompt (partial): Based ONLY on the following evidence sentences, classify the claim as SUPPORTS, REFUTES, or NOT ENOUGH INFO.\n",
      "\n",
      "Claim: 'Hezbollah received a type of training from Iran.'\n",
      "\n",
      "Evidence:\n",
      "- Since the Iranian R...\n",
      "-_--_--_--_--_--_--_--_--_--_-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing Claims:  40%|████      | 12/30 [09:03<14:05, 46.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG 3.2/3.3 (module_3_classification):\n",
      "\tLLM Classification Result: SUPPORTS\n",
      "\tExit Status: OK\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "Adding gold label/evidence to prediction.\n",
      "##########################################################################\n",
      "\n",
      "##########################################################################\n",
      "\n",
      "Time to process claim: 70.44 seconds.\n",
      "------------------------------------------------\n",
      "\n",
      "\n",
      "--- Processing Claim ID: 54298 ---\n",
      "DEBUG 1.0 (query_generator):\n",
      "\tClaim: In states still employing the electric chair to execute people, the prisoner is allowed the choice of lethal injection as an alternative method.\n",
      "\tEntities: []\n",
      "\tLLM Output: ['electric_chair', 'lethal_injection', 'capital_punishment_in_the_United_States', 'death_penalty', 'methods_of_execution', 'prisoner_rights']\n",
      "-_--_--_--_--_-\n",
      "DEBUG 1.1 (query_generator):\n",
      "\tEntities: []\n",
      "\tGenerated Potential Titles: ['death_penalty', 'electric_chair', 'prisoner_rights', 'lethal_injection', 'methods_of_execution', 'capital_punishment_in_the_United_States']\n",
      "\tSelected Titles for Retrieval: ['death_penalty', 'electric_chair', 'prisoner_rights', 'lethal_injection', 'methods_of_execution', 'capital_punishment_in_the_United_States']\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "###### M1: Retrieving Documents ######\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Capital punishment in the United States' (searched for 'death_penalty')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Electric chair' (searched for 'electric_chair')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Prisoner rights in the United States' (searched for 'prisoner_rights')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Lethal injection' (searched for 'lethal_injection')\n",
      "DEBUG 1.2: Skipping duplicate intro for 'Lethal injection' (searched for 'methods_of_execution')\n",
      "DEBUG 1.2: Skipping duplicate intro for 'Capital punishment in the United States' (searched for 'capital_punishment_in_the_United_States')\n",
      "DEBUG 1.2: Retrieved content for 4 pages out of 6 potential titles.\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M2: Starting Evidence Extraction ######\n",
      "DEBUG 2.1 (module_2_controls):\n",
      "\tClaim: In states still employing the electric chair to execute people, the prisoner is allowed the choice of lethal injection as an alternative method.\n",
      "\tEntities: []\n",
      "\tKeywords: ['states', 'prisoner', 'people', 'method', 'lethal']\n",
      "\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n",
      "\tMax Evidence Target: 8, Max Iterations: 10\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 55\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 1: Sending 55 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "While some states retain electrocution as a legal execution method, it is often a secondary option based on the condemned's preference. Electric chair  \n",
      "As of 2025, electrocution remains an option in states like Alabama, South Carolina and Florida, where inmates may choose lethal injection instead. Electric chair\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 1: LLM selected 2 sentences.\n",
      "M2 Iter 1: LLM selected few items (2). Lowering sBERT threshold to 0.350\n",
      "DEBUG 2. End Iter 1: Total evidence found: 2\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: In jurisdictions that continue to use the electric chair for executions, inmates are permitted to opt for lethal injection as another method.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 58\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 2: Sending 56 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "2. These three states also authorize electrocution as an alternative if lethal injection is deemed unavailable. Electric chair\n",
      "23. Arkansas, Kentucky, and Tennessee offer the electric chair to those sentenced before a certain date. Electric chair\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 2: LLM selected 2 sentences.\n",
      "M2 Iter 2: LLM selected few items (2). Lowering sBERT threshold to 0.300\n",
      "DEBUG 2. End Iter 2: Total evidence found: 4\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: In states that still use the electric chair for executions, prisoners have the option to select lethal injection as an alternative method.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 62\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 3: Sending 58 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 3: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "M2 Iter 3: Stopping because LLM found no evidence after two tries.\n",
      "M2: Finished. Selected 4 evidence items.\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M3: Starting Classification ######\n",
      "DEBUG 3.1 (module_3_classification):\n",
      "\tClaim: In states still employing the electric chair to execute people, the prisoner is allowed the choice of lethal injection as an alternative method.\n",
      "\tEvidence Texts Sent (4):\n",
      "\tPrompt (partial): Based ONLY on the following evidence sentences, classify the claim as SUPPORTS, REFUTES, or NOT ENOUGH INFO.\n",
      "\n",
      "Claim: 'In states still employing the electric chair to execute people, the prisoner is al...\n",
      "-_--_--_--_--_--_--_--_--_--_-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing Claims:  43%|████▎     | 13/30 [10:11<15:06, 53.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG 3.2/3.3 (module_3_classification):\n",
      "\tLLM Classification Result: SUPPORTS\n",
      "\tExit Status: OK\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "Adding gold label/evidence to prediction.\n",
      "##########################################################################\n",
      "\n",
      "##########################################################################\n",
      "\n",
      "Time to process claim: 68.01 seconds.\n",
      "------------------------------------------------\n",
      "\n",
      "\n",
      "--- Processing Claim ID: 222749 ---\n",
      "DEBUG 1.0 (query_generator):\n",
      "\tClaim: Practical Magic is an American romantic drama film.\n",
      "\tEntities: ['Practical', 'American', 'Magic']\n",
      "\tLLM Output: ['practical_magic', 'romantic_drama_film', 'list_of_american_romantic_drama_films', 'film_genre', 'film_adaptation']\n",
      "-_--_--_--_--_-\n",
      "DEBUG 1.1 (query_generator):\n",
      "\tEntities: ['Practical', 'American', 'Magic']\n",
      "\tGenerated Potential Titles: ['film_genre', 'film_adaptation', 'practical_magic', 'romantic_drama_film', 'list_of_american_romantic_drama_films']\n",
      "\tSelected Titles for Retrieval: ['film_genre', 'film_adaptation', 'practical_magic', 'romantic_drama_film', 'list_of_american_romantic_drama_films']\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "###### M1: Retrieving Documents ######\n",
      "DEBUG 1.2: DisambiguationError for title: 'film_genre'. Options: ['film genre', 'Genre Films', 'Genre (1996 film)', 'Genre', 'B movie']...\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Film genre' (disambiguated to 'film genre')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Pride and Prejudice' (searched for 'film_adaptation')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Practical Magic (novel)' (searched for 'practical_magic')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Romantic (film)' (searched for 'romantic_drama_film')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Romance film' (searched for 'list_of_american_romantic_drama_films')\n",
      "DEBUG 1.2: Retrieved content for 5 pages out of 5 potential titles.\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M2: Starting Evidence Extraction ######\n",
      "DEBUG 2.1 (module_2_controls):\n",
      "\tClaim: Practical Magic is an American romantic drama film.\n",
      "\tEntities: ['Practical', 'American', 'Magic']\n",
      "\tKeywords: ['romantic', 'practical', 'magic', 'film', 'drama']\n",
      "\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n",
      "\tMax Evidence Target: 8, Max Iterations: 10\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 11\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 1: Sending 11 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 1: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "   Lowering threshold to 0.350 for next try.\n",
      "DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: Practical Magic is a romantic drama movie produced in the United States.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 12\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 2: Sending 12 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 2: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "   Lowering threshold to 0.300 for next try.\n",
      "DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: Practical Magic is a romantic drama film made in the United States.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 14\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 3: Sending 14 new candidates to LLM for selection.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing Claims:  47%|████▋     | 14/30 [10:51<13:08, 49.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 3: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "M2 Iter 3: Stopping because LLM found no evidence after two tries.\n",
      "M2: Finished iterations. No evidence selected.\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M3: Starting Classification ######\n",
      "M3: No evidence text provided. Classifying as NOT ENOUGH INFO.\n",
      "Adding gold label/evidence to prediction.\n",
      "##########################################################################\n",
      "\n",
      "##########################################################################\n",
      "\n",
      "Time to process claim: 39.90 seconds.\n",
      "------------------------------------------------\n",
      "\n",
      "\n",
      "--- Processing Claim ID: 219675 ---\n",
      "DEBUG 1.0 (query_generator):\n",
      "\tClaim: Corsica belongs to Italy.\n",
      "\tEntities: ['Corsica', 'Italy']\n",
      "\tLLM Output: ['corsica', 'italy', 'history_of_corsica', 'political_status_of_corsica', 'territorial_claims_of_italy']\n",
      "-_--_--_--_--_-\n",
      "DEBUG 1.1 (query_generator):\n",
      "\tEntities: ['Corsica', 'Italy']\n",
      "\tGenerated Potential Titles: ['italy', 'corsica', 'history_of_corsica', 'political_status_of_corsica', 'territorial_claims_of_italy']\n",
      "\tSelected Titles for Retrieval: ['italy', 'corsica', 'history_of_corsica', 'political_status_of_corsica', 'territorial_claims_of_italy']\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "###### M1: Retrieving Documents ######\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Kingdom of Italy' (searched for 'italy')\n",
      "DEBUG 1.2: DisambiguationError for title: 'corsica'. Options: ['Corsica, Pennsylvania', 'Corsica, South Dakota', 'Chevrolet Corsica', 'Artemisia Gentileschi', 'Sluggy Freelance', 'Corsica (album)', 'Corsica Coachworks', 'Corsican (disambiguation)', 'Corse (disambiguation)', 'Corsa (disambiguation)']...\n",
      "Warning: Error retrieving disambiguated page 'Corsican (disambiguation)': \"Corsican\" may refer to: \n",
      "Corsica\n",
      "Corsicans\n",
      "Corsican language\n",
      "Corsican Republic\n",
      "Hearts of Oak militia\n",
      "List of all pages beginning with \"Corsican\"\n",
      "List of Corsicans\n",
      "Corsicana, Texas\n",
      "Corsica (disambiguation)\n",
      "Corse (disambiguation)\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Italian occupation of Corsica' (searched for 'history_of_corsica')\n",
      "DEBUG 1.2: Skipping duplicate intro for 'Italian occupation of Corsica' (searched for 'political_status_of_corsica')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Italian occupation of France' (searched for 'territorial_claims_of_italy')\n",
      "DEBUG 1.2: Retrieved content for 3 pages out of 5 potential titles.\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M2: Starting Evidence Extraction ######\n",
      "DEBUG 2.1 (module_2_controls):\n",
      "\tClaim: Corsica belongs to Italy.\n",
      "\tEntities: ['Corsica', 'Italy']\n",
      "\tKeywords: ['italy', 'corsica', 'belongs']\n",
      "\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n",
      "\tMax Evidence Target: 8, Max Iterations: 10\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 11\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 1: Sending 11 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "1. The Italian occupation of Corsica refers to the military (and administrative) occupation by the Kingdom of Italy of the French island of Corsica during the Second World War, from November 1942 to September 1943. Italian occupation of Corsica\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 1: LLM selected 1 sentences.\n",
      "M2 Iter 1: LLM selected few items (1). Lowering sBERT threshold to 0.350\n",
      "DEBUG 2. End Iter 1: Total evidence found: 1\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: Corsica is a part of Italy.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 22\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 2: Sending 21 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 2: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "   Lowering threshold to 0.300 for next try.\n",
      "DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: Corsica is part of Italy.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 27\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 3: Sending 26 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 3: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "M2 Iter 3: Stopping because LLM found no evidence after two tries.\n",
      "M2: Finished. Selected 1 evidence items.\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M3: Starting Classification ######\n",
      "DEBUG 3.1 (module_3_classification):\n",
      "\tClaim: Corsica belongs to Italy.\n",
      "\tEvidence Texts Sent (1):\n",
      "\tPrompt (partial): Based ONLY on the following evidence sentences, classify the claim as SUPPORTS, REFUTES, or NOT ENOUGH INFO.\n",
      "\n",
      "Claim: 'Corsica belongs to Italy.'\n",
      "\n",
      "Evidence:\n",
      "- The Italian occupation of Corsica refers t...\n",
      "-_--_--_--_--_--_--_--_--_--_-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing Claims:  50%|█████     | 15/30 [11:31<11:39, 46.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG 3.2/3.3 (module_3_classification):\n",
      "\tLLM Classification Result: REFUTES\n",
      "\tExit Status: OK\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "Adding gold label/evidence to prediction.\n",
      "##########################################################################\n",
      "\n",
      "##########################################################################\n",
      "\n",
      "Time to process claim: 40.55 seconds.\n",
      "------------------------------------------------\n",
      "\n",
      "\n",
      "--- Processing Claim ID: 134850 ---\n",
      "DEBUG 1.0 (query_generator):\n",
      "\tClaim: Ice-T refused to ever make hip-hop music.\n",
      "\tEntities: []\n",
      "\tLLM Output: ['ice-t', 'hip_hop_music', 'ice-t_discography', 'ice-t_career', 'hip_hop', 'rap_music']\n",
      "-_--_--_--_--_-\n",
      "DEBUG 1.1 (query_generator):\n",
      "\tEntities: []\n",
      "\tGenerated Potential Titles: ['ice-t', 'hip_hop', 'rap_music', 'ice-t_career', 'hip_hop_music', 'ice-t_discography']\n",
      "\tSelected Titles for Retrieval: ['ice-t', 'hip_hop', 'rap_music', 'ice-t_career', 'hip_hop_music', 'ice-t_discography']\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "###### M1: Retrieving Documents ######\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Ice-T's Rap School' (searched for 'ice-t')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Hip-hop production' (searched for 'hip_hop')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'List of hip-hop genres' (searched for 'rap_music')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Ice-T' (searched for 'ice-t_career')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Hip-Hop Evolution' (searched for 'hip_hop_music')\n",
      "DEBUG 1.2: Skipping duplicate intro for 'Ice-T' (searched for 'ice-t_discography')\n",
      "DEBUG 1.2: Retrieved content for 5 pages out of 6 potential titles.\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M2: Starting Evidence Extraction ######\n",
      "DEBUG 2.1 (module_2_controls):\n",
      "\tClaim: Ice-T refused to ever make hip-hop music.\n",
      "\tEntities: []\n",
      "\tKeywords: ['refused', 'music', 'make', 'ice', 'hop']\n",
      "\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n",
      "\tMax Evidence Target: 8, Max Iterations: 10\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 29\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 1: Sending 29 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "He is active in both hip hop and heavy metal. Ice-T  \n",
      "Ice-T began his career as an underground rapper in the 1980s and was signed to Sire Records in 1987, when he released his debut album Rhyme Pays. Ice-T  \n",
      "The following year, he founded the record label Rhyme $yndicate Records (named after his collective of fellow hip-hop artists called the \"Rhyme $yndicate\") and released another album, Power (1988), which would go platinum. Ice-T  \n",
      "Ice-T released two more albums in the late 1990s and one in the 2000s before focusing on both his acting career and Body Count, who have released eight studio albums to date, the latest being 2024's Merciless. Ice-T  \n",
      "Tracy Lauren Marrow (born February 16, 1958), better known by his stage name Ice-T (or Ice T), is an American rapper and actor. Ice-T\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 1: LLM selected 5 sentences.\n",
      "M2 Iter 1: LLM selected few items (5). Lowering sBERT threshold to 0.350\n",
      "DEBUG 2. End Iter 1: Total evidence found: 5\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: Ice-T has stated that he will never create hip-hop music.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 31\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 2: Sending 26 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 2: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "   Lowering threshold to 0.300 for next try.\n",
      "DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: Ice-T has declared that he will not produce hip-hop music at any point.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 32\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 3: Sending 27 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 3: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "M2 Iter 3: Stopping because LLM found no evidence after two tries.\n",
      "M2: Finished. Selected 5 evidence items.\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M3: Starting Classification ######\n",
      "DEBUG 3.1 (module_3_classification):\n",
      "\tClaim: Ice-T refused to ever make hip-hop music.\n",
      "\tEvidence Texts Sent (5):\n",
      "\tPrompt (partial): Based ONLY on the following evidence sentences, classify the claim as SUPPORTS, REFUTES, or NOT ENOUGH INFO.\n",
      "\n",
      "Claim: 'Ice-T refused to ever make hip-hop music.'\n",
      "\n",
      "Evidence:\n",
      "- Ice-T released two more al...\n",
      "-_--_--_--_--_--_--_--_--_--_-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing Claims:  53%|█████▎    | 16/30 [12:20<11:01, 47.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG 3.2/3.3 (module_3_classification):\n",
      "\tLLM Classification Result: REFUTES\n",
      "\tExit Status: OK\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "Adding gold label/evidence to prediction.\n",
      "##########################################################################\n",
      "\n",
      "##########################################################################\n",
      "\n",
      "Time to process claim: 48.73 seconds.\n",
      "------------------------------------------------\n",
      "\n",
      "\n",
      "--- Processing Claim ID: 124578 ---\n",
      "DEBUG 1.0 (query_generator):\n",
      "\tClaim: The Gettysburg Address is a speech.\n",
      "\tEntities: ['Gettysburg Address']\n",
      "\tLLM Output: ['gettysburg_address', 'speech', 'abraham_lincoln', 'american_civil_war', 'historical_speeches']\n",
      "-_--_--_--_--_-\n",
      "DEBUG 1.1 (query_generator):\n",
      "\tEntities: ['Gettysburg Address']\n",
      "\tGenerated Potential Titles: ['speech', 'abraham_lincoln', 'american_civil_war', 'gettysburg_address', 'historical_speeches']\n",
      "\tSelected Titles for Retrieval: ['speech', 'abraham_lincoln', 'american_civil_war', 'gettysburg_address', 'historical_speeches']\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "###### M1: Retrieving Documents ######\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Speech' (searched for 'speech')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Abraham Lincoln and slavery' (searched for 'abraham_lincoln')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Conclusion of the American Civil War' (searched for 'american_civil_war')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'A Gettysburg Address' (searched for 'gettysburg_address')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Gettysburg Address' (searched for 'historical_speeches')\n",
      "DEBUG 1.2: Retrieved content for 5 pages out of 5 potential titles.\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M2: Starting Evidence Extraction ######\n",
      "DEBUG 2.1 (module_2_controls):\n",
      "\tClaim: The Gettysburg Address is a speech.\n",
      "\tEntities: ['Gettysburg Address']\n",
      "\tKeywords: ['speech', 'gettysburg', 'address']\n",
      "\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n",
      "\tMax Evidence Target: 8, Max Iterations: 10\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 29\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 1: Sending 29 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "The Gettysburg Address is a speech delivered by Abraham Lincoln, the 16th U.S. president, following the Battle of Gettysburg during the American Civil War. Gettysburg Address\n",
      "Lincoln delivered the speech on the afternoon of November 19, 1863, during a formal dedication of Soldiers' National Cemetery, now known as Gettysburg National Cemetery, on the grounds where the Battle of Gettysburg was fought four and a half months earlier, between July 1 and July 3, 1863, in Gettysburg, Pennsylvania. Gettysburg Address\n",
      "In his brief but historical speech, Lincoln described the Union's cause in the Civil War as necessary to validate that the sovereignty and freedoms the nation successfully secured less than nine decades earlier in the American Revolution, Revolutionary War, and nation's establishment, could prove enduring. Gettysburg Address\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 1: LLM selected 3 sentences.\n",
      "M2 Iter 1: LLM selected few items (3). Lowering sBERT threshold to 0.350\n",
      "DEBUG 2. End Iter 1: Total evidence found: 3\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: The Gettysburg Address is an oration.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 28\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 2: Sending 25 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "As the Gettysburg Address gained in popularity, it became a staple of school textbooks and readers, and the succinctness of the three paragraph oration permitted it to be memorized by generations of American school children,\" the History Channel reported in November 2024. Gettysburg Address\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 2: LLM selected 1 sentences.\n",
      "M2 Iter 2: LLM selected few items (1). Lowering sBERT threshold to 0.300\n",
      "DEBUG 2. End Iter 2: Total evidence found: 4\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: The Gettysburg Address is a public address.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 27\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 3: Sending 23 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "The historical and enduring significance and fame of the Gettysburg Address is at least partly attributable to its conscious brevity, including only 271 words and comprising less than two minutes before a crowd of approximately 15,000 people, which gathered to join in commemorating the sacrifice of the Union soldiers, over 23,000 of whom were killed over the three day Battle of Gettysburg. Gettysburg Address\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 3: LLM selected 1 sentences.\n",
      "M2 Iter 3: LLM selected few items (1). Lowering sBERT threshold to 0.250\n",
      "DEBUG 2. End Iter 3: Total evidence found: 5\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 4/10, Current sBERT Thresh: 0.250\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: The Gettysburg Address is a formal discourse.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 52\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 4: Sending 47 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "1. Over time, however, it came to be viewed widely as one of the greatest and most influential statements ever delivered on the American national purpose, enduring in significance throughout the nation's history, and also as one of the most prominent examples of the successful use of the English language and rhetoric to advance a political cause. Gettysburg Address\n",
      "3. The speech has come to be viewed as one of the most famous, enduring, and historically significant speeches in American history. Gettysburg Address\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 4: LLM selected 2 sentences.\n",
      "M2 Iter 4: LLM selected few items (2). Lowering sBERT threshold to 0.200\n",
      "DEBUG 2. End Iter 4: Total evidence found: 7\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 5/10, Current sBERT Thresh: 0.200\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: The Gettysburg Address is a spoken presentation.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 55\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 5: Sending 48 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "3. Despite the historical significance and fame that the speech ultimately obtained, Lincoln was not intended to serve as the primary speaker at the cemetery's dedication that day, and his brief speech consumed a very small fraction of the day's event, which lasted for several hours. Gettysburg Address\n",
      "5. Lincoln began his 271-word address in Gettysburg with the now famed phrase, \"Four score and seven years ago\", a reference to the nation's founding in the American Revolution, during which the Founding Fathers ultimately concluded that they could not reconcile their differences with King George III and instead needed to enjoin and prevail in the Revolutionary War in pursuit of full independence from British colonial rule. Gettysburg Address\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 5: LLM selected 2 sentences.\n",
      "M2 Iter 5: LLM selected few items (1). Lowering sBERT threshold to 0.150\n",
      "DEBUG 2. End Iter 5: Total evidence found: 8\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 5: Reached target evidence count (8).\n",
      "M2: Finished. Selected 8 evidence items.\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M3: Starting Classification ######\n",
      "DEBUG 3.1 (module_3_classification):\n",
      "\tClaim: The Gettysburg Address is a speech.\n",
      "\tEvidence Texts Sent (8):\n",
      "\tPrompt (partial): Based ONLY on the following evidence sentences, classify the claim as SUPPORTS, REFUTES, or NOT ENOUGH INFO.\n",
      "\n",
      "Claim: 'The Gettysburg Address is a speech.'\n",
      "\n",
      "Evidence:\n",
      "- Despite the historical significa...\n",
      "-_--_--_--_--_--_--_--_--_--_-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing Claims:  57%|█████▋    | 17/30 [14:26<15:20, 70.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG 3.2/3.3 (module_3_classification):\n",
      "\tLLM Classification Result: SUPPORTS\n",
      "\tExit Status: OK\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "Adding gold label/evidence to prediction.\n",
      "##########################################################################\n",
      "\n",
      "##########################################################################\n",
      "\n",
      "Time to process claim: 125.62 seconds.\n",
      "------------------------------------------------\n",
      "\n",
      "\n",
      "--- Processing Claim ID: 134126 ---\n",
      "DEBUG 1.0 (query_generator):\n",
      "\tClaim: Jason Bourne removed Riz Ahmed from the movie's cast.\n",
      "\tEntities: ['Riz Ahmed', 'Bourne', 'Jason']\n",
      "\tLLM Output: ['jason_bourne', 'riz_ahmed', 'movie_cast', 'the_bourne_identity', 'the_bourne_suppremacy', 'the_bourne_ultimatum', 'the_bourne_legacy', 'jason_bourne_(film)']\n",
      "-_--_--_--_--_-\n",
      "DEBUG 1.1 (query_generator):\n",
      "\tEntities: ['Riz Ahmed', 'Bourne', 'Jason']\n",
      "\tGenerated Potential Titles: ['riz_ahmed', 'movie_cast', 'jason_bourne', 'the_bourne_legacy', 'the_bourne_identity', 'jason_bourne_(film)', 'the_bourne_ultimatum', 'the_bourne_suppremacy']\n",
      "\tSelected Titles for Retrieval: ['riz_ahmed', 'movie_cast', 'jason_bourne', 'the_bourne_legacy', 'the_bourne_identity', 'jason_bourne_(film)', 'the_bourne_ultimatum', 'the_bourne_suppremacy']\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "###### M1: Retrieving Documents ######\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Jason Bourne (film)' (searched for 'riz_ahmed')\n",
      "DEBUG 1.2: DisambiguationError for title: 'movie_cast'. Options: ['Cast (band)', 'Cast (Mexican band)', 'Mairi Campbell', 'Trespassers William', 'KAT-TUN', 'Casting (metalworking)', 'Cast iron', 'Cast (geology)', 'earthworms', 'Cast of the eye', 'Orthopedic cast', 'Cast (computer science)', 'Urinary cast', 'Google Cast', 'Game cast', 'Cast (archery)', 'falconry', 'fishing', 'Cast member', 'magic spell', 'Nusantara TV', 'Plaster cast', 'Cast, Finistère', 'Doncaster', 'Kristin Cast', 'P. C. Cast', 'CAST (disambiguation)', 'Caste (disambiguation)', 'Casting (disambiguation)', 'Hard cast (disambiguation)', 'The die is cast', '\"The Die Is Cast\" (Star Trek: Deep Space Nine)']...\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Casting (performing arts)' (disambiguated to 'Cast member')\n",
      "DEBUG 1.2: Skipping duplicate intro for 'Jason Bourne (film)' (searched for 'jason_bourne')\n",
      "DEBUG 1.2: Skipping duplicate intro for 'Jason Bourne (film)' (searched for 'the_bourne_legacy')\n",
      "DEBUG 1.2: Skipping duplicate intro for 'Jason Bourne (film)' (searched for 'the_bourne_identity')\n",
      "DEBUG 1.2: Skipping duplicate intro for 'Jason Bourne (film)' (searched for 'jason_bourne_(film)')\n",
      "DEBUG 1.2: Skipping duplicate intro for 'Jason Bourne (film)' (searched for 'the_bourne_ultimatum')\n",
      "DEBUG 1.2: No Wikipedia page found for potential title: 'the_bourne_suppremacy'\n",
      "DEBUG 1.2: Retrieved content for 2 pages out of 8 potential titles.\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M2: Starting Evidence Extraction ######\n",
      "DEBUG 2.1 (module_2_controls):\n",
      "\tClaim: Jason Bourne removed Riz Ahmed from the movie's cast.\n",
      "\tEntities: ['Riz Ahmed', 'Bourne', 'Jason']\n",
      "\tKeywords: ['riz', 'removed', 'movie', 'jason', 'cast']\n",
      "\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n",
      "\tMax Evidence Target: 8, Max Iterations: 10\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 9\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 1: Sending 9 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 1: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "   Lowering threshold to 0.350 for next try.\n",
      "DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: Riz Ahmed was taken out of the cast of the movie by Jason Bourne.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 9\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 2: Sending 9 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 2: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "   Lowering threshold to 0.300 for next try.\n",
      "DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: Riz Ahmed was eliminated from the cast of the film by Jason Bourne.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 9\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 3: Sending 9 new candidates to LLM for selection.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing Claims:  60%|██████    | 18/30 [14:52<11:29, 57.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 3: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "M2 Iter 3: Stopping because LLM found no evidence after two tries.\n",
      "M2: Finished iterations. No evidence selected.\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M3: Starting Classification ######\n",
      "M3: No evidence text provided. Classifying as NOT ENOUGH INFO.\n",
      "Adding gold label/evidence to prediction.\n",
      "##########################################################################\n",
      "\n",
      "##########################################################################\n",
      "\n",
      "Time to process claim: 26.44 seconds.\n",
      "------------------------------------------------\n",
      "\n",
      "\n",
      "--- Processing Claim ID: 125577 ---\n",
      "DEBUG 1.0 (query_generator):\n",
      "\tClaim: Ron Dennis is unemployed.\n",
      "\tEntities: ['Dennis', 'Ron']\n",
      "\tLLM Output: ['ron_dennis', 'mclaren', 'formula_one', 'automobile_racing']\n",
      "-_--_--_--_--_-\n",
      "DEBUG 1.1 (query_generator):\n",
      "\tEntities: ['Dennis', 'Ron']\n",
      "\tGenerated Potential Titles: ['mclaren', 'ron_dennis', 'formula_one', 'automobile_racing']\n",
      "\tSelected Titles for Retrieval: ['mclaren', 'ron_dennis', 'formula_one', 'automobile_racing']\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "###### M1: Retrieving Documents ######\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Lucky Diamond Rich' (searched for 'mclaren')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Ron Dennis' (searched for 'ron_dennis')\n",
      "DEBUG 1.2: Successfully retrieved intro from '2007 Formula One World Championship' (searched for 'formula_one')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'ARCA Menards Series' (searched for 'automobile_racing')\n",
      "DEBUG 1.2: Retrieved content for 4 pages out of 4 potential titles.\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M2: Starting Evidence Extraction ######\n",
      "DEBUG 2.1 (module_2_controls):\n",
      "\tClaim: Ron Dennis is unemployed.\n",
      "\tEntities: ['Dennis', 'Ron']\n",
      "\tKeywords: ['unemployed', 'ron', 'dennis']\n",
      "\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n",
      "\tMax Evidence Target: 8, Max Iterations: 10\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 8\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 1: Sending 8 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 1: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "   Lowering threshold to 0.350 for next try.\n",
      "DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: Ron Dennis does not have a job.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 10\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 2: Sending 10 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 2: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "   Lowering threshold to 0.300 for next try.\n",
      "DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: Ron Dennis currently lacks employment.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 10\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 3: Sending 10 new candidates to LLM for selection.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing Claims:  63%|██████▎   | 19/30 [15:32<09:32, 52.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 3: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "M2 Iter 3: Stopping because LLM found no evidence after two tries.\n",
      "M2: Finished iterations. No evidence selected.\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M3: Starting Classification ######\n",
      "M3: No evidence text provided. Classifying as NOT ENOUGH INFO.\n",
      "Adding gold label/evidence to prediction.\n",
      "##########################################################################\n",
      "\n",
      "##########################################################################\n",
      "\n",
      "Time to process claim: 39.29 seconds.\n",
      "------------------------------------------------\n",
      "\n",
      "\n",
      "--- Processing Claim ID: 132244 ---\n",
      "DEBUG 1.0 (query_generator):\n",
      "\tClaim: Wolfgang Amadeus Mozart showed he was a child protege.\n",
      "\tEntities: ['Amadeus Mozart', 'Wolfgang']\n",
      "\tLLM Output: ['wolfgang_amadeus_mozart', 'child_prodigy', 'history_of_classical_music', 'classical_music', 'mozart_effect']\n",
      "-_--_--_--_--_-\n",
      "DEBUG 1.1 (query_generator):\n",
      "\tEntities: ['Amadeus Mozart', 'Wolfgang']\n",
      "\tGenerated Potential Titles: ['child_prodigy', 'mozart_effect', 'classical_music', 'wolfgang_amadeus_mozart', 'history_of_classical_music']\n",
      "\tSelected Titles for Retrieval: ['child_prodigy', 'mozart_effect', 'classical_music', 'wolfgang_amadeus_mozart', 'history_of_classical_music']\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "###### M1: Retrieving Documents ######\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Child prodigy' (searched for 'child_prodigy')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Wolfgang Amadeus Mozart' (searched for 'mozart_effect')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Classical music' (searched for 'classical_music')\n",
      "DEBUG 1.2: Skipping duplicate intro for 'Wolfgang Amadeus Mozart' (searched for 'wolfgang_amadeus_mozart')\n",
      "DEBUG 1.2: Skipping duplicate intro for 'Classical music' (searched for 'history_of_classical_music')\n",
      "DEBUG 1.2: Retrieved content for 3 pages out of 5 potential titles.\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M2: Starting Evidence Extraction ######\n",
      "DEBUG 2.1 (module_2_controls):\n",
      "\tClaim: Wolfgang Amadeus Mozart showed he was a child protege.\n",
      "\tEntities: ['Amadeus Mozart', 'Wolfgang']\n",
      "\tKeywords: ['wolfgang', 'showed', 'protege', 'mozart', 'child']\n",
      "\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n",
      "\tMax Evidence Target: 8, Max Iterations: 10\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 19\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 1: Sending 19 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "At age five, he was already competent on keyboard and violin, had begun to compose, and performed before European royalty. Wolfgang Amadeus Mozart  \n",
      "Born in Salzburg, Mozart showed prodigious ability from his earliest childhood. Wolfgang Amadeus Mozart\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 1: LLM selected 2 sentences.\n",
      "M2 Iter 1: LLM selected few items (2). Lowering sBERT threshold to 0.350\n",
      "DEBUG 2. End Iter 1: Total evidence found: 2\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: Wolfgang Amadeus Mozart demonstrated that he was a prodigy in his childhood.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 28\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 2: Sending 26 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "1. Despite his short life, his rapid pace of composition and proficiency from an early age resulted in more than 800 works representing virtually every Western classical genre of his time. Wolfgang Amadeus Mozart\n",
      "8. His father Leopold Mozart took him on a grand tour of Europe and then three trips to Italy. Wolfgang Amadeus Mozart\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 2: LLM selected 2 sentences.\n",
      "M2 Iter 2: LLM selected few items (2). Lowering sBERT threshold to 0.300\n",
      "DEBUG 2. End Iter 2: Total evidence found: 4\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: Wolfgang Amadeus Mozart proved to be a musical prodigy during his youth.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 43\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 3: Sending 39 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "1. The term wunderkind (from German Wunderkind; literally \"wonder child\") is sometimes used as a synonym for child prodigy, particularly in media accounts. Child prodigy\n",
      "2. A child prodigy is, technically, a child under the age of 10 who produces meaningful work in some domain at the level of an adult expert. Child prodigy\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 3: LLM selected 2 sentences.\n",
      "M2 Iter 3: LLM selected few items (2). Lowering sBERT threshold to 0.250\n",
      "DEBUG 2. End Iter 3: Total evidence found: 6\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 4/10, Current sBERT Thresh: 0.250\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: Wolfgang Amadeus Mozart revealed that he was a musical genius at a young age.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 44\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 4: Sending 38 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 4: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "M2 Iter 4: Stopping because LLM found no evidence after two tries.\n",
      "M2: Finished. Selected 6 evidence items.\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M3: Starting Classification ######\n",
      "DEBUG 3.1 (module_3_classification):\n",
      "\tClaim: Wolfgang Amadeus Mozart showed he was a child protege.\n",
      "\tEvidence Texts Sent (6):\n",
      "\tPrompt (partial): Based ONLY on the following evidence sentences, classify the claim as SUPPORTS, REFUTES, or NOT ENOUGH INFO.\n",
      "\n",
      "Claim: 'Wolfgang Amadeus Mozart showed he was a child protege.'\n",
      "\n",
      "Evidence:\n",
      "- Despite his s...\n",
      "-_--_--_--_--_--_--_--_--_--_-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing Claims:  67%|██████▋   | 20/30 [16:34<09:11, 55.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG 3.2/3.3 (module_3_classification):\n",
      "\tLLM Classification Result: SUPPORTS\n",
      "\tExit Status: OK\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "Adding gold label/evidence to prediction.\n",
      "##########################################################################\n",
      "\n",
      "##########################################################################\n",
      "\n",
      "Time to process claim: 62.37 seconds.\n",
      "------------------------------------------------\n",
      "\n",
      "\n",
      "--- Processing Claim ID: 225798 ---\n",
      "DEBUG 1.0 (query_generator):\n",
      "\tClaim: Chinatown's writer is a convicted statutory rapist.\n",
      "\tEntities: ['Chinatown']\n",
      "\tLLM Output: ['chinatown', 'robert_towne', 'statutory_rape', 'conviction', 'criminal_law', 'sexual_offenses']\n",
      "-_--_--_--_--_-\n",
      "DEBUG 1.1 (query_generator):\n",
      "\tEntities: ['Chinatown']\n",
      "\tGenerated Potential Titles: ['chinatown', 'conviction', 'criminal_law', 'robert_towne', 'statutory_rape', 'sexual_offenses']\n",
      "\tSelected Titles for Retrieval: ['chinatown', 'conviction', 'criminal_law', 'robert_towne', 'statutory_rape', 'sexual_offenses']\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "###### M1: Retrieving Documents ######\n",
      "DEBUG 1.2: DisambiguationError for title: 'chinatown'. Options: ['List of Chinatowns', 'Template:Chinatowns', 'Chinatown, Bangkok', 'Chinatown, Binondo', 'Chinatown, Boston', 'Chinatown, Brooklyn', 'Chinatown, California (disambiguation)', 'Chinatown, Chicago', 'Chinatown, Cleveland (disambiguation)', 'Chinatown, Darwin', 'Chinatown, Flushing', 'Chinatown, Glasgow', 'Chinatown, Honiara', 'Chinatown, Honolulu', 'Chinatown, Houston', 'Chinatown, Johannesburg (disambiguation)', 'Chinatown, Kolkata', 'Chinatown, Lae', 'Chinatown, London', 'Chinatown, Los Angeles', 'Chinatown, Manhattan', 'Chinatown, Milan', 'Chinatown, Montreal', 'Chinatown, Mumbai', 'Chinatown, New Orleans', 'Chinatown, Newark, New Jersey', 'Chinatown, Philadelphia', 'Chinatown (Pittsburgh)', 'Chinatown, Portland (disambiguation)', 'Chinatown, Queens', 'Chinatown, San Francisco (disambiguation)', 'Chinatown–International District, Seattle', 'Chinatown, Singapore', 'Chinatown, South Dakota', 'Chinatown, Washington, D.C.', 'Chinatown, Wisconsin', 'China Town (1962 film)', 'Chinatown (1974 film)', '36 China Town', 'China Town (2011 film)', 'Chinatown, My Chinatown (film)', 'Captured in Chinatown', 'Grand Theft Auto: Chinatown Wars', 'Chinatown (band)', 'Chinatown (The Be Good Tanyas album)', 'Chinatown (Thin Lizzy album)', 'Chinatown, My Chinatown', '\"Chinatown\" (The Move song)', '\"Chinatown\" (Liam Gallagher song)', '\"Chinatown\" (Bleachers song)', 'Mykki Blanco & the Mutant Angels', 'Kaputt', \"Livin' on the Fault Line\", 'The Felice Brothers', 'Next of Kihn', 'Night and Day', 'Orange Rhyming Dictionary', 'Penthouse', 'Pop Beloved', 'Chinatown', 'Toto XIV', 'Sports Car', 'A Different Kind of Truth', 'Gemini', 'Naked City', '\"Chinatown\" (Due South)', '\"Chinatown\" (Entourage)', '\"Chinatown\" (Space Ghost Coast to Coast)', 'Chinatown station (disambiguation)', 'Rose Livingston']...\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Chinatown (1974 film)' (disambiguated to 'Chinatown (1974 film)')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'The Conviction' (searched for 'conviction')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Criminal Law (film)' (searched for 'criminal_law')\n",
      "DEBUG 1.2: Skipping duplicate intro for 'Chinatown (1974 film)' (searched for 'robert_towne')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Statutory rape' (searched for 'statutory_rape')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Sex offender' (searched for 'sexual_offenses')\n",
      "DEBUG 1.2: Retrieved content for 5 pages out of 6 potential titles.\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M2: Starting Evidence Extraction ######\n",
      "DEBUG 2.1 (module_2_controls):\n",
      "\tClaim: Chinatown's writer is a convicted statutory rapist.\n",
      "\tEntities: ['Chinatown']\n",
      "\tKeywords: ['writer', 'statutory', 'rapist', 'convicted', 'chinatown']\n",
      "\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n",
      "\tMax Evidence Target: 8, Max Iterations: 10\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 13\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 1: Sending 13 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 1: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "   Lowering threshold to 0.350 for next try.\n",
      "DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: The writer of Chinatown has been convicted of statutory rape.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 21\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 2: Sending 21 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 2: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "   Lowering threshold to 0.300 for next try.\n",
      "DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: The author of Chinatown has been found guilty of statutory rape.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 22\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 3: Sending 22 new candidates to LLM for selection.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing Claims:  70%|███████   | 21/30 [17:18<07:46, 51.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 3: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "M2 Iter 3: Stopping because LLM found no evidence after two tries.\n",
      "M2: Finished iterations. No evidence selected.\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M3: Starting Classification ######\n",
      "M3: No evidence text provided. Classifying as NOT ENOUGH INFO.\n",
      "Adding gold label/evidence to prediction.\n",
      "##########################################################################\n",
      "\n",
      "##########################################################################\n",
      "\n",
      "Time to process claim: 44.03 seconds.\n",
      "------------------------------------------------\n",
      "\n",
      "\n",
      "--- Processing Claim ID: 46810 ---\n",
      "DEBUG 1.0 (query_generator):\n",
      "\tClaim: One Dance has always been banned in the Netherlands.\n",
      "\tEntities: ['Netherlands', 'Dance']\n",
      "\tLLM Output: ['one_dance', 'netherlands', 'music_ban', 'drake', 'censorship_in_music', 'dance_music']\n",
      "-_--_--_--_--_-\n",
      "DEBUG 1.1 (query_generator):\n",
      "\tEntities: ['Netherlands', 'Dance']\n",
      "\tGenerated Potential Titles: ['drake', 'music_ban', 'one_dance', 'netherlands', 'dance_music', 'censorship_in_music']\n",
      "\tSelected Titles for Retrieval: ['drake', 'music_ban', 'one_dance', 'netherlands', 'dance_music', 'censorship_in_music']\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "###### M1: Retrieving Documents ######\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Some Sexy Songs 4 U' (searched for 'drake')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Dancing ban' (searched for 'music_ban')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'One Dance UK' (searched for 'one_dance')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Beer in the Netherlands' (searched for 'netherlands')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Eurodance' (searched for 'dance_music')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'List of K-pop music videos banned by South Korean television networks' (searched for 'censorship_in_music')\n",
      "DEBUG 1.2: Retrieved content for 6 pages out of 6 potential titles.\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M2: Starting Evidence Extraction ######\n",
      "DEBUG 2.1 (module_2_controls):\n",
      "\tClaim: One Dance has always been banned in the Netherlands.\n",
      "\tEntities: ['Netherlands', 'Dance']\n",
      "\tKeywords: ['netherlands', 'dance', 'banned']\n",
      "\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n",
      "\tMax Evidence Target: 8, Max Iterations: 10\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 6\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 1: Sending 6 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 1: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "   Lowering threshold to 0.350 for next try.\n",
      "DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: The Netherlands has consistently prohibited One Dance.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 8\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 2: Sending 8 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 2: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "   Lowering threshold to 0.300 for next try.\n",
      "DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: One Dance has been consistently forbidden in the Netherlands.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 18\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 3: Sending 18 new candidates to LLM for selection.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing Claims:  73%|███████▎  | 22/30 [17:59<06:27, 48.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 3: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "M2 Iter 3: Stopping because LLM found no evidence after two tries.\n",
      "M2: Finished iterations. No evidence selected.\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M3: Starting Classification ######\n",
      "M3: No evidence text provided. Classifying as NOT ENOUGH INFO.\n",
      "Adding gold label/evidence to prediction.\n",
      "##########################################################################\n",
      "\n",
      "##########################################################################\n",
      "\n",
      "Time to process claim: 40.69 seconds.\n",
      "------------------------------------------------\n",
      "\n",
      "\n",
      "--- Processing Claim ID: 85923 ---\n",
      "DEBUG 1.0 (query_generator):\n",
      "\tClaim: Adidas designs items.\n",
      "\tEntities: ['Adidas']\n",
      "\tLLM Output: ['adidas', 'adidas_products', 'sportswear', 'fashion_design', 'athletic_shoes']\n",
      "-_--_--_--_--_-\n",
      "DEBUG 1.1 (query_generator):\n",
      "\tEntities: ['Adidas']\n",
      "\tGenerated Potential Titles: ['adidas', 'sportswear', 'fashion_design', 'athletic_shoes', 'adidas_products']\n",
      "\tSelected Titles for Retrieval: ['adidas', 'sportswear', 'fashion_design', 'athletic_shoes', 'adidas_products']\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "###### M1: Retrieving Documents ######\n",
      "DEBUG 1.2: DisambiguationError for title: 'adidas'. Options: ['Adidas SC', 'The New Toronto 3', '\"A.D.I.D.A.S.\" (Korn song)', '\"A.D.I.D.A.S.\" (Killer Mike song)', 'Ras Kass', 'Get Weird']...\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Adidas SC' (disambiguated to 'Adidas SC')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Onda (sportswear)' (searched for 'sportswear')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Design' (searched for 'fashion_design')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Adidas' (searched for 'athletic_shoes')\n",
      "DEBUG 1.2: Skipping duplicate intro for 'Adidas' (searched for 'adidas_products')\n",
      "DEBUG 1.2: Retrieved content for 4 pages out of 5 potential titles.\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M2: Starting Evidence Extraction ######\n",
      "DEBUG 2.1 (module_2_controls):\n",
      "\tClaim: Adidas designs items.\n",
      "\tEntities: ['Adidas']\n",
      "\tKeywords: ['items', 'designs', 'adidas']\n",
      "\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n",
      "\tMax Evidence Target: 8, Max Iterations: 10\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 17\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 1: Sending 17 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "The three stripes are Adidas's identity mark, having been used on the company's clothing and shoe designs as a marketing aid. Adidas\n",
      "Dassler assisted in the development of spiked running shoes (spikes) for multiple athletic events. Adidas\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 1: LLM selected 2 sentences.\n",
      "M2 Iter 1: LLM selected few items (2). Lowering sBERT threshold to 0.350\n",
      "DEBUG 2. End Iter 1: Total evidence found: 2\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: Adidas creates products.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 17\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 2: Sending 15 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "4. To enhance the quality of spiked athletic footwear, he transitioned from a previous model of heavy metal spikes to utilising canvas and rubber. Adidas\n",
      "15. Dassler persuaded U.S. sprinter Jesse Owens to use his handmade spikes at the 1936 Summer Olympics. Adidas\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 2: LLM selected 2 sentences.\n",
      "M2 Iter 2: LLM selected few items (2). Lowering sBERT threshold to 0.300\n",
      "DEBUG 2. End Iter 2: Total evidence found: 4\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: Adidas produces designs for various items.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 23\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 3: Sending 19 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 3: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "M2 Iter 3: Stopping because LLM found no evidence after two tries.\n",
      "M2: Finished. Selected 4 evidence items.\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M3: Starting Classification ######\n",
      "DEBUG 3.1 (module_3_classification):\n",
      "\tClaim: Adidas designs items.\n",
      "\tEvidence Texts Sent (4):\n",
      "\tPrompt (partial): Based ONLY on the following evidence sentences, classify the claim as SUPPORTS, REFUTES, or NOT ENOUGH INFO.\n",
      "\n",
      "Claim: 'Adidas designs items.'\n",
      "\n",
      "Evidence:\n",
      "- Dassler persuaded U.S. sprinter Jesse Owens to...\n",
      "-_--_--_--_--_--_--_--_--_--_-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing Claims:  77%|███████▋  | 23/30 [18:26<04:54, 42.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG 3.2/3.3 (module_3_classification):\n",
      "\tLLM Classification Result: SUPPORTS\n",
      "\tExit Status: OK\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "Adding gold label/evidence to prediction.\n",
      "##########################################################################\n",
      "\n",
      "##########################################################################\n",
      "\n",
      "Time to process claim: 27.32 seconds.\n",
      "------------------------------------------------\n",
      "\n",
      "\n",
      "--- Processing Claim ID: 181252 ---\n",
      "DEBUG 1.0 (query_generator):\n",
      "\tClaim: Sean Gunn is an American poet.\n",
      "\tEntities: ['American', 'Gunn', 'Sean']\n",
      "\tLLM Output: ['sean_gunn', 'american_poets', 'list_of_american_poets']\n",
      "-_--_--_--_--_-\n",
      "DEBUG 1.1 (query_generator):\n",
      "\tEntities: ['American', 'Gunn', 'Sean']\n",
      "\tGenerated Potential Titles: ['sean_gunn', 'american_poets', 'list_of_american_poets']\n",
      "\tSelected Titles for Retrieval: ['sean_gunn', 'american_poets', 'list_of_american_poets']\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "###### M1: Retrieving Documents ######\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Sean Gunn' (searched for 'sean_gunn')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'American poetry' (searched for 'american_poets')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'List of poets from the United States' (searched for 'list_of_american_poets')\n",
      "DEBUG 1.2: Retrieved content for 3 pages out of 3 potential titles.\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M2: Starting Evidence Extraction ######\n",
      "DEBUG 2.1 (module_2_controls):\n",
      "\tClaim: Sean Gunn is an American poet.\n",
      "\tEntities: ['American', 'Gunn', 'Sean']\n",
      "\tKeywords: ['sean', 'poet', 'gunn', 'american']\n",
      "\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n",
      "\tMax Evidence Target: 8, Max Iterations: 10\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 23\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 1: Sending 23 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 1: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "   Lowering threshold to 0.350 for next try.\n",
      "DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: Sean Gunn is a poet from the United States.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 23\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 2: Sending 23 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 2: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "   Lowering threshold to 0.300 for next try.\n",
      "DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: Sean Gunn is a poet hailing from America.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 23\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 3: Sending 23 new candidates to LLM for selection.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing Claims:  80%|████████  | 24/30 [18:49<03:37, 36.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 3: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "M2 Iter 3: Stopping because LLM found no evidence after two tries.\n",
      "M2: Finished iterations. No evidence selected.\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M3: Starting Classification ######\n",
      "M3: No evidence text provided. Classifying as NOT ENOUGH INFO.\n",
      "Adding gold label/evidence to prediction.\n",
      "##########################################################################\n",
      "\n",
      "##########################################################################\n",
      "\n",
      "Time to process claim: 22.71 seconds.\n",
      "------------------------------------------------\n",
      "\n",
      "\n",
      "--- Processing Claim ID: 1933 ---\n",
      "DEBUG 1.0 (query_generator):\n",
      "\tClaim: Dissociative identity disorder is known as multiple personality disorder.\n",
      "\tEntities: []\n",
      "\tLLM Output: ['dissociative_identity_disorder', 'multiple_personality_disorder', 'mental_disorders', 'psychological_conditions', 'dissociation']\n",
      "-_--_--_--_--_-\n",
      "DEBUG 1.1 (query_generator):\n",
      "\tEntities: []\n",
      "\tGenerated Potential Titles: ['dissociation', 'mental_disorders', 'psychological_conditions', 'multiple_personality_disorder', 'dissociative_identity_disorder']\n",
      "\tSelected Titles for Retrieval: ['dissociation', 'mental_disorders', 'psychological_conditions', 'multiple_personality_disorder', 'dissociative_identity_disorder']\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "###### M1: Retrieving Documents ######\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Dissociative identity disorder' (searched for 'dissociation')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Classification of mental disorders' (searched for 'mental_disorders')\n",
      "DEBUG 1.2: Skipping duplicate intro for 'Dissociative identity disorder' (searched for 'psychological_conditions')\n",
      "DEBUG 1.2: Skipping duplicate intro for 'Dissociative identity disorder' (searched for 'multiple_personality_disorder')\n",
      "DEBUG 1.2: Skipping duplicate intro for 'Dissociative identity disorder' (searched for 'dissociative_identity_disorder')\n",
      "DEBUG 1.2: Retrieved content for 2 pages out of 5 potential titles.\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M2: Starting Evidence Extraction ######\n",
      "DEBUG 2.1 (module_2_controls):\n",
      "\tClaim: Dissociative identity disorder is known as multiple personality disorder.\n",
      "\tEntities: []\n",
      "\tKeywords: ['disorder', 'personality', 'multiple', 'known', 'identity']\n",
      "\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n",
      "\tMax Evidence Target: 8, Max Iterations: 10\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 27\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 1: Sending 27 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "Dissociative identity disorder (DID), previously known as multiple personality disorder (MPD), is characterized by the presence of at least two personality states or \"alters\". Dissociative identity disorder\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 1: LLM selected 1 sentences.\n",
      "M2 Iter 1: LLM selected few items (1). Lowering sBERT threshold to 0.350\n",
      "DEBUG 2. End Iter 1: Total evidence found: 1\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: Dissociative identity disorder is referred to as multiple personality disorder.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 27\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 2: Sending 26 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 2: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "   Lowering threshold to 0.300 for next try.\n",
      "DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: Dissociative identity disorder is commonly called multiple personality disorder.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 27\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 3: Sending 26 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 3: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "M2 Iter 3: Stopping because LLM found no evidence after two tries.\n",
      "M2: Finished. Selected 1 evidence items.\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M3: Starting Classification ######\n",
      "DEBUG 3.1 (module_3_classification):\n",
      "\tClaim: Dissociative identity disorder is known as multiple personality disorder.\n",
      "\tEvidence Texts Sent (1):\n",
      "\tPrompt (partial): Based ONLY on the following evidence sentences, classify the claim as SUPPORTS, REFUTES, or NOT ENOUGH INFO.\n",
      "\n",
      "Claim: 'Dissociative identity disorder is known as multiple personality disorder.'\n",
      "\n",
      "Eviden...\n",
      "-_--_--_--_--_--_--_--_--_--_-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing Claims:  83%|████████▎ | 25/30 [19:31<03:10, 38.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG 3.2/3.3 (module_3_classification):\n",
      "\tLLM Classification Result: SUPPORTS\n",
      "\tExit Status: OK\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "Adding gold label/evidence to prediction.\n",
      "##########################################################################\n",
      "\n",
      "##########################################################################\n",
      "\n",
      "Time to process claim: 42.16 seconds.\n",
      "------------------------------------------------\n",
      "\n",
      "\n",
      "--- Processing Claim ID: 88894 ---\n",
      "DEBUG 1.0 (query_generator):\n",
      "\tClaim: Zoe Saldana is a Leo.\n",
      "\tEntities: ['Saldana', 'Zoe']\n",
      "\tLLM Output: ['zoe_saldana', 'leo_(astrology)', 'zodiac_signs', 'astrology']\n",
      "-_--_--_--_--_-\n",
      "DEBUG 1.1 (query_generator):\n",
      "\tEntities: ['Saldana', 'Zoe']\n",
      "\tGenerated Potential Titles: ['astrology', 'zoe_saldana', 'zodiac_signs', 'leo_(astrology)']\n",
      "\tSelected Titles for Retrieval: ['astrology', 'zoe_saldana', 'zodiac_signs', 'leo_(astrology)']\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "###### M1: Retrieving Documents ######\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Astrology' (searched for 'astrology')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Zoe Saldaña' (searched for 'zoe_saldana')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Leo (astrology)' (searched for 'zodiac_signs')\n",
      "DEBUG 1.2: DisambiguationError for title: 'leo_(astrology)'. Options: ['Leo (constellation)', 'Leo (astrology)', 'Lateral epitaxial overgrowth', 'Law enforcement officer', 'Law enforcement organisation', 'Louisville Eccentric Observer', 'Michigan Department of Labor and Economic Opportunity', 'L.E.O. (band)', 'Leo (soundtrack)', 'Leo (2000 film)', 'Leo (2002 film)', 'Josef Fares', 'Leo (2012 film)', 'Leo (2023 American film)', 'Leo (2023 Indian film)', 'Leo the Lion (MGM)', 'Leo Awards', 'episode of Being Erica', 'Animal Crackers', 'Fabien Cloutier', 'Leo Namibia', 'Leo Pharma', 'Leo Records', 'Lioré et Olivier', 'The Leo Group', 'Leo (given name)', 'Léo', 'Leo (surname)', 'Leonid dynasty', 'Arakel Babakhanian', 'Leo (singer)', 'Léo (footballer, born 1975)', 'Leo (footballer, born November 1989)', 'Leo (footballer, born December 1989)', 'Léo (footballer, born 1990)', 'Léo (footballer, born 1992)', 'Leo (wrestler)', 'Luiz Eduardo de Oliveira', 'Léo Department', 'Léo, Burkina Faso', 'Leo-Cedarville, Indiana', 'Leo, Ohio', 'Leo, West Virginia', 'Leo Islands', 'Low Earth orbit', 'Launch and Early Orbit phase', 'LEO (spacecraft)', 'Leo (horse)', 'Leonberger', 'Long-term Ecosystem Observatory', 'Panthera', 'LEO (computer)', 'Leo (text editor)', 'BC Lions', 'The PBA Leo Awards', 'LEO (website)', 'Leo Petroglyph', 'Leo clubs', 'Leotard', 'Leos (disambiguation)', 'Lio (disambiguation)', 'St. Leo (disambiguation)']...\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Leo (astrology)' (disambiguated to 'Leo (astrology)')\n",
      "DEBUG 1.2: Retrieved content for 4 pages out of 4 potential titles.\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M2: Starting Evidence Extraction ######\n",
      "DEBUG 2.1 (module_2_controls):\n",
      "\tClaim: Zoe Saldana is a Leo.\n",
      "\tEntities: ['Saldana', 'Zoe']\n",
      "\tKeywords: ['zoe', 'saldana', 'leo']\n",
      "\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n",
      "\tMax Evidence Target: 8, Max Iterations: 10\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 25\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 1: Sending 25 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 1: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "   Lowering threshold to 0.350 for next try.\n",
      "DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: Zoe Saldana falls under the zodiac sign of Leo.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 26\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 2: Sending 26 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 2: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "   Lowering threshold to 0.300 for next try.\n",
      "DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: Zoe Saldana belongs to the Leo zodiac sign.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 34\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 3: Sending 34 new candidates to LLM for selection.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing Claims:  87%|████████▋ | 26/30 [20:06<02:28, 37.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 3: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "M2 Iter 3: Stopping because LLM found no evidence after two tries.\n",
      "M2: Finished iterations. No evidence selected.\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M3: Starting Classification ######\n",
      "M3: No evidence text provided. Classifying as NOT ENOUGH INFO.\n",
      "Adding gold label/evidence to prediction.\n",
      "##########################################################################\n",
      "\n",
      "##########################################################################\n",
      "\n",
      "Time to process claim: 35.03 seconds.\n",
      "------------------------------------------------\n",
      "\n",
      "\n",
      "--- Processing Claim ID: 17915 ---\n",
      "DEBUG 1.0 (query_generator):\n",
      "\tClaim: Fred Seibert has produced comedy programs.\n",
      "\tEntities: ['Seibert', 'Fred']\n",
      "\tLLM Output: ['fred_seibert', 'comedy_television', 'animation', 'television_producer', 'list_of_animated_television_series']\n",
      "-_--_--_--_--_-\n",
      "DEBUG 1.1 (query_generator):\n",
      "\tEntities: ['Seibert', 'Fred']\n",
      "\tGenerated Potential Titles: ['animation', 'fred_seibert', 'comedy_television', 'television_producer', 'list_of_animated_television_series']\n",
      "\tSelected Titles for Retrieval: ['animation', 'fred_seibert', 'comedy_television', 'television_producer', 'list_of_animated_television_series']\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "###### M1: Retrieving Documents ######\n",
      "DEBUG 1.2: Successfully retrieved intro from 'List of DreamWorks Animation productions' (searched for 'animation')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Fred Seibert cartoon shorts filmography' (searched for 'fred_seibert')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'List of comedy television series' (searched for 'comedy_television')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Bob Stewart (television producer)' (searched for 'television_producer')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'List of adult animated television series' (searched for 'list_of_animated_television_series')\n",
      "DEBUG 1.2: Retrieved content for 5 pages out of 5 potential titles.\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M2: Starting Evidence Extraction ######\n",
      "DEBUG 2.1 (module_2_controls):\n",
      "\tClaim: Fred Seibert has produced comedy programs.\n",
      "\tEntities: ['Seibert', 'Fred']\n",
      "\tKeywords: ['seibert', 'programs', 'produced', 'fred', 'comedy']\n",
      "\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n",
      "\tMax Evidence Target: 8, Max Iterations: 10\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 5\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 1: Sending 5 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 1: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "   Lowering threshold to 0.350 for next try.\n",
      "DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: Comedy programs have been produced by Fred Seibert.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 14\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 2: Sending 14 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "These are the complete filmographies for the cartoon shorts series created by American animation producer Fred Seibert from 1995 through 2022, at Hanna-Barbera Cartoons and his animation production companies Frederator Studios and FredFilms. Fred Seibert cartoon shorts filmography\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 2: LLM selected 1 sentences.\n",
      "M2 Iter 2: LLM selected few items (1). Lowering sBERT threshold to 0.300\n",
      "DEBUG 2. End Iter 2: Total evidence found: 1\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: Fred Seibert is responsible for the production of comedy programs.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 21\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 3: Sending 20 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 3: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "M2 Iter 3: Stopping because LLM found no evidence after two tries.\n",
      "M2: Finished. Selected 1 evidence items.\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M3: Starting Classification ######\n",
      "DEBUG 3.1 (module_3_classification):\n",
      "\tClaim: Fred Seibert has produced comedy programs.\n",
      "\tEvidence Texts Sent (1):\n",
      "\tPrompt (partial): Based ONLY on the following evidence sentences, classify the claim as SUPPORTS, REFUTES, or NOT ENOUGH INFO.\n",
      "\n",
      "Claim: 'Fred Seibert has produced comedy programs.'\n",
      "\n",
      "Evidence:\n",
      "- These are the complete fi...\n",
      "-_--_--_--_--_--_--_--_--_--_-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing Claims:  90%|█████████ | 27/30 [20:39<01:47, 35.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG 3.2/3.3 (module_3_classification):\n",
      "\tLLM Classification Result: NOT ENOUGH INFO\n",
      "\tExit Status: NOT ENOUGH INFO\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "Adding gold label/evidence to prediction.\n",
      "##########################################################################\n",
      "\n",
      "##########################################################################\n",
      "\n",
      "Time to process claim: 32.94 seconds.\n",
      "------------------------------------------------\n",
      "\n",
      "\n",
      "--- Processing Claim ID: 58396 ---\n",
      "DEBUG 1.0 (query_generator):\n",
      "\tClaim: Konidela Production Company was established.\n",
      "\tEntities: ['Production Company', 'Konidela']\n",
      "\tLLM Output: ['konidela_production_company', 'chiranjeevi', 'telugu_cinema', 'film_production_companies_in_india']\n",
      "-_--_--_--_--_-\n",
      "DEBUG 1.1 (query_generator):\n",
      "\tEntities: ['Production Company', 'Konidela']\n",
      "\tGenerated Potential Titles: ['chiranjeevi', 'telugu_cinema', 'konidela_production_company', 'film_production_companies_in_india']\n",
      "\tSelected Titles for Retrieval: ['chiranjeevi', 'telugu_cinema', 'konidela_production_company', 'film_production_companies_in_india']\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "###### M1: Retrieving Documents ######\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Konidela–Allu family' (searched for 'chiranjeevi')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Cinema of India' (searched for 'telugu_cinema')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Konidela Production Company' (searched for 'konidela_production_company')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Kavithalayaa Productions' (searched for 'film_production_companies_in_india')\n",
      "DEBUG 1.2: Retrieved content for 4 pages out of 4 potential titles.\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M2: Starting Evidence Extraction ######\n",
      "DEBUG 2.1 (module_2_controls):\n",
      "\tClaim: Konidela Production Company was established.\n",
      "\tEntities: ['Production Company', 'Konidela']\n",
      "\tKeywords: ['production', 'konidela', 'established', 'company']\n",
      "\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n",
      "\tMax Evidence Target: 8, Max Iterations: 10\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 8\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 1: Sending 8 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "Konidela Production Company is an Indian film production company established by actor Ram Charan. Konidela Production Company\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 1: LLM selected 1 sentences.\n",
      "M2 Iter 1: LLM selected few items (1). Lowering sBERT threshold to 0.350\n",
      "DEBUG 2. End Iter 1: Total evidence found: 1\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: The founding of Konidela Production Company took place.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 8\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 2: Sending 7 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 2: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "   Lowering threshold to 0.300 for next try.\n",
      "DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: The establishment of Konidela Production Company occurred.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 9\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 3: Sending 8 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 3: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "M2 Iter 3: Stopping because LLM found no evidence after two tries.\n",
      "M2: Finished. Selected 1 evidence items.\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M3: Starting Classification ######\n",
      "DEBUG 3.1 (module_3_classification):\n",
      "\tClaim: Konidela Production Company was established.\n",
      "\tEvidence Texts Sent (1):\n",
      "\tPrompt (partial): Based ONLY on the following evidence sentences, classify the claim as SUPPORTS, REFUTES, or NOT ENOUGH INFO.\n",
      "\n",
      "Claim: 'Konidela Production Company was established.'\n",
      "\n",
      "Evidence:\n",
      "- Konidela Production Com...\n",
      "-_--_--_--_--_--_--_--_--_--_-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing Claims:  93%|█████████▎| 28/30 [21:08<01:07, 33.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG 3.2/3.3 (module_3_classification):\n",
      "\tLLM Classification Result: SUPPORTS\n",
      "\tExit Status: OK\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "Adding gold label/evidence to prediction.\n",
      "##########################################################################\n",
      "\n",
      "##########################################################################\n",
      "\n",
      "Time to process claim: 28.96 seconds.\n",
      "------------------------------------------------\n",
      "\n",
      "\n",
      "--- Processing Claim ID: 150751 ---\n",
      "DEBUG 1.0 (query_generator):\n",
      "\tClaim: Paul von Hindenburg was a man.\n",
      "\tEntities: ['Paul']\n",
      "\tLLM Output: ['paul_von_hindenburg', 'man', 'german_field_marshals', 'politicians_of_the_weimar_republic', 'presidents_of_germany']\n",
      "-_--_--_--_--_-\n",
      "DEBUG 1.1 (query_generator):\n",
      "\tEntities: ['Paul']\n",
      "\tGenerated Potential Titles: ['man', 'paul_von_hindenburg', 'presidents_of_germany', 'german_field_marshals', 'politicians_of_the_weimar_republic']\n",
      "\tSelected Titles for Retrieval: ['man', 'paul_von_hindenburg', 'presidents_of_germany', 'german_field_marshals', 'politicians_of_the_weimar_republic']\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "###### M1: Retrieving Documents ######\n",
      "DEBUG 1.2: DisambiguationError for title: 'man'. Options: ['Isle of Man', 'Man Island (Andaman and Nicobar Islands)', 'Man (Vidhan Sabha constituency)', 'Man Department', 'Man, Ivory Coast', 'Roman Catholic Diocese of Man', 'Man, West Virginia', 'Manitoba', 'Man, Vikramgad', 'Man, Jamaica', 'Manchester Airport', 'Manchester Piccadilly station', 'Man (band)', 'Man (Man album)', 'Man (Neneh Cherry album)', '\"Man\" (Skepta song)', '\"Man\" (JoJo song)', 'Staring at the Sun', 'Fever to Tell', 'Air for Free', 'Blue Madonna', 'Netflix', 'MAN Truck & Bus', 'Man Group', 'Manpower, Inc.', 'Great National Assembly', \"Mongolian People's Party\", 'Movement of Arab Nationalists', 'National Action Movement (Portugal)', 'National Action Movement (Venezuela)', 'National Alternative Movement', 'Partido MAN', 'Metropolitan area network', 'man page', 'Edward Man', 'Man!', 'Man (name)', 'Man (word)', 'The Man', 'Nanman', 'Manchu people', 'Man (Middle-earth)', 'Man (journal)', 'Man (unit)', 'Virgil Cantini', 'Ultraman', 'Mard (1985 film)', 'Algiz', 'chess piece', 'All pages with titles containing man', 'De Man (disambiguation)', 'Mankind (disambiguation)', 'Men (disambiguation)', 'The Man (disambiguation)', 'The Men (disambiguation)', 'Mans (disambiguation)', 'Masculinity', 'Gentleman']...\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Man (name)' (disambiguated to 'Man (name)')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Paul von Hindenburg' (searched for 'paul_von_hindenburg')\n",
      "DEBUG 1.2: DisambiguationError for title: 'presidents_of_germany'. Options: ['President of the Federal Republic of Germany', 'president of Germany during the Weimar Republic', 'president', 'William I', 'List of German presidents']...\n",
      "Warning: Error retrieving disambiguated page 'president of Germany during the Weimar Republic': Page id \"president of Germany during the Weimar Republic\" does not match any pages. Try another id!\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Ernst Busch (field marshal)' (searched for 'german_field_marshals')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Hermann Dietrich' (searched for 'politicians_of_the_weimar_republic')\n",
      "DEBUG 1.2: Retrieved content for 4 pages out of 5 potential titles.\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M2: Starting Evidence Extraction ######\n",
      "DEBUG 2.1 (module_2_controls):\n",
      "\tClaim: Paul von Hindenburg was a man.\n",
      "\tEntities: ['Paul']\n",
      "\tKeywords: ['von', 'paul', 'man', 'hindenburg']\n",
      "\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n",
      "\tMax Evidence Target: 8, Max Iterations: 10\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 22\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 1: Sending 22 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "Paul Ludwig Hans Anton von Beneckendorff und von Hindenburg (2 October 1847 – 2 August 1934) was a Prussian-born German military officer and politician who led the Imperial German Army during the First World War and later became president of Germany from 1925 until his death in 1934. Paul von Hindenburg\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 1: LLM selected 1 sentences.\n",
      "M2 Iter 1: LLM selected few items (1). Lowering sBERT threshold to 0.350\n",
      "DEBUG 2. End Iter 1: Total evidence found: 1\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: Paul von Hindenburg was an individual of male gender.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 25\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 2: Sending 24 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "He oversaw crushing victories against the Russians that made him a national hero and the center of a pervasive cult of personality. Paul von Hindenburg\n",
      "\n",
      "After World War I began in 1914, Hindenburg was recalled and achieved fame on the Eastern Front as the victor of Tannenberg. Paul von Hindenburg\n",
      "\n",
      "He played a key role in the Nazi seizure of power in 1933 when he appointed Adolf Hitler as chancellor of Germany. Paul von Hindenburg\n",
      "\n",
      "In 1885, he was promoted to major and became a member of the German General Staff. Paul von Hindenburg\n",
      "\n",
      "He saw combat during the Austro-Prussian and Franco-Prussian wars. Paul von Hindenburg\n",
      "\n",
      "Upon completing his education as a cadet, he enlisted in the Third Regiment of Foot Guards as a second lieutenant. Paul von Hindenburg\n",
      "\n",
      "In 1873, he was admitted to the prestigious War Academy in Berlin, where he studied before being appointed to the General Staff Corps. Paul von Hindenburg\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 2: LLM selected 7 sentences.\n",
      "DEBUG 2. End Iter 2: Total evidence found: 8\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 2: Reached target evidence count (8).\n",
      "M2: Finished. Selected 8 evidence items.\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M3: Starting Classification ######\n",
      "DEBUG 3.1 (module_3_classification):\n",
      "\tClaim: Paul von Hindenburg was a man.\n",
      "\tEvidence Texts Sent (8):\n",
      "\tPrompt (partial): Based ONLY on the following evidence sentences, classify the claim as SUPPORTS, REFUTES, or NOT ENOUGH INFO.\n",
      "\n",
      "Claim: 'Paul von Hindenburg was a man.'\n",
      "\n",
      "Evidence:\n",
      "- He saw combat during the Austro-Pruss...\n",
      "-_--_--_--_--_--_--_--_--_--_-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing Claims:  97%|█████████▋| 29/30 [21:43<00:34, 34.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG 3.2/3.3 (module_3_classification):\n",
      "\tLLM Classification Result: SUPPORTS\n",
      "\tExit Status: OK\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "Adding gold label/evidence to prediction.\n",
      "##########################################################################\n",
      "\n",
      "##########################################################################\n",
      "\n",
      "Time to process claim: 35.66 seconds.\n",
      "------------------------------------------------\n",
      "\n",
      "\n",
      "--- Processing Claim ID: 179831 ---\n",
      "DEBUG 1.0 (query_generator):\n",
      "\tClaim: Vic Mensa was born June 12, 1993.\n",
      "\tEntities: ['Mensa', 'Vic']\n",
      "\tLLM Output: ['vic_mensa', 'list_of_rappers_from_chicago', '2010s_rap_music', 'hip_hop_music']\n",
      "-_--_--_--_--_-\n",
      "DEBUG 1.1 (query_generator):\n",
      "\tEntities: ['Mensa', 'Vic']\n",
      "\tGenerated Potential Titles: ['vic_mensa', 'hip_hop_music', '2010s_rap_music', 'list_of_rappers_from_chicago']\n",
      "\tSelected Titles for Retrieval: ['vic_mensa', 'hip_hop_music', '2010s_rap_music', 'list_of_rappers_from_chicago']\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "###### M1: Retrieving Documents ######\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Vic Mensa' (searched for 'vic_mensa')\n",
      "DEBUG 1.2: Successfully retrieved intro from '1999 in hip-hop' (searched for 'hip_hop_music')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'Rap metal' (searched for '2010s_rap_music')\n",
      "DEBUG 1.2: Successfully retrieved intro from 'List of LGBTQ people from Chicago' (searched for 'list_of_rappers_from_chicago')\n",
      "DEBUG 1.2: Retrieved content for 4 pages out of 4 potential titles.\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M2: Starting Evidence Extraction ######\n",
      "DEBUG 2.1 (module_2_controls):\n",
      "\tClaim: Vic Mensa was born June 12, 1993.\n",
      "\tEntities: ['Mensa', 'Vic']\n",
      "\tKeywords: ['vic', 'mensa', 'june', 'born', '1993']\n",
      "\tInitial sBERT Thresh: 0.4, Min Thresh: 0.1\n",
      "\tMax Evidence Target: 8, Max Iterations: 10\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 1/10, Current sBERT Thresh: 0.400\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 8\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 1: Sending 8 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 1: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "   Lowering threshold to 0.350 for next try.\n",
      "DEBUG 2. Iteration 2/10, Current sBERT Thresh: 0.350\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: Vic Mensa's birth date is June 12, 1993.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 8\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 2: Sending 8 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 2: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "   Lowering threshold to 0.300 for next try.\n",
      "DEBUG 2. Iteration 3/10, Current sBERT Thresh: 0.300\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: Vic Mensa's date of birth is June 12, 1993.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 8\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 3: Sending 8 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "Victor Kwesi Mensah (born June 6, 1993), known professionally as Vic Mensa, is an American rapper. Vic Mensa\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 3: LLM selected 1 sentences.\n",
      "M2 Iter 3: LLM selected few items (1). Lowering sBERT threshold to 0.250\n",
      "DEBUG 2. End Iter 3: Total evidence found: 1\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "DEBUG 2. Iteration 4/10, Current sBERT Thresh: 0.250\n",
      "DEBUG 2.1.1 (rephrase_claim):\n",
      "\tRephrased Claim: Vic Mensa's birthday is June 12, 1993.\n",
      "-_--_--_--_--_-\n",
      "DEBUG 2.2.2 (sbert_filter):\n",
      "\tTotal candidates found across all docs: 8\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "M2 Iter 4: Sending 7 new candidates to LLM for selection.\n",
      "DEBUG 2.3.3 (LLM Selection):\n",
      "\tLLM Raw Output:\n",
      "NOT ENOUGH INFO\n",
      "-_--_--_--_--_-\n",
      "M2 Iter 4: LLM indicated 'NOT ENOUGH INFO' from the provided candidates.\n",
      "M2 Iter 4: Stopping because LLM found no evidence after two tries.\n",
      "M2: Finished. Selected 1 evidence items.\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###### M3: Starting Classification ######\n",
      "DEBUG 3.1 (module_3_classification):\n",
      "\tClaim: Vic Mensa was born June 12, 1993.\n",
      "\tEvidence Texts Sent (1):\n",
      "\tPrompt (partial): Based ONLY on the following evidence sentences, classify the claim as SUPPORTS, REFUTES, or NOT ENOUGH INFO.\n",
      "\n",
      "Claim: 'Vic Mensa was born June 12, 1993.'\n",
      "\n",
      "Evidence:\n",
      "- Victor Kwesi Mensah (born June 6, ...\n",
      "-_--_--_--_--_--_--_--_--_--_-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Claims: 100%|██████████| 30/30 [22:11<00:00, 44.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG 3.2/3.3 (module_3_classification):\n",
      "\tLLM Classification Result: REFUTES\n",
      "\tExit Status: OK\n",
      "-_--_--_--_--_--_--_--_--_--_-\n",
      "Adding gold label/evidence to prediction.\n",
      "##########################################################################\n",
      "\n",
      "##########################################################################\n",
      "\n",
      "Time to process claim: 27.34 seconds.\n",
      "------------------------------------------------\n",
      "\n",
      "\n",
      "Finished processing 30 claims.\n",
      "\n",
      "--- FEVER Scoring Results ---\n",
      "Strict Score (Exact Match): 50.00%\n",
      "Label Accuracy: 76.67%\n",
      "Evidence Precision: 47.50%\n",
      "Evidence Recall: 35.00%\n",
      "Evidence F1 Score: 40.30%\n",
      "Number of test cases scored: 30\n",
      "\n",
      "--- Sample Predictions (Output Format) ---\n",
      "{\n",
      "  \"id\": 113501,\n",
      "  \"predicted_label\": \"REFUTES\",\n",
      "  \"predicted_evidence\": [\n",
      "    [\n",
      "      \"Grease_-LRB-film-RRB-\",\n",
      "      3\n",
      "    ],\n",
      "    [\n",
      "      \"Grease_-LRB-film-RRB-\",\n",
      "      5\n",
      "    ]\n",
      "  ],\n",
      "  \"label\": \"NOT ENOUGH INFO\",\n",
      "  \"evidence\": []\n",
      "}\n",
      "{\n",
      "  \"id\": 163803,\n",
      "  \"predicted_label\": \"NOT ENOUGH INFO\",\n",
      "  \"predicted_evidence\": [],\n",
      "  \"label\": \"SUPPORTS\",\n",
      "  \"evidence\": [\n",
      "    [\n",
      "      [\n",
      "        null,\n",
      "        null,\n",
      "        \"Ukrainian_Soviet_Socialist_Republic\",\n",
      "        7\n",
      "      ]\n",
      "    ],\n",
      "    [\n",
      "      [\n",
      "        null,\n",
      "        null,\n",
      "        \"Ukrainian_Soviet_Socialist_Republic\",\n",
      "        7\n",
      "      ],\n",
      "      [\n",
      "        null,\n",
      "        null,\n",
      "        \"United_Nations\",\n",
      "        0\n",
      "      ]\n",
      "    ],\n",
      "    [\n",
      "      [\n",
      "        null,\n",
      "        null,\n",
      "        \"Ukrainian_Soviet_Socialist_Republic\",\n",
      "        7\n",
      "      ]\n",
      "    ],\n",
      "    [\n",
      "      [\n",
      "        null,\n",
      "        null,\n",
      "        \"Ukrainian_Soviet_Socialist_Republic\",\n",
      "        7\n",
      "      ]\n",
      "    ],\n",
      "    [\n",
      "      [\n",
      "        null,\n",
      "        null,\n",
      "        \"Ukrainian_Soviet_Socialist_Republic\",\n",
      "        7\n",
      "      ]\n",
      "    ],\n",
      "    [\n",
      "      [\n",
      "        null,\n",
      "        null,\n",
      "        \"Ukrainian_Soviet_Socialist_Republic\",\n",
      "        7\n",
      "      ]\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"id\": 70041,\n",
      "  \"predicted_label\": \"SUPPORTS\",\n",
      "  \"predicted_evidence\": [\n",
      "    [\n",
      "      \"Two_Hearts_-LRB-Kish_Mauve_song-RRB-\",\n",
      "      0\n",
      "    ],\n",
      "    [\n",
      "      \"Two_Hearts_-LRB-Kish_Mauve_song-RRB-\",\n",
      "      2\n",
      "    ],\n",
      "    [\n",
      "      \"Kylie_Minogue\",\n",
      "      13\n",
      "    ],\n",
      "    [\n",
      "      \"Two_Hearts_-LRB-Kish_Mauve_song-RRB-\",\n",
      "      3\n",
      "    ]\n",
      "  ],\n",
      "  \"label\": \"SUPPORTS\",\n",
      "  \"evidence\": [\n",
      "    [\n",
      "      [\n",
      "        null,\n",
      "        null,\n",
      "        \"2_Hearts_-LRB-Kylie_Minogue_song-RRB-\",\n",
      "        0\n",
      "      ]\n",
      "    ],\n",
      "    [\n",
      "      [\n",
      "        null,\n",
      "        null,\n",
      "        \"2_Hearts_-LRB-Kylie_Minogue_song-RRB-\",\n",
      "        0\n",
      "      ]\n",
      "    ],\n",
      "    [\n",
      "      [\n",
      "        null,\n",
      "        null,\n",
      "        \"2_Hearts_-LRB-Kylie_Minogue_song-RRB-\",\n",
      "        0\n",
      "      ]\n",
      "    ],\n",
      "    [\n",
      "      [\n",
      "        null,\n",
      "        null,\n",
      "        \"2_Hearts_-LRB-Kylie_Minogue_song-RRB-\",\n",
      "        0\n",
      "      ]\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"id\": 202314,\n",
      "  \"predicted_label\": \"REFUTES\",\n",
      "  \"predicted_evidence\": [\n",
      "    [\n",
      "      \"New_Jersey_Turnpike\",\n",
      "      14\n",
      "    ]\n",
      "  ],\n",
      "  \"label\": \"REFUTES\",\n",
      "  \"evidence\": [\n",
      "    [\n",
      "      [\n",
      "        null,\n",
      "        null,\n",
      "        \"New_Jersey_Turnpike\",\n",
      "        15\n",
      "      ]\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"id\": 57085,\n",
      "  \"predicted_label\": \"NOT ENOUGH INFO\",\n",
      "  \"predicted_evidence\": [],\n",
      "  \"label\": \"NOT ENOUGH INFO\",\n",
      "  \"evidence\": []\n",
      "}\n",
      "{\n",
      "  \"id\": 6032,\n",
      "  \"predicted_label\": \"REFUTES\",\n",
      "  \"predicted_evidence\": [\n",
      "    [\n",
      "      \"Aruba\",\n",
      "      5\n",
      "    ],\n",
      "    [\n",
      "      \"ABC_islands_-LRB-Leeward_Antilles-RRB-\",\n",
      "      0\n",
      "    ]\n",
      "  ],\n",
      "  \"label\": \"REFUTES\",\n",
      "  \"evidence\": [\n",
      "    [\n",
      "      [\n",
      "        null,\n",
      "        null,\n",
      "        \"ABC_islands_-LRB-Lesser_Antilles-RRB-\",\n",
      "        0\n",
      "      ]\n",
      "    ],\n",
      "    [\n",
      "      [\n",
      "        null,\n",
      "        null,\n",
      "        \"ABC_islands_-LRB-Lesser_Antilles-RRB-\",\n",
      "        1\n",
      "      ]\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"id\": 176630,\n",
      "  \"predicted_label\": \"NOT ENOUGH INFO\",\n",
      "  \"predicted_evidence\": [],\n",
      "  \"label\": \"NOT ENOUGH INFO\",\n",
      "  \"evidence\": []\n",
      "}\n",
      "{\n",
      "  \"id\": 130048,\n",
      "  \"predicted_label\": \"REFUTES\",\n",
      "  \"predicted_evidence\": [\n",
      "    [\n",
      "      \"Burbank,_California\",\n",
      "      4\n",
      "    ],\n",
      "    [\n",
      "      \"Burbank,_California\",\n",
      "      8\n",
      "    ]\n",
      "  ],\n",
      "  \"label\": \"REFUTES\",\n",
      "  \"evidence\": [\n",
      "    [\n",
      "      [\n",
      "        null,\n",
      "        null,\n",
      "        \"Burbank,_California\",\n",
      "        7\n",
      "      ]\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"id\": 100046,\n",
      "  \"predicted_label\": \"NOT ENOUGH INFO\",\n",
      "  \"predicted_evidence\": [],\n",
      "  \"label\": \"NOT ENOUGH INFO\",\n",
      "  \"evidence\": []\n",
      "}\n",
      "{\n",
      "  \"id\": 204575,\n",
      "  \"predicted_label\": \"REFUTES\",\n",
      "  \"predicted_evidence\": [\n",
      "    [\n",
      "      \"Commodore_-LRB-rank-RRB-\",\n",
      "      1\n",
      "    ]\n",
      "  ],\n",
      "  \"label\": \"REFUTES\",\n",
      "  \"evidence\": [\n",
      "    [\n",
      "      [\n",
      "        null,\n",
      "        null,\n",
      "        \"Commodore_-LRB-rank-RRB-\",\n",
      "        0\n",
      "      ]\n",
      "    ],\n",
      "    [\n",
      "      [\n",
      "        null,\n",
      "        null,\n",
      "        \"Commodore_-LRB-rank-RRB-\",\n",
      "        9\n",
      "      ],\n",
      "      [\n",
      "        null,\n",
      "        null,\n",
      "        \"Rear_admiral\",\n",
      "        0\n",
      "      ]\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"id\": 107539,\n",
      "  \"predicted_label\": \"NOT ENOUGH INFO\",\n",
      "  \"predicted_evidence\": [],\n",
      "  \"label\": \"NOT ENOUGH INFO\",\n",
      "  \"evidence\": []\n",
      "}\n",
      "{\n",
      "  \"id\": 164883,\n",
      "  \"predicted_label\": \"SUPPORTS\",\n",
      "  \"predicted_evidence\": [\n",
      "    [\n",
      "      \"Hezbollah\\u2013Iran_relations\",\n",
      "      1\n",
      "    ],\n",
      "    [\n",
      "      \"Hezbollah\\u2013Iran_relations\",\n",
      "      8\n",
      "    ],\n",
      "    [\n",
      "      \"Hezbollah\\u2013Iran_relations\",\n",
      "      4\n",
      "    ],\n",
      "    [\n",
      "      \"Iran_and_state-sponsored_terrorism\",\n",
      "      0\n",
      "    ],\n",
      "    [\n",
      "      \"Iran_and_state-sponsored_terrorism\",\n",
      "      3\n",
      "    ]\n",
      "  ],\n",
      "  \"label\": \"SUPPORTS\",\n",
      "  \"evidence\": [\n",
      "    [\n",
      "      [\n",
      "        null,\n",
      "        null,\n",
      "        \"Hezbollah\",\n",
      "        7\n",
      "      ]\n",
      "    ],\n",
      "    [\n",
      "      [\n",
      "        null,\n",
      "        null,\n",
      "        \"Hezbollah\",\n",
      "        7\n",
      "      ]\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"id\": 54298,\n",
      "  \"predicted_label\": \"SUPPORTS\",\n",
      "  \"predicted_evidence\": [\n",
      "    [\n",
      "      \"Electric_chair\",\n",
      "      8\n",
      "    ],\n",
      "    [\n",
      "      \"Electric_chair\",\n",
      "      10\n",
      "    ],\n",
      "    [\n",
      "      \"Electric_chair\",\n",
      "      14\n",
      "    ],\n",
      "    [\n",
      "      \"Electric_chair\",\n",
      "      11\n",
      "    ]\n",
      "  ],\n",
      "  \"label\": \"SUPPORTS\",\n",
      "  \"evidence\": [\n",
      "    [\n",
      "      [\n",
      "        null,\n",
      "        null,\n",
      "        \"Electric_chair\",\n",
      "        0\n",
      "      ],\n",
      "      [\n",
      "        null,\n",
      "        null,\n",
      "        \"Capital_punishment\",\n",
      "        1\n",
      "      ],\n",
      "      [\n",
      "        null,\n",
      "        null,\n",
      "        \"Electric_chair\",\n",
      "        14\n",
      "      ]\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"id\": 222749,\n",
      "  \"predicted_label\": \"NOT ENOUGH INFO\",\n",
      "  \"predicted_evidence\": [],\n",
      "  \"label\": \"NOT ENOUGH INFO\",\n",
      "  \"evidence\": []\n",
      "}\n",
      "{\n",
      "  \"id\": 219675,\n",
      "  \"predicted_label\": \"REFUTES\",\n",
      "  \"predicted_evidence\": [\n",
      "    [\n",
      "      \"Italian_occupation_of_Corsica\",\n",
      "      0\n",
      "    ]\n",
      "  ],\n",
      "  \"label\": \"REFUTES\",\n",
      "  \"evidence\": [\n",
      "    [\n",
      "      [\n",
      "        null,\n",
      "        null,\n",
      "        \"Corsica\",\n",
      "        5\n",
      "      ]\n",
      "    ],\n",
      "    [\n",
      "      [\n",
      "        null,\n",
      "        null,\n",
      "        \"Corsica\",\n",
      "        0\n",
      "      ]\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"id\": 134850,\n",
      "  \"predicted_label\": \"REFUTES\",\n",
      "  \"predicted_evidence\": [\n",
      "    [\n",
      "      \"Ice-T\",\n",
      "      1\n",
      "    ],\n",
      "    [\n",
      "      \"Ice-T\",\n",
      "      2\n",
      "    ],\n",
      "    [\n",
      "      \"Ice-T\",\n",
      "      3\n",
      "    ],\n",
      "    [\n",
      "      \"Ice-T\",\n",
      "      12\n",
      "    ],\n",
      "    [\n",
      "      \"Ice-T\",\n",
      "      0\n",
      "    ]\n",
      "  ],\n",
      "  \"label\": \"REFUTES\",\n",
      "  \"evidence\": [\n",
      "    [\n",
      "      [\n",
      "        null,\n",
      "        null,\n",
      "        \"Ice-T\",\n",
      "        1\n",
      "      ]\n",
      "    ],\n",
      "    [\n",
      "      [\n",
      "        null,\n",
      "        null,\n",
      "        \"Ice-T\",\n",
      "        2\n",
      "      ]\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"id\": 124578,\n",
      "  \"predicted_label\": \"SUPPORTS\",\n",
      "  \"predicted_evidence\": [\n",
      "    [\n",
      "      \"Gettysburg_Address\",\n",
      "      0\n",
      "    ],\n",
      "    [\n",
      "      \"Gettysburg_Address\",\n",
      "      2\n",
      "    ],\n",
      "    [\n",
      "      \"Gettysburg_Address\",\n",
      "      5\n",
      "    ],\n",
      "    [\n",
      "      \"Gettysburg_Address\",\n",
      "      10\n",
      "    ],\n",
      "    [\n",
      "      \"Gettysburg_Address\",\n",
      "      4\n",
      "    ],\n",
      "    [\n",
      "      \"Gettysburg_Address\",\n",
      "      8\n",
      "    ],\n",
      "    [\n",
      "      \"Gettysburg_Address\",\n",
      "      1\n",
      "    ],\n",
      "    [\n",
      "      \"Gettysburg_Address\",\n",
      "      6\n",
      "    ]\n",
      "  ],\n",
      "  \"label\": \"SUPPORTS\",\n",
      "  \"evidence\": [\n",
      "    [\n",
      "      [\n",
      "        null,\n",
      "        null,\n",
      "        \"Gettysburg_Address\",\n",
      "        13\n",
      "      ]\n",
      "    ],\n",
      "    [\n",
      "      [\n",
      "        null,\n",
      "        null,\n",
      "        \"Gettysburg_Address\",\n",
      "        14\n",
      "      ]\n",
      "    ],\n",
      "    [\n",
      "      [\n",
      "        null,\n",
      "        null,\n",
      "        \"Gettysburg_Address\",\n",
      "        0\n",
      "      ]\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"id\": 134126,\n",
      "  \"predicted_label\": \"NOT ENOUGH INFO\",\n",
      "  \"predicted_evidence\": [],\n",
      "  \"label\": \"REFUTES\",\n",
      "  \"evidence\": [\n",
      "    [\n",
      "      [\n",
      "        null,\n",
      "        null,\n",
      "        \"Jason_Bourne_-LRB-film-RRB-\",\n",
      "        6\n",
      "      ]\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"id\": 125577,\n",
      "  \"predicted_label\": \"NOT ENOUGH INFO\",\n",
      "  \"predicted_evidence\": [],\n",
      "  \"label\": \"REFUTES\",\n",
      "  \"evidence\": [\n",
      "    [\n",
      "      [\n",
      "        null,\n",
      "        null,\n",
      "        \"Ron_Dennis\",\n",
      "        0\n",
      "      ]\n",
      "    ],\n",
      "    [\n",
      "      [\n",
      "        null,\n",
      "        null,\n",
      "        \"Ron_Dennis\",\n",
      "        1\n",
      "      ]\n",
      "    ],\n",
      "    [\n",
      "      [\n",
      "        null,\n",
      "        null,\n",
      "        \"Ron_Dennis\",\n",
      "        28\n",
      "      ]\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"id\": 132244,\n",
      "  \"predicted_label\": \"SUPPORTS\",\n",
      "  \"predicted_evidence\": [\n",
      "    [\n",
      "      \"Wolfgang_Amadeus_Mozart\",\n",
      "      5\n",
      "    ],\n",
      "    [\n",
      "      \"Wolfgang_Amadeus_Mozart\",\n",
      "      4\n",
      "    ],\n",
      "    [\n",
      "      \"Wolfgang_Amadeus_Mozart\",\n",
      "      1\n",
      "    ],\n",
      "    [\n",
      "      \"Wolfgang_Amadeus_Mozart\",\n",
      "      6\n",
      "    ],\n",
      "    [\n",
      "      \"Child_prodigy\",\n",
      "      2\n",
      "    ],\n",
      "    [\n",
      "      \"Child_prodigy\",\n",
      "      0\n",
      "    ]\n",
      "  ],\n",
      "  \"label\": \"NOT ENOUGH INFO\",\n",
      "  \"evidence\": []\n",
      "}\n",
      "\n",
      "--- Run Report ---\n",
      "        id                                              claim  time_to_check                                   entities                                           keywords  retrieved_pages   module2_status                             predicted_evidence_ids                           predicted_evidence_texts   module3_result   module3_status                                     module3_prompt                             module1_report_details                             module2_report_details  query_client_temp  rephrase_client_temp  sentEx_client_temp  nli_client_temp  disambiguate_client_temp  strict_score  label_accuracy  precision  recall       f1\n",
      "0   113501                            Grease had bad reviews.      43.821479                                     Grease                               reviews, grease, bad              NaN               OK  [[\"Grease_-LRB-film-RRB-\", 3], [\"Grease_-LRB-f...  [\"The film was successful both critically and ...          REFUTES               OK  Based ONLY on the following evidence sentences...  {\"mod_1_total_documents\": 5, \"total_document_t...  {\"claim\": \"Grease had bad reviews.\", \"final_ev...                0.5                   0.9                 0.2              0.1                       0.2           0.5        0.766667      0.475    0.35  0.40303\n",
      "1   163803  Ukrainian Soviet Socialist Republic was a foun...      46.741827  Socialist Republic, Ukrainian, Soviet, UN  ukrainian, soviet, socialist, republic, partic...              NaN               OK  [[\"Republics_of_the_Soviet_Union\", 3], [\"Repub...  [\"As a result of its status as a sovereign sta...  NOT ENOUGH INFO  NOT ENOUGH INFO  Based ONLY on the following evidence sentences...  {\"mod_1_total_documents\": 5, \"total_document_t...  {\"claim\": \"Ukrainian Soviet Socialist Republic...                0.5                   0.9                 0.2              0.1                       0.2           0.5        0.766667      0.475    0.35  0.40303\n",
      "2    70041      2 Hearts is a musical composition by Minogue.      65.181998                                    Minogue              musical, minogue, hearts, composition              NaN               OK  [[\"Two_Hearts_-LRB-Kish_Mauve_song-RRB-\", 0], ...  [\"Minogue's version was released on 9 November...         SUPPORTS               OK  Based ONLY on the following evidence sentences...  {\"mod_1_total_documents\": 3, \"total_document_t...  {\"claim\": \"2 Hearts is a musical composition b...                0.5                   0.9                 0.2              0.1                       0.2           0.5        0.766667      0.475    0.35  0.40303\n",
      "3   202314        The New Jersey Turnpike has zero shoulders.      46.425935                        New Jersey Turnpike             zero, turnpike, shoulders, new, jersey              NaN               OK                      [[\"New_Jersey_Turnpike\", 14]]  [\"The turnpike has 12-foot-wide (3.7 m) lanes,...          REFUTES               OK  Based ONLY on the following evidence sentences...  {\"mod_1_total_documents\": 3, \"total_document_t...  {\"claim\": \"The New Jersey Turnpike has zero sh...                0.5                   0.9                 0.2              0.1                       0.2           0.5        0.766667      0.475    0.35  0.40303\n",
      "4    57085  Legendary Entertainment is the owner of Wanda ...      30.241279                              Wanda Cinemas    wanda, owner, legendary, entertainment, cinemas              NaN  NOT ENOUGH INFO                                                 []                                                 []  NOT ENOUGH INFO  NOT ENOUGH INFO                       No evidence provided to LLM.  {\"mod_1_total_documents\": 5, \"total_document_t...  {\"claim\": \"Legendary Entertainment is the owne...                0.5                   0.9                 0.2              0.1                       0.2           0.5        0.766667      0.475    0.35  0.40303\n",
      "..     ...                                                ...            ...                                        ...                                                ...              ...              ...                                                ...                                                ...              ...              ...                                                ...                                                ...                                                ...                ...                   ...                 ...              ...                       ...           ...             ...        ...     ...      ...\n",
      "25   88894                              Zoe Saldana is a Leo.      35.029692                               Saldana, Zoe                                  zoe, saldana, leo              NaN  NOT ENOUGH INFO                                                 []                                                 []  NOT ENOUGH INFO  NOT ENOUGH INFO                       No evidence provided to LLM.  {\"mod_1_total_documents\": 4, \"total_document_t...  {\"claim\": \"Zoe Saldana is a Leo.\", \"final_evid...                0.5                   0.9                 0.2              0.1                       0.2           0.5        0.766667      0.475    0.35  0.40303\n",
      "26   17915         Fred Seibert has produced comedy programs.      32.943932                              Seibert, Fred          seibert, programs, produced, fred, comedy              NaN               OK   [[\"Fred_Seibert_cartoon_shorts_filmography\", 0]]  [\"These are the complete filmographies for the...  NOT ENOUGH INFO  NOT ENOUGH INFO  Based ONLY on the following evidence sentences...  {\"mod_1_total_documents\": 5, \"total_document_t...  {\"claim\": \"Fred Seibert has produced comedy pr...                0.5                   0.9                 0.2              0.1                       0.2           0.5        0.766667      0.475    0.35  0.40303\n",
      "27   58396       Konidela Production Company was established.      28.956254               Production Company, Konidela         production, konidela, established, company              NaN               OK               [[\"Konidela_Production_Company\", 0]]  [\"Konidela Production Company is an Indian fil...         SUPPORTS               OK  Based ONLY on the following evidence sentences...  {\"mod_1_total_documents\": 4, \"total_document_t...  {\"claim\": \"Konidela Production Company was est...                0.5                   0.9                 0.2              0.1                       0.2           0.5        0.766667      0.475    0.35  0.40303\n",
      "28  150751                     Paul von Hindenburg was a man.      35.661462                                       Paul                         von, paul, man, hindenburg              NaN               OK  [[\"Paul_von_Hindenburg\", 0], [\"Paul_von_Hinden...  [\"He saw combat during the Austro-Prussian and...         SUPPORTS               OK  Based ONLY on the following evidence sentences...  {\"mod_1_total_documents\": 4, \"total_document_t...  {\"claim\": \"Paul von Hindenburg was a man.\", \"f...                0.5                   0.9                 0.2              0.1                       0.2           0.5        0.766667      0.475    0.35  0.40303\n",
      "29  179831                  Vic Mensa was born June 12, 1993.      27.344809                                 Mensa, Vic                       vic, mensa, june, born, 1993              NaN               OK                                 [[\"Vic_Mensa\", 0]]  [\"Victor Kwesi Mensah (born June 6, 1993), kno...          REFUTES               OK  Based ONLY on the following evidence sentences...  {\"mod_1_total_documents\": 4, \"total_document_t...  {\"claim\": \"Vic Mensa was born June 12, 1993.\",...                0.5                   0.9                 0.2              0.1                       0.2           0.5        0.766667      0.475    0.35  0.40303\n",
      "\n",
      "[30 rows x 24 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Report saved to: /content/drive/MyDrive/SUNY_Poly_DSA598/datasets/FEVER/paper_test_results/tuned_GPT-sBERTn1024-sentEx-RephsHist-T4_run_report_test_n30_250504_2348.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Execution & Scoring ---\n",
    "# Get the time now (UTC + 8 hours)\n",
    "time_str = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=8))).strftime(\"%y%m%d_%H%M\")\n",
    "\n",
    "# Ensure test_data is loaded\n",
    "if test_data:\n",
    "    # Run the system on a subset of the test data\n",
    "    NUM_TEST_CLAIMS = 30\n",
    "    initial_sbert_thresh=0.4\n",
    "    min_sbert_thresh=0.1\n",
    "    thresh_decay=0.05\n",
    "    max_evidence=8\n",
    "    max_iterations=10\n",
    "    near_match_thresh=0.8\n",
    "    max_pages_to_fetch=25\n",
    "    num_search_results=20\n",
    "    query_client_temp=0.5\n",
    "    rephrase_client_temp=0.9 # RESET THIS HIGHER IF CONTINUING TESTING WITH CLAIM REPHRASING. OTHERWISE, THIS IS A WARNING PLACEHOLDER FOR VISUALIZATIONS LATER.\n",
    "    sentEx_client_temp=0.2\n",
    "    nli_client_temp=0.1\n",
    "    disambiguate_client_temp=0.2\n",
    "    predictions, report_df = module_0_sys_control(test_data, NUM_TEST_CLAIMS, initial_sbert_thresh, min_sbert_thresh, thresh_decay, max_evidence, max_iterations, near_match_thresh, max_pages_to_fetch, num_search_results, query_client_temp, rephrase_client_temp, sentEx_client_temp, nli_client_temp, disambiguate_client_temp, verbose=1, debug=True)\n",
    "\n",
    "    # --- FEVER Scoring ---\n",
    "    # NOTE: This will run the scorer, but scores are only meaningful if 'predictions'\n",
    "    # includes the GOLD 'label' and 'evidence' fields.\n",
    "    # The 'strict_score' might be somewhat informative if NEI predictions align.\n",
    "    print(\"\\n--- FEVER Scoring Results ---\")\n",
    "    if predictions:\n",
    "        # The scorer expects 'evidence' to be a list of lists of possible evidence sets.\n",
    "        for p in predictions:\n",
    "            if \"predicted_evidence\" not in p or not isinstance(p[\"predicted_evidence\"], list):\n",
    "                 # If gold evidence wasn't loaded or is malformed, provide the expected structure.\n",
    "                 # A list containing one element: a list of gold evidence items [ [ [None, None, title, id], ... ], ...]\n",
    "                 # Or if the label is NEI, it expects evidence: []\n",
    "                 if p[\"predicted_label\"] == \"NOT ENOUGH INFO\":\n",
    "                      p[\"predicted_evidence\"] = []\n",
    "                 else:\n",
    "                     # For SUPPORTS/REFUTES where we lack gold data, technically the scorer\n",
    "                     # expects at least one evidence set. Providing an empty set list\n",
    "                     # signals no provable evidence was found/available in gold data.\n",
    "                     p[\"predicted_evidence\"] = [[]] # Represents verifiable but no specific gold sentences provided\n",
    "            if p[\"predicted_label\"] == \"NOT ENOUGH INFO\":\n",
    "                p[\"predicted_evidence\"] = []\n",
    "\n",
    "        # Ensure predicted_evidence is always a list (even if empty)\n",
    "        for p in predictions:\n",
    "            if \"predicted_evidence\" not in p:\n",
    "                p[\"predicted_evidence\"] = []\n",
    "\n",
    "        try:\n",
    "            strict_score, label_accuracy, precision, recall, f1 = fever_score(predictions)\n",
    "            print(f\"Strict Score (Exact Match): {strict_score*100:.2f}%\")\n",
    "            print(f\"Label Accuracy: {label_accuracy*100:.2f}%\")\n",
    "            print(f\"Evidence Precision: {precision*100:.2f}%\")\n",
    "            print(f\"Evidence Recall: {recall*100:.2f}%\")\n",
    "            print(f\"Evidence F1 Score: {f1*100:.2f}%\")\n",
    "            print(f\"Number of test cases scored: {len(predictions)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during FEVER scoring: {e}\")\n",
    "            print(\"Scoring skipped. Check prediction format and scorer compatibility.\")\n",
    "\n",
    "    else:\n",
    "        print(\"No predictions generated to score.\")\n",
    "\n",
    "    # --- Display Results ---\n",
    "    print(\"\\n--- Sample Predictions (Output Format) ---\")\n",
    "    for i, item in enumerate(predictions[:20]): # Show first 20 predictions\n",
    "        print(json.dumps(item, indent=2))\n",
    "        if i >= 20: break # Limit output\n",
    "\n",
    "    print(\"\\n--- Run Report ---\")\n",
    "    # Configure pandas display options\n",
    "    pd.set_option('display.max_rows', 10)\n",
    "    pd.set_option('display.max_columns', 25)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    pd.set_option('display.max_colwidth', 50) # Limit column width\n",
    "\n",
    "    if not report_df.empty:\n",
    "        # Add the strict_score, label_accuracy, precision, recall, f1\n",
    "        # Create new columns\n",
    "        report_df['query_client_temp'] = [query_client_temp] * len(report_df)\n",
    "        report_df['rephrase_client_temp'] = [rephrase_client_temp] * len(report_df)\n",
    "        report_df['sentEx_client_temp'] = [sentEx_client_temp] * len(report_df)\n",
    "        report_df['nli_client_temp'] = [nli_client_temp] * len(report_df)\n",
    "        report_df['disambiguate_client_temp'] = [disambiguate_client_temp] * len(report_df)\n",
    "        report_df['strict_score'] = [strict_score] * len(report_df)\n",
    "        report_df['label_accuracy'] = [label_accuracy] * len(report_df)\n",
    "        report_df['precision'] = [precision] * len(report_df)\n",
    "        report_df['recall'] = [recall] * len(report_df)\n",
    "        report_df['f1'] = [f1] * len(report_df)\n",
    "        print(report_df)\n",
    "        # Save the report\n",
    "        try:\n",
    "             report_filename = f'/content/drive/MyDrive/SUNY_Poly_DSA598/datasets/FEVER/paper_test_results/tuned_GPT-sBERTn1024-sentEx-RephsHist-T4_run_report_test_n{len(predictions)}_{time_str}.csv'\n",
    "             report_df.to_csv(report_filename, index=False)\n",
    "             print(f\"\\nReport saved to: {report_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving report: {e}\")\n",
    "    else:\n",
    "        print(\"Report DataFrame is empty.\")\n",
    "\n",
    "else:\n",
    "    print(\"Test data not loaded. Cannot run system.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1746375054968,
     "user": {
      "displayName": "Henry Zelenak",
      "userId": "01809909909045225068"
     },
     "user_tz": 240
    },
    "id": "iq1SvLGi9NEb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "iYkfA_GJcldU",
    "tOMbJ1uncldW",
    "zElKAY14cldY"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
