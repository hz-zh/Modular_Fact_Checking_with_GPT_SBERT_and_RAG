{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Testing of System Configurations and Claim Rephrasing Techniques\n",
    "\n",
    "**Henry Zelenak | Last updated: 05/12/2025**\n",
    "\n",
    "This notebook contains the statistical testing of the system configurations and claim rephrasing techniques. The results are based on the evaluation of the system configurations and claim rephrasing techniques on the paper_test.jsonl dataset (Thorne et al., 2018, April). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7555,
     "status": "ok",
     "timestamp": 1747020688809,
     "user": {
      "displayName": "Henry Zelenak",
      "userId": "01809909909045225068"
     },
     "user_tz": 240
    },
    "id": "zoZad_GVCAl8",
    "outputId": "3ed02b3c-ceb3-4f03-f6f2-ca6968c2c871"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install kaleido --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21496,
     "status": "ok",
     "timestamp": 1747020710343,
     "user": {
      "displayName": "Henry Zelenak",
      "userId": "01809909909045225068"
     },
     "user_tz": 240
    },
    "id": "S0lW9uCGBfNc",
    "outputId": "ede68d13-debd-43a2-e702-de791082c1c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Merged script for loading, processing, and analyzing FEVER test results.\n",
    "\n",
    "Handles both single-repetition tests and multi-repetition tests (averaging results).\n",
    "Performs statistical comparisons and generates visualizations.\n",
    "\"\"\"\n",
    "\n",
    "# --- Core Libraries ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import ast\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "import warnings # To potentially suppress warnings if needed\n",
    "\n",
    "# --- Plotting Libraries ---\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import matplotlib.pyplot as plt # statsmodels qqplot uses matplotlib\n",
    "import kaleido # Required for static image export with Plotly (often implicit)\n",
    "\n",
    "# --- Statistical Models & Plots ---\n",
    "from statsmodels.graphics.gofplots import qqplot # More direct qqplot generation\n",
    "\n",
    "# --- Google Colab Integration (Optional, keep if running in Colab) ---\n",
    "\n",
    "from google.colab import drive, userdata\n",
    "drive.mount('/content/drive')\n",
    "# Define BASE_DIR using Colab path\n",
    "BASE_DIR = '/content/drive/My Drive/SUNY_Poly_DSA598/'\n",
    "\n",
    "\n",
    "# --- Constants: Directory Paths ---\n",
    "# Ensure BASE_DIR is defined before using it\n",
    "FT_BASE_DIR = os.path.join(BASE_DIR, 'datasets/FEVER/paper_test_results/fine-tuning_baseline/')\n",
    "SINGLE_DIR = os.path.join(BASE_DIR, 'datasets/FEVER/paper_test_results/single_tuned/')\n",
    "REPHRS_DIR = os.path.join(BASE_DIR, 'datasets/FEVER/paper_test_results/claim_rephrasing/')\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, 'datasets/FEVER/paper_test_results/') # For saving combined results\n",
    "COMBINED_CSV_PATH = os.path.join(OUTPUT_DIR, 'all_10_results_merged.csv') # Specific path for the combined file\n",
    "# Define specific results directories\n",
    "SAVE_DIR = os.path.join(BASE_DIR, 'presentation/figures/')\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'datasets/FEVER/paper_test_results/')\n",
    "# Corrected paths: Remove leading '/' if joining with another path\n",
    "FT_RESULTS_SAVE_PATH = os.path.join(SAVE_DIR, 'fine-tuning') # (Group 1 Plots)\n",
    "CR_RESULTS_SAVE_PATH = os.path.join(SAVE_DIR, 'claim_rephrasing') # (Group 2 Plots)\n",
    "\n",
    "# Create save directories if they don't exist\n",
    "os.makedirs(FT_RESULTS_SAVE_PATH, exist_ok=True)\n",
    "os.makedirs(CR_RESULTS_SAVE_PATH, exist_ok=True)\n",
    "\n",
    "# --- Configuration & Paths ---\n",
    "pio.templates.default = \"plotly_white\" # Set a clean default theme for plots\n",
    "# Plot saving configuration\n",
    "SAVE_SCALE = 4 # Increase scale for higher resolution PNGs\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- Load Data ---\n",
    "all_results_df = pd.read_csv(os.path.join(DATA_DIR, 'all_10_results.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24963,
     "status": "ok",
     "timestamp": 1746988218217,
     "user": {
      "displayName": "Henry Zelenak",
      "userId": "01809909909045225068"
     },
     "user_tz": 240
    },
    "id": "ViaufnBKrr6R",
    "outputId": "e85ad0c8-d2e6-4ae7-99af-456a23ab8e90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'fever-scorer'...\n",
      "remote: Enumerating objects: 224, done.\u001b[K\n",
      "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
      "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
      "remote: Total 224 (delta 0), reused 0 (delta 0), pack-reused 219 (from 1)\u001b[K\n",
      "Receiving objects: 100% (224/224), 1.13 MiB | 5.69 MiB/s, done.\n",
      "Resolving deltas: 100% (110/110), done.\n",
      "/content/fever-scorer\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (1.17.0)\n",
      "setup.py updated\n",
      "Processing /content/fever-scorer\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from fever-scorer==0.0.0) (1.17.0)\n",
      "Building wheels for collected packages: fever-scorer\n",
      "  Building wheel for fever-scorer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for fever-scorer: filename=fever_scorer-0.0.0-py3-none-any.whl size=8288 sha256=4bfb53b15ea6bc01065f36e6ebf8a3b07763fca240671f0eaf911cbbd132b9e6\n",
      "  Stored in directory: /root/.cache/pip/wheels/f7/a5/f9/dffaef703ff054c8aa2ea4534130aae0e1ff9450753d0d7556\n",
      "Successfully built fever-scorer\n",
      "Installing collected packages: fever-scorer\n",
      "Successfully installed fever-scorer-0.0.0\n",
      "/content\n"
     ]
    }
   ],
   "source": [
    "# Ensure fever-scorer is installed correctly (assuming previous steps worked)\n",
    "!git clone -b release-v2.0 https://github.com/sheffieldnlp/fever-scorer.git\n",
    "%cd fever-scorer\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "# Open /setup.py and add 'license=\"MIT\"' on line 12, then overwrite the file\n",
    "import os\n",
    "with open('setup.py', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    lines[11] = 'license=\"MIT\"\\n'\n",
    "with open('setup.py', 'w') as f:\n",
    "    f.writelines(lines)\n",
    "    f.close()\n",
    "    print(\"setup.py updated\")\n",
    "!pip install .\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 74,
     "status": "ok",
     "timestamp": 1746988218294,
     "user": {
      "displayName": "Henry Zelenak",
      "userId": "01809909909045225068"
     },
     "user_tz": 240
    },
    "id": "RKfiXqblZuKa"
   },
   "outputs": [],
   "source": [
    "from fever.scorer import fever_score # Import the FEVER scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1180,
     "status": "ok",
     "timestamp": 1746953682496,
     "user": {
      "displayName": "Henry Zelenak",
      "userId": "01809909909045225068"
     },
     "user_tz": 240
    },
    "id": "lF6PoUfVBzlA",
    "outputId": "ff540f5c-127f-49a8-f5f7-5d1783c52bf3"
   },
   "outputs": [],
   "source": [
    "# Load the gold standard data for per-claim FEVER score, label F1 and label accuracy\n",
    "FEVER_GOLD_STANDARD_PATH = os.path.join(BASE_DIR, 'datasets/FEVER/paper_test.jsonl')\n",
    "with open(FEVER_GOLD_STANDARD_PATH, 'r') as f:\n",
    "    gold_data = [json.loads(line) for line in f]\n",
    "gold_df = pd.DataFrame(gold_data)\n",
    "# Take the first 30 rows\n",
    "gold_df = gold_df.iloc[:30]\n",
    "print(gold_df.head())\n",
    "\n",
    "# Load the CSV files from the fine-tuning baseline directory\n",
    "\n",
    "def load_dfs(dir):\n",
    "    \"\"\"\n",
    "    Load all CSV files from the DATA_DIR into a list of DataFrames.\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for file in os.listdir(dir):\n",
    "        if file.endswith('.csv'):\n",
    "            df = pd.read_csv(os.path.join(dir, file))\n",
    "            # Add a column for the system state when tested (the same value in all rows)\n",
    "            df['system_config'] = file.split('_')[0] + '_' + file.split('_')[1]\n",
    "            dfs.append(df)\n",
    "    return dfs\n",
    "\n",
    "### Single-repetition tests (e.g., all_base-Tx, tuned_GPT-sentEx, tuned_GPT-clf, tuned_GPT-query, tuned_sBERT-n1024)\n",
    "def process_dfs(dfs):\n",
    "    \"\"\"\n",
    "    Process the loaded DataFrames to extract relevant information.\n",
    "    \"\"\"\n",
    "    print(f\"Processing {len(dfs)} DataFrames for single-repetition tests\")\n",
    "    processed_dfs = []\n",
    "    for df in dfs:\n",
    "        # Unpack the module 1 report into new columns\n",
    "        module_1_report = df['module1_report_details'].apply(json.loads)\n",
    "        df['number_of_pages_retrieved'] = module_1_report.apply(lambda x: x['mod_1_total_documents'])\n",
    "        df['total_document_tokens'] = module_1_report.apply(lambda x: x['total_document_tokens'])\n",
    "        df['potential_titles'] = module_1_report.apply(lambda x: x['potential_titles'])\n",
    "        df['retrieved_pages'] = module_1_report.apply(lambda x: x['retrieved_titles'])\n",
    "\n",
    "        # Unpack the module 2 report into new columns\n",
    "        module_2_report = df['module2_report_details'].apply(json.loads)\n",
    "        for key in module_2_report.iloc[0]:\n",
    "            df[key] = module_2_report.apply(lambda x: x[key])\n",
    "\n",
    "        # Rename 'llm_total_tokens' as GPT_total_tokens\n",
    "        df.rename(columns={'llm_total_tokens': 'gpt_total_tokens'}, inplace=True)\n",
    "        # Rename 'llm_total_sentences' as GPT_total_sentences\n",
    "        df.rename(columns={'llm_total_sentences': 'gpt_total_sentences'}, inplace=True)\n",
    "\n",
    "        # Create a 'number_of_evidence_sentences' column from the length of final_evidence_ids\n",
    "        df['number_of_evidence_sentences'] = df['final_evidence_ids'].apply(len)\n",
    "        '''\n",
    "                instances = [\n",
    "          {\n",
    "              \"label\": \"REFUTES\",\n",
    "              \"predicted_label\": \"REFUTES\",\n",
    "              \"predicted_evidence\": [[\"Page1\", 1], [\"Page3\", 2]],\n",
    "              \"evidence\": [\n",
    "                  [\n",
    "                      [None, None, \"Page1\", 1],\n",
    "                      [None, None, \"Page3\", 1],\n",
    "                      [None, None, \"Page3\", 2],\n",
    "                  ],\n",
    "              ],\n",
    "          },\n",
    "          {\n",
    "              \"label\": \"SUPPORTS\",\n",
    "              \"predicted_label\": \"SUPPORTS\",\n",
    "              \"predicted_evidence\": [[\"Page3\", 3]],\n",
    "              \"evidence\": [\n",
    "                  [\n",
    "                      [None, None, \"Page3\", 3]\n",
    "                  ]\n",
    "              ],\n",
    "          },\n",
    "          {\n",
    "              \"label\": \"NOT ENOUGH INFO\",\n",
    "              \"predicted_label\": \"NOT ENOUGH INFO\",\n",
    "              \"predicted_evidence\": [],\n",
    "              \"evidence\": [],\n",
    "          },\n",
    "        ]\n",
    "        '''\n",
    "        # Unpack the 'predicted_evidence_ids' column into a list of dictionaries (predicted_evidence_ids is a list of lists, where each inner list is a list of [page, sentence])\n",
    "        for row in df.iterrows():\n",
    "            # Get the corresponding row in the gold_df using the id\n",
    "            id = row[1]['id']\n",
    "            gold_row = gold_df[gold_df['id'] == id]\n",
    "            if not gold_row.empty:\n",
    "                # rename strict_score,label_accuracy,precision,recall,f1 to have \"avg_\"\n",
    "                for key in ['strict_score', 'label_accuracy', 'precision', 'recall', 'f1']:\n",
    "                    df.rename(columns={key: 'avg_' + key}, inplace=True)\n",
    "\n",
    "                # Get the label and predicted label\n",
    "                label = gold_row['label'].values[0]\n",
    "                predicted_label = row[1]['module3_result']\n",
    "                # Get the predicted evidence ids\n",
    "                   # Get the evidence ids\n",
    "                if predicted_label == 'NOT ENOUGH INFO':\n",
    "                    predicted_evidence_ids = []\n",
    "                else:\n",
    "                  predicted_evidence_ids = row[1]['predicted_evidence_ids']\n",
    "                  # Use ast to convert the string to a list\n",
    "                  predicted_evidence_ids = ast.literal_eval(predicted_evidence_ids)\n",
    "\n",
    "                # Get the evidence\n",
    "                evidence = gold_row['evidence'].values[0]\n",
    "\n",
    "                # Create a dictionary for the instance\n",
    "                instance = {\n",
    "                    'label': label,\n",
    "                    'predicted_label': predicted_label,\n",
    "                    'predicted_evidence': predicted_evidence_ids,\n",
    "                    'evidence': evidence\n",
    "                }\n",
    "\n",
    "                strict_score, label_accuracy, precision, recall, f1 = fever_score([instance])\n",
    "                print(f\"Strict score: {strict_score}, Label accuracy: {label_accuracy}, Precision: {precision}, Recall: {recall}, F1: {f1}\")\n",
    "\n",
    "                # Add new columns for each per-claim score\n",
    "                df.loc[row[0], 'pc_strict_score'] = strict_score\n",
    "                df.loc[row[0], 'pc_label_accuracy'] = label_accuracy\n",
    "                df.loc[row[0], 'pc_precision'] = precision\n",
    "                df.loc[row[0], 'pc_recall'] = recall\n",
    "                df.loc[row[0], 'pc_f1'] = f1\n",
    "\n",
    "        processed_dfs.append(df)\n",
    "        # Concatenate the DataFrames\n",
    "    df = pd.concat(processed_dfs, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "single_rep_dfs = load_dfs(SINGLE_DIR)\n",
    "single_rep_df = process_dfs(single_rep_dfs)\n",
    "\n",
    "### Quadruple-repetition tests\n",
    "\n",
    "### Use the same load_dfs function as above\n",
    "def process_reps_dfs(dfs):\n",
    "    \"\"\"\n",
    "    Process the loaded DataFrames to extract relevant information.\n",
    "    \"\"\"\n",
    "    print(f\"Processing {len(dfs)} DataFrames for quadruple-repetition tests\")\n",
    "    to_average_dfs = []\n",
    "    averaged_df = pd.DataFrame()\n",
    "    unique_configs = []\n",
    "\n",
    "    for df in dfs:\n",
    "        # Unpack the module 1 report into new columns\n",
    "        module_1_report = df['module1_report_details'].apply(json.loads)\n",
    "        df['number_of_pages_retrieved'] = module_1_report.apply(lambda x: x['mod_1_total_documents'])\n",
    "        df['total_document_tokens'] = module_1_report.apply(lambda x: x['total_document_tokens'])\n",
    "        df['potential_titles'] = module_1_report.apply(lambda x: x['potential_titles'])\n",
    "        df['retrieved_pages'] = module_1_report.apply(lambda x: x['retrieved_titles'])\n",
    "\n",
    "        # Unpack the module 2 report into new columns\n",
    "        module_2_report = df['module2_report_details'].apply(json.loads)\n",
    "        for key in module_2_report.iloc[0]:\n",
    "            df[key] = module_2_report.apply(lambda x: x[key])\n",
    "\n",
    "        # Rename 'llm_total_tokens' as GPT_total_tokens\n",
    "        df.rename(columns={'llm_total_tokens': 'gpt_total_tokens'}, inplace=True)\n",
    "        # Rename 'llm_total_sentences' as GPT_total_sentences\n",
    "        df.rename(columns={'llm_total_sentences': 'gpt_total_sentences'}, inplace=True)\n",
    "\n",
    "        # Create a 'number_of_evidence_sentences' column from the length of final_evidence_ids\n",
    "        df['number_of_evidence_sentences'] = df['final_evidence_ids'].apply(len)\n",
    "\n",
    "\n",
    "        # Split on '-' and take everything except the last item\n",
    "        trimmed_sys_config = df['system_config'].apply(lambda x: x.split('-')[:-1]).apply('-'.join)\n",
    "        df['system_config'] = trimmed_sys_config\n",
    "        unique_configs.extend(trimmed_sys_config)\n",
    "        unique_configs = list(set(unique_configs))\n",
    "\n",
    "        to_average_dfs.append(df)\n",
    "\n",
    "    for config in unique_configs:\n",
    "        # Get the DataFrames for this config\n",
    "        print(f\"Processing config: {config}\")\n",
    "        config_dfs = [df for df in to_average_dfs if df['system_config'].iloc[0] == config]\n",
    "        print(f\"Number of DataFrames for config {config}: {len(config_dfs)}\")\n",
    "        # Concatenate them into a single DataFrame\n",
    "        config_df = pd.concat(config_dfs)\n",
    "        # Drop non-numeric columns\n",
    "        config_df_num = config_df.select_dtypes(include=[np.number])\n",
    "\n",
    "        # for each row that shares an id in config_df_num, average the other columns and create a new row for that id and add it to averaged_df\n",
    "        averaged_rows = []\n",
    "        for id in config_df_num['id'].unique():\n",
    "            # Get the rows for this id\n",
    "            id_rows = config_df_num[config_df_num['id'] == id]\n",
    "\n",
    "            # rename strict_score,label_accuracy,precision,recall,f1 to have \"avg_\"\n",
    "            for key in ['strict_score', 'label_accuracy', 'precision', 'recall', 'f1']:\n",
    "                config_df_num.rename(columns={key: 'avg_' + key}, inplace=True)\n",
    "\n",
    "            # Average the numeric columns\n",
    "            averaged_row = id_rows.mean()\n",
    "\n",
    "            # Add the id to the averaged row\n",
    "            averaged_row['id'] = id\n",
    "            # Add the system config to the averaged row\n",
    "            averaged_row['system_config'] = config\n",
    "            # Add the module3_result back to the averaged row\n",
    "            averaged_row['module3_result'] = config_df[config_df['id'] == id]['module3_result'].iloc[0]\n",
    "            # Add the \"predicted_evidence_ids\" back\n",
    "            averaged_row['predicted_evidence_ids'] = config_df[config_df['id'] == id]['predicted_evidence_ids'].iloc[0]\n",
    "\n",
    "            # Unpack the 'predicted_evidence_ids' column into a list of dictionaries (predicted_evidence_ids is a list of lists, where each inner list is a list of [page, sentence])\n",
    "            # Get the corresponding row in the gold_df using the id\n",
    "            gold_row = gold_df[gold_df['id'] == id]\n",
    "            if not gold_row.empty:\n",
    "                # Get the label and predicted label\n",
    "                label = gold_row['label'].values[0]\n",
    "                predicted_label = averaged_row['module3_result']\n",
    "                # Get the predicted evidence ids\n",
    "                if predicted_label == 'NOT ENOUGH INFO':\n",
    "                    predicted_evidence_ids = []\n",
    "                else:\n",
    "                    predicted_evidence_ids = averaged_row['predicted_evidence_ids']\n",
    "                    # Use ast to convert the string to a list\n",
    "                    predicted_evidence_ids = ast.literal_eval(predicted_evidence_ids)\n",
    "\n",
    "                # Get the evidence\n",
    "                evidence = gold_row['evidence'].values[0]\n",
    "\n",
    "                # Create a dictionary for the instance\n",
    "                instance = {\n",
    "                    'label': label,\n",
    "                    'predicted_label': predicted_label,\n",
    "                    'predicted_evidence': predicted_evidence_ids,\n",
    "                    'evidence': evidence\n",
    "                }\n",
    "\n",
    "                strict_score, label_accuracy, precision, recall, f1 = fever_score([instance])\n",
    "\n",
    "                # Add new columns for each per-claim score\n",
    "                averaged_row['pc_strict_score'] = strict_score\n",
    "                averaged_row['pc_label_accuracy'] = label_accuracy\n",
    "                averaged_row['pc_precision'] = precision\n",
    "                averaged_row['pc_recall'] = recall\n",
    "                averaged_row['pc_f1'] = f1\n",
    "\n",
    "            # Append the averaged row to the list\n",
    "            averaged_rows.append(averaged_row)\n",
    "        # Create a DataFrame from the averaged rows\n",
    "        averaged_df = pd.concat([averaged_df, pd.DataFrame(averaged_rows)], ignore_index=True)\n",
    "\n",
    "    return averaged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35ylmii2itb3"
   },
   "outputs": [],
   "source": [
    "reps_ft_base_dfs = load_dfs(FT_BASE_DIR)\n",
    "reps_ft_base_df = process_reps_dfs(reps_ft_base_dfs)\n",
    "reps_dfs = load_dfs(REPHRS_DIR)\n",
    "reps_df = process_reps_dfs(reps_dfs)\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "reps_df = pd.concat([reps_ft_base_df, reps_df], ignore_index=True)\n",
    "\n",
    "# Now we have two DataFrames: single_rep_df and reps_df, the first with the single-repetition tests (GPT-sentEx, tuned_GPT-clf, tuned_GPT-query, tuned_sBERT-n1024) and the second with the quadruple-repetition tests (all_base, tuned_GPT-sBERTn1024-sentEx-naiveRephrs, tuned_GPT-sBERTn1024-sentEx-rephsHist, tuned_GPT-sBERTn1024-sentEx+GPT-query, tuned_GPT-sBERTn1024-sentEx+PT).\n",
    "\n",
    "# We concatenate them into a single DataFrame\n",
    "all_results_df = pd.concat([single_rep_df, reps_df], ignore_index=True)\n",
    "\n",
    "# We can now select the relevant columns for our analysis\n",
    "         # - number_of_pages_retrieved\n",
    "         # - gpt_total_tokens\n",
    "         # - number_of_evidence_sentences\n",
    "         # - iterations_run\n",
    "         # - time_to_check\n",
    "         # - label_accuracy\n",
    "         # - strict_score\n",
    "         # - f1\n",
    "\n",
    "relevant_columns = [\n",
    "    'system_config',\n",
    "    'number_of_pages_retrieved',\n",
    "    'gpt_total_tokens',\n",
    "    'number_of_evidence_sentences',\n",
    "    'iterations_run',\n",
    "    'time_to_check',\n",
    "    'avg_label_accuracy',\n",
    "    'avg_strict_score',\n",
    "    'avg_f1',\n",
    "    'pc_strict_score',\n",
    "    'pc_label_accuracy',\n",
    "    'pc_precision',\n",
    "    'pc_recall',\n",
    "    'pc_f1'\n",
    "]\n",
    "\"\"\"\n",
    "We end up with 10 system configurations:\n",
    "- all_base (fine-tuning baseline, an average of 4 repetitions)\n",
    "- tuned_GPT-sentEx\n",
    "- tuned_GPT-clf\n",
    "- tuned_GPT-query\n",
    "- tuned_sBERT-n1024\n",
    "- tuned_GPT-sBERTn1024-sentEx-noRephrs (fine-tuning baseline for claim rephrasing, an average of 4 repetitions)\n",
    "- tuned_GPT-sBERTn1024-sentEx-naiveRephrs (naive rephrasing, an average of 4 repetitions)\n",
    "- tuned_GPT-sBERTn1024-sentEx-rephsHist (rephrase history, an average of 4 repetitions)\n",
    "- tuned_GPT-sBERTn1024-sentEx+GPT-query\n",
    "- tuned_GPT-sBERTn1024-sentEx+PT\n",
    "\"\"\"\n",
    "# Select only the relevant columns\n",
    "all_results_df = all_results_df[relevant_columns]\n",
    "\n",
    "# We can now save the DataFrame to a CSV file\n",
    "all_results_df.to_csv(os.path.join(BASE_DIR, 'datasets/FEVER/paper_test_results/all_10_results.csv'), index=False)\n",
    "\n",
    "# we now want to test the 4 single-model configurations (tuned_GPT-sentEx, tuned_GPT-clf, tuned_GPT-query, tuned_sBERT-n1024) relative to the fine-tuning baseline (all_base) and the 2 claim rephrasing configurations (tuned_GPT-sBERTn1024-sentEx-naiveRephrs, tuned_GPT-sBERTn1024-sentEx-rephsHist) relative the claim rephrasing baseline (no rephrasing) (tuned_GPT-sBERTn1024-sentEx-noRephrs) using a t-test or ANOVA, depending on the number of groups we are comparing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1746991019583,
     "user": {
      "displayName": "Henry Zelenak",
      "userId": "01809909909045225068"
     },
     "user_tz": 240
    },
    "id": "IOQ6hKd7x2Hb",
    "outputId": "6845aae1-ae5e-4201-cb63-530b494df0ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 240 entries, 0 to 239\n",
      "Data columns (total 14 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   system_config                 240 non-null    object \n",
      " 1   number_of_pages_retrieved     240 non-null    float64\n",
      " 2   gpt_total_tokens              240 non-null    float64\n",
      " 3   number_of_evidence_sentences  240 non-null    float64\n",
      " 4   iterations_run                240 non-null    float64\n",
      " 5   time_to_check                 240 non-null    float64\n",
      " 6   avg_label_accuracy            236 non-null    float64\n",
      " 7   avg_strict_score              236 non-null    float64\n",
      " 8   avg_f1                        236 non-null    float64\n",
      " 9   pc_strict_score               240 non-null    float64\n",
      " 10  pc_label_accuracy             240 non-null    float64\n",
      " 11  pc_precision                  240 non-null    float64\n",
      " 12  pc_recall                     240 non-null    float64\n",
      " 13  pc_f1                         240 non-null    float64\n",
      "dtypes: float64(13), object(1)\n",
      "memory usage: 26.4+ KB\n"
     ]
    }
   ],
   "source": [
    "all_results_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Testing and Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19686,
     "status": "ok",
     "timestamp": 1746994050844,
     "user": {
      "displayName": "Henry Zelenak",
      "userId": "01809909909045225068"
     },
     "user_tz": 240
    },
    "id": "r_2X-2Y7lsKe",
    "outputId": "12983689-1b1f-4ad9-c7c5-24beb1082eb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Color Mapping: {\n",
      "  \"all_base\": \"rgb(136, 136, 136)\",\n",
      "  \"tuned_GPT-clf\": \"rgb(102, 17, 0)\",\n",
      "  \"tuned_GPT-query\": \"rgb(136, 34, 85)\",\n",
      "  \"tuned_GPT-sBERTn1024-sentEx-naiveRephrs\": \"rgb(153, 153, 51)\",\n",
      "  \"tuned_GPT-sBERTn1024-sentEx-noRephrs\": \"rgb(68, 170, 153)\",\n",
      "  \"tuned_GPT-sBERTn1024-sentEx-rephsHist\": \"rgb(170, 68, 153)\",\n",
      "  \"tuned_GPT-sentEx\": \"rgb(51, 34, 136)\",\n",
      "  \"tuned_sBERT-n1024\": \"rgb(17, 119, 51)\"\n",
      "}\n",
      "\n",
      "--- Group 1 Visualizations (vs Baseline) ---\n",
      "NOTE: Comparing single-rep results to a quad-rep baseline's per-claim scores.\n",
      "Group 1: Overall Average Performance Scores:\n",
      "        system_config              Metric  Average Score (Overall Run)\n",
      "0            all_base  avg_label_accuracy                     0.633333\n",
      "1       tuned_GPT-clf  avg_label_accuracy                     0.566667\n",
      "2     tuned_GPT-query  avg_label_accuracy                     0.600000\n",
      "3    tuned_GPT-sentEx  avg_label_accuracy                     0.733333\n",
      "4   tuned_sBERT-n1024  avg_label_accuracy                     0.733333\n",
      "5            all_base    avg_strict_score                     0.408333\n",
      "6       tuned_GPT-clf    avg_strict_score                     0.366667\n",
      "7     tuned_GPT-query    avg_strict_score                     0.433333\n",
      "8    tuned_GPT-sentEx    avg_strict_score                     0.433333\n",
      "9   tuned_sBERT-n1024    avg_strict_score                     0.466667\n",
      "10           all_base              avg_f1                     0.414960\n",
      "11      tuned_GPT-clf              avg_f1                     0.377778\n",
      "12    tuned_GPT-query              avg_f1                     0.359551\n",
      "13   tuned_GPT-sentEx              avg_f1                     0.320574\n",
      "14  tuned_sBERT-n1024              avg_f1                     0.421302\n",
      "\n",
      "Group 1: Description of pc_precision per system configuration:\n",
      "                   count      mean       std  min   25%  50%  75%  max\n",
      "system_config                                                         \n",
      "all_base            30.0  0.684444  0.430604  0.0  0.20  1.0  1.0  1.0\n",
      "tuned_GPT-clf       30.0  0.673333  0.444067  0.0  0.20  1.0  1.0  1.0\n",
      "tuned_GPT-query     30.0  0.760000  0.411557  0.0  0.55  1.0  1.0  1.0\n",
      "tuned_GPT-sentEx    30.0  0.631111  0.468382  0.0  0.00  1.0  1.0  1.0\n",
      "tuned_sBERT-n1024   30.0  0.630000  0.446558  0.0  0.20  1.0  1.0  1.0\n",
      "\n",
      "Group 1: Description of pc_recall per system configuration:\n",
      "                   count      mean       std  min  25%  50%   75%  max\n",
      "system_config                                                         \n",
      "all_base            30.0  0.266667  0.449776  0.0  0.0  0.0  0.75  1.0\n",
      "tuned_GPT-clf       30.0  0.200000  0.406838  0.0  0.0  0.0  0.00  1.0\n",
      "tuned_GPT-query     30.0  0.166667  0.379049  0.0  0.0  0.0  0.00  1.0\n",
      "tuned_GPT-sentEx    30.0  0.166667  0.379049  0.0  0.0  0.0  0.00  1.0\n",
      "tuned_sBERT-n1024   30.0  0.266667  0.449776  0.0  0.0  0.0  0.75  1.0\n",
      "\n",
      "Group 1: Description of pc_f1 per system configuration:\n",
      "                   count      mean       std  min  25%  50%   75%  max\n",
      "system_config                                                         \n",
      "all_base            30.0  0.169048  0.324439  0.0  0.0  0.0  0.25  1.0\n",
      "tuned_GPT-clf       30.0  0.119048  0.277594  0.0  0.0  0.0  0.00  1.0\n",
      "tuned_GPT-query     30.0  0.107937  0.275383  0.0  0.0  0.0  0.00  1.0\n",
      "tuned_GPT-sentEx    30.0  0.113492  0.281696  0.0  0.0  0.0  0.00  1.0\n",
      "tuned_sBERT-n1024   30.0  0.171429  0.327201  0.0  0.0  0.0  0.25  1.0\n",
      "\n",
      "Group 1: Description of number_of_pages_retrieved per system configuration:\n",
      "                   count      mean       std  min  25%   50%   75%  max\n",
      "system_config                                                          \n",
      "all_base            30.0  4.041667  0.873665  2.0  3.5  4.25  4.75  5.5\n",
      "tuned_GPT-clf       30.0  3.400000  1.037238  2.0  3.0  3.00  4.00  5.0\n",
      "tuned_GPT-query     30.0  1.000000  0.000000  1.0  1.0  1.00  1.00  1.0\n",
      "tuned_GPT-sentEx    30.0  3.400000  1.191927  1.0  3.0  3.00  4.00  5.0\n",
      "tuned_sBERT-n1024   30.0  4.266667  1.460593  2.0  3.0  4.00  5.00  8.0\n",
      "\n",
      "Group 1: Description of gpt_total_tokens per system configuration:\n",
      "                   count         mean          std    min        25%  \\\n",
      "system_config                                                          \n",
      "all_base            30.0  1036.291667   641.953773  212.0   547.1875   \n",
      "tuned_GPT-clf       30.0   919.500000   735.129440  129.0   435.5000   \n",
      "tuned_GPT-query     30.0   557.233333   386.849403   17.0   220.5000   \n",
      "tuned_GPT-sentEx    30.0   906.066667   631.558550   14.0   523.7500   \n",
      "tuned_sBERT-n1024   30.0  2363.166667  1685.699140  250.0  1340.5000   \n",
      "\n",
      "                        50%        75%      max  \n",
      "system_config                                    \n",
      "all_base            905.125  1442.3125  2871.25  \n",
      "tuned_GPT-clf       741.500  1207.0000  3277.00  \n",
      "tuned_GPT-query     576.000   763.5000  1588.00  \n",
      "tuned_GPT-sentEx    670.000  1295.2500  2645.00  \n",
      "tuned_sBERT-n1024  1921.500  2883.5000  6721.00  \n",
      "\n",
      "Group 1: Description of number_of_evidence_sentences per system configuration:\n",
      "                   count      mean       std  min     25%    50%    75%  max\n",
      "system_config                                                               \n",
      "all_base            30.0  2.708333  2.585673  0.0  0.8125  1.375  4.625  8.0\n",
      "tuned_GPT-clf       30.0  2.700000  3.108941  0.0  0.0000  1.000  6.000  8.0\n",
      "tuned_GPT-query     30.0  1.900000  2.832691  0.0  0.0000  0.000  3.000  8.0\n",
      "tuned_GPT-sentEx    30.0  1.600000  1.922642  0.0  0.0000  1.000  3.000  8.0\n",
      "tuned_sBERT-n1024   30.0  3.166667  3.141308  0.0  0.2500  2.000  6.000  8.0\n",
      "\n",
      "Group 1: Description of iterations_run per system configuration:\n",
      "                   count      mean       std  min  25%  50%   75%   max\n",
      "system_config                                                          \n",
      "all_base            30.0  3.183333  0.262065  3.0  3.0  3.0  3.25  3.75\n",
      "tuned_GPT-clf       30.0  3.200000  0.484234  3.0  3.0  3.0  3.00  5.00\n",
      "tuned_GPT-query     30.0  3.166667  0.791478  2.0  3.0  3.0  3.00  6.00\n",
      "tuned_GPT-sentEx    30.0  3.266667  0.691492  3.0  3.0  3.0  3.00  6.00\n",
      "tuned_sBERT-n1024   30.0  3.100000  0.402578  2.0  3.0  3.0  3.00  4.00\n",
      "\n",
      "Group 1: Description of time_to_check per system configuration:\n",
      "                   count       mean        std        min        25%  \\\n",
      "system_config                                                          \n",
      "all_base            30.0  40.113509   9.786958  25.150750  33.725157   \n",
      "tuned_GPT-clf       30.0  40.014227  15.972321  17.570386  27.096728   \n",
      "tuned_GPT-query     30.0  21.816739   7.796761  10.911057  14.842137   \n",
      "tuned_GPT-sentEx    30.0  40.745201  14.560821  11.909568  32.408338   \n",
      "tuned_sBERT-n1024   30.0  54.197906  20.008766  21.235106  36.767217   \n",
      "\n",
      "                         50%        75%        max  \n",
      "system_config                                       \n",
      "all_base           39.036862  44.819150  62.747203  \n",
      "tuned_GPT-clf      42.170428  48.517143  81.153345  \n",
      "tuned_GPT-query    20.846692  27.540416  38.056562  \n",
      "tuned_GPT-sentEx   40.425229  46.297637  70.919715  \n",
      "tuned_sBERT-n1024  54.923619  69.861993  91.719412  \n",
      "\n",
      "--- Group 2 Analysis: Claim Rephrasing (Quad-Rep) ---\n",
      "Group 2: Overall Average Performance Scores:\n",
      "                             system_config              Metric  \\\n",
      "0  tuned_GPT-sBERTn1024-sentEx-naiveRephrs  avg_label_accuracy   \n",
      "1     tuned_GPT-sBERTn1024-sentEx-noRephrs  avg_label_accuracy   \n",
      "2    tuned_GPT-sBERTn1024-sentEx-rephsHist  avg_label_accuracy   \n",
      "3  tuned_GPT-sBERTn1024-sentEx-naiveRephrs    avg_strict_score   \n",
      "4     tuned_GPT-sBERTn1024-sentEx-noRephrs    avg_strict_score   \n",
      "5    tuned_GPT-sBERTn1024-sentEx-rephsHist    avg_strict_score   \n",
      "6  tuned_GPT-sBERTn1024-sentEx-naiveRephrs              avg_f1   \n",
      "7     tuned_GPT-sBERTn1024-sentEx-noRephrs              avg_f1   \n",
      "8    tuned_GPT-sBERTn1024-sentEx-rephsHist              avg_f1   \n",
      "\n",
      "   Average Score (Overall Run)  \n",
      "0                     0.775000  \n",
      "1                     0.675000  \n",
      "2                     0.741667  \n",
      "3                     0.525000  \n",
      "4                     0.475000  \n",
      "5                     0.491667  \n",
      "6                     0.407314  \n",
      "7                     0.382134  \n",
      "8                     0.391446  \n",
      "\n",
      "--- Analyzing Per-Claim Metric: Pc Strict Score (Group 2) ---\n",
      "Statistical Test for Binary Metric: pc_strict_score\n",
      "  tuned_GPT-sBERTn1024-sentEx-naiveRephrs: Successes=16.0, Failures=14.0 (Total=30)\n",
      "  tuned_GPT-sBERTn1024-sentEx-noRephrs: Successes=14.0, Failures=16.0 (Total=30)\n",
      "  tuned_GPT-sBERTn1024-sentEx-rephsHist: Successes=15.0, Failures=15.0 (Total=30)\n",
      "  Chi-Squared Test: chi2 = 0.2667, p = 0.8752, df = 2\n",
      "  No significant difference in proportions found between groups (p >= 0.05).\n",
      "----------------------------------------\n",
      "\n",
      "--- Analyzing Per-Claim Metric: Pc Label Accuracy (Group 2) ---\n",
      "Statistical Test for Binary Metric: pc_label_accuracy\n",
      "  tuned_GPT-sBERTn1024-sentEx-naiveRephrs: Successes=24.0, Failures=6.0 (Total=30)\n",
      "  tuned_GPT-sBERTn1024-sentEx-noRephrs: Successes=20.0, Failures=10.0 (Total=30)\n",
      "  tuned_GPT-sBERTn1024-sentEx-rephsHist: Successes=24.0, Failures=6.0 (Total=30)\n",
      "  Chi-Squared Test: chi2 = 1.9251, p = 0.3819, df = 2\n",
      "  No significant difference in proportions found between groups (p >= 0.05).\n",
      "----------------------------------------\n",
      "\n",
      "--- Analyzing Per-Claim Metric: Pc Precision (Group 2) ---\n",
      "Normality Check (Q-Q Plots & Shapiro-Wilk) for pc_precision:\n",
      "  Shapiro-Wilk p-value for tuned_GPT-sBERTn1024-sentEx-naiveRephrs: 0.0000 -> Potential non-normality (p < 0.05)\n",
      "  Shapiro-Wilk p-value for tuned_GPT-sBERTn1024-sentEx-noRephrs: 0.0000 -> Potential non-normality (p < 0.05)\n",
      "  Shapiro-Wilk p-value for tuned_GPT-sBERTn1024-sentEx-rephsHist: 0.0000 -> Potential non-normality (p < 0.05)\n",
      "\n",
      "Variance Check (Levene's Test) for pc_precision:\n",
      "  Levene's test p-value: 0.7493 -> Equal variances plausible (p >= 0.05)\n",
      "\n",
      "Group Comparison for pc_precision:\n",
      "  Using Kruskal-Wallis (non-parametric test due to potential assumption violation).\n",
      "  Kruskal-Wallis Result: H = 0.6026, p = 0.7398\n",
      "  No significant difference found between groups (p >= 0.05).\n",
      "----------------------------------------\n",
      "\n",
      "--- Analyzing Per-Claim Metric: Pc Recall (Group 2) ---\n",
      "Normality Check (Q-Q Plots & Shapiro-Wilk) for pc_recall:\n",
      "  Shapiro-Wilk p-value for tuned_GPT-sBERTn1024-sentEx-naiveRephrs: 0.0000 -> Potential non-normality (p < 0.05)\n",
      "  Shapiro-Wilk p-value for tuned_GPT-sBERTn1024-sentEx-noRephrs: 0.0000 -> Potential non-normality (p < 0.05)\n",
      "  Shapiro-Wilk p-value for tuned_GPT-sBERTn1024-sentEx-rephsHist: 0.0000 -> Potential non-normality (p < 0.05)\n",
      "\n",
      "Variance Check (Levene's Test) for pc_recall:\n",
      "  Levene's test p-value: 0.7720 -> Equal variances plausible (p >= 0.05)\n",
      "\n",
      "Group Comparison for pc_recall:\n",
      "  Using Kruskal-Wallis (non-parametric test due to potential assumption violation).\n",
      "  Kruskal-Wallis Result: H = 0.5278, p = 0.7681\n",
      "  No significant difference found between groups (p >= 0.05).\n",
      "----------------------------------------\n",
      "\n",
      "--- Analyzing Per-Claim Metric: Pc F1 (Group 2) ---\n",
      "Normality Check (Q-Q Plots & Shapiro-Wilk) for pc_f1:\n",
      "  Shapiro-Wilk p-value for tuned_GPT-sBERTn1024-sentEx-naiveRephrs: 0.0000 -> Potential non-normality (p < 0.05)\n",
      "  Shapiro-Wilk p-value for tuned_GPT-sBERTn1024-sentEx-noRephrs: 0.0000 -> Potential non-normality (p < 0.05)\n",
      "  Shapiro-Wilk p-value for tuned_GPT-sBERTn1024-sentEx-rephsHist: 0.0000 -> Potential non-normality (p < 0.05)\n",
      "\n",
      "Variance Check (Levene's Test) for pc_f1:\n",
      "  Levene's test p-value: 0.7637 -> Equal variances plausible (p >= 0.05)\n",
      "\n",
      "Group Comparison for pc_f1:\n",
      "  Using Kruskal-Wallis (non-parametric test due to potential assumption violation).\n",
      "  Kruskal-Wallis Result: H = 0.5546, p = 0.7578\n",
      "  No significant difference found between groups (p >= 0.05).\n",
      "----------------------------------------\n",
      "\n",
      "--- Analyzing Per-Claim Metric: Number Of Pages Retrieved (Group 2) ---\n",
      "Normality Check (Q-Q Plots & Shapiro-Wilk) for number_of_pages_retrieved:\n",
      "  Shapiro-Wilk p-value for tuned_GPT-sBERTn1024-sentEx-naiveRephrs: 0.0726 -> Normality plausible (p >= 0.05)\n",
      "  Shapiro-Wilk p-value for tuned_GPT-sBERTn1024-sentEx-noRephrs: 0.0078 -> Potential non-normality (p < 0.05)\n",
      "  Shapiro-Wilk p-value for tuned_GPT-sBERTn1024-sentEx-rephsHist: 0.0746 -> Normality plausible (p >= 0.05)\n",
      "\n",
      "Variance Check (Levene's Test) for number_of_pages_retrieved:\n",
      "  Levene's test p-value: 0.9891 -> Equal variances plausible (p >= 0.05)\n",
      "\n",
      "Group Comparison for number_of_pages_retrieved:\n",
      "  Using Kruskal-Wallis (non-parametric test due to potential assumption violation).\n",
      "  Kruskal-Wallis Result: H = 1.2564, p = 0.5336\n",
      "  No significant difference found between groups (p >= 0.05).\n",
      "----------------------------------------\n",
      "\n",
      "--- Analyzing Per-Claim Metric: Gpt Total Tokens (Group 2) ---\n",
      "Normality Check (Q-Q Plots & Shapiro-Wilk) for gpt_total_tokens:\n",
      "  Shapiro-Wilk p-value for tuned_GPT-sBERTn1024-sentEx-naiveRephrs: 0.0028 -> Potential non-normality (p < 0.05)\n",
      "  Shapiro-Wilk p-value for tuned_GPT-sBERTn1024-sentEx-noRephrs: 0.0470 -> Potential non-normality (p < 0.05)\n",
      "  Shapiro-Wilk p-value for tuned_GPT-sBERTn1024-sentEx-rephsHist: 0.0046 -> Potential non-normality (p < 0.05)\n",
      "\n",
      "Variance Check (Levene's Test) for gpt_total_tokens:\n",
      "  Levene's test p-value: 0.9369 -> Equal variances plausible (p >= 0.05)\n",
      "\n",
      "Group Comparison for gpt_total_tokens:\n",
      "  Using Kruskal-Wallis (non-parametric test due to potential assumption violation).\n",
      "  Kruskal-Wallis Result: H = 0.0904, p = 0.9558\n",
      "  No significant difference found between groups (p >= 0.05).\n",
      "----------------------------------------\n",
      "\n",
      "--- Analyzing Per-Claim Metric: Number Of Evidence Sentences (Group 2) ---\n",
      "Normality Check (Q-Q Plots & Shapiro-Wilk) for number_of_evidence_sentences:\n",
      "  Shapiro-Wilk p-value for tuned_GPT-sBERTn1024-sentEx-naiveRephrs: 0.0001 -> Potential non-normality (p < 0.05)\n",
      "  Shapiro-Wilk p-value for tuned_GPT-sBERTn1024-sentEx-noRephrs: 0.0002 -> Potential non-normality (p < 0.05)\n",
      "  Shapiro-Wilk p-value for tuned_GPT-sBERTn1024-sentEx-rephsHist: 0.0001 -> Potential non-normality (p < 0.05)\n",
      "\n",
      "Variance Check (Levene's Test) for number_of_evidence_sentences:\n",
      "  Levene's test p-value: 0.8214 -> Equal variances plausible (p >= 0.05)\n",
      "\n",
      "Group Comparison for number_of_evidence_sentences:\n",
      "  Using Kruskal-Wallis (non-parametric test due to potential assumption violation).\n",
      "  Kruskal-Wallis Result: H = 0.5166, p = 0.7724\n",
      "  No significant difference found between groups (p >= 0.05).\n",
      "----------------------------------------\n",
      "\n",
      "--- Analyzing Per-Claim Metric: Iterations Run (Group 2) ---\n",
      "Normality Check (Q-Q Plots & Shapiro-Wilk) for iterations_run:\n",
      "  Shapiro-Wilk p-value for tuned_GPT-sBERTn1024-sentEx-naiveRephrs: 0.0000 -> Potential non-normality (p < 0.05)\n",
      "  Shapiro-Wilk p-value for tuned_GPT-sBERTn1024-sentEx-noRephrs: 0.0000 -> Potential non-normality (p < 0.05)\n",
      "  Shapiro-Wilk p-value for tuned_GPT-sBERTn1024-sentEx-rephsHist: 0.0000 -> Potential non-normality (p < 0.05)\n",
      "\n",
      "Variance Check (Levene's Test) for iterations_run:\n",
      "  Levene's test p-value: 0.4713 -> Equal variances plausible (p >= 0.05)\n",
      "\n",
      "Group Comparison for iterations_run:\n",
      "  Using Kruskal-Wallis (non-parametric test due to potential assumption violation).\n",
      "  Kruskal-Wallis Result: H = 1.1269, p = 0.5692\n",
      "  No significant difference found between groups (p >= 0.05).\n",
      "----------------------------------------\n",
      "\n",
      "--- Analyzing Per-Claim Metric: Time To Check (Group 2) ---\n",
      "Normality Check (Q-Q Plots & Shapiro-Wilk) for time_to_check:\n",
      "  Shapiro-Wilk p-value for tuned_GPT-sBERTn1024-sentEx-naiveRephrs: 0.0025 -> Potential non-normality (p < 0.05)\n",
      "  Shapiro-Wilk p-value for tuned_GPT-sBERTn1024-sentEx-noRephrs: 0.2918 -> Normality plausible (p >= 0.05)\n",
      "  Shapiro-Wilk p-value for tuned_GPT-sBERTn1024-sentEx-rephsHist: 0.0017 -> Potential non-normality (p < 0.05)\n",
      "\n",
      "Variance Check (Levene's Test) for time_to_check:\n",
      "  Levene's test p-value: 0.9975 -> Equal variances plausible (p >= 0.05)\n",
      "\n",
      "Group Comparison for time_to_check:\n",
      "  Using Kruskal-Wallis (non-parametric test due to potential assumption violation).\n",
      "  Kruskal-Wallis Result: H = 2.5156, p = 0.2843\n",
      "  No significant difference found between groups (p >= 0.05).\n",
      "----------------------------------------\n",
      "\n",
      "--- Analysis Complete ---\n"
     ]
    }
   ],
   "source": [
    "# --- Create Color Mapping ---\n",
    "unique_system_configs = sorted(all_results_df['system_config'].unique())\n",
    "color_mapping = {}\n",
    "palette = px.colors.qualitative.Safe_r\n",
    "for i, system_config in enumerate(unique_system_configs):\n",
    "    color_mapping[system_config] = palette[i % len(palette)]\n",
    "print(\"\\nColor Mapping:\", json.dumps(color_mapping, indent=2))\n",
    "\n",
    "# Define metric groups\n",
    "overall_perf_metrics = ['avg_label_accuracy', 'avg_strict_score', 'avg_f1'] # These are overall run averages\n",
    "pc_binary_metrics = ['pc_strict_score', 'pc_label_accuracy']\n",
    "pc_continuous_metrics = ['pc_precision', 'pc_recall', 'pc_f1']\n",
    "other_operational_metrics = [\n",
    "    'number_of_pages_retrieved',\n",
    "    'gpt_total_tokens',\n",
    "    'number_of_evidence_sentences',\n",
    "    'iterations_run',\n",
    "    'time_to_check'\n",
    "]\n",
    "\n",
    "# Metrics for Group 2 statistical tests (per-claim level)\n",
    "metrics_to_test_group2_pc = pc_binary_metrics + pc_continuous_metrics + other_operational_metrics\n",
    "\n",
    "# --- Group 1 Comparison ---\n",
    "group1_configs_tuned = [\n",
    "    'tuned_GPT-sentEx', 'tuned_GPT-clf', 'tuned_GPT-query', 'tuned_sBERT-n1024'\n",
    "]\n",
    "group1_baseline = 'all_base'\n",
    "group1_all_configs = sorted(group1_configs_tuned + [group1_baseline])\n",
    "group1_df = all_results_df[all_results_df['system_config'].isin(group1_all_configs)].copy()\n",
    "\n",
    "print(\"\\n--- Group 1 Visualizations (vs Baseline) ---\")\n",
    "print(\"NOTE: Comparing single-rep results to a quad-rep baseline's per-claim scores.\")\n",
    "\n",
    "# Plot 1.1: Grouped Bar Chart for OVERALL Performance Metrics (avg_ columns)\n",
    "group1_agg_df = group1_df.groupby('system_config')[overall_perf_metrics].mean().reset_index()\n",
    "group1_perf_melted = pd.melt(group1_agg_df, id_vars='system_config', value_vars=overall_perf_metrics,\n",
    "                             var_name='Metric', value_name='Average Score (Overall Run)')\n",
    "plot_name_perf1 = 'group1_overall_avg_performance_bar'\n",
    "fig_perf1 = px.bar(group1_perf_melted, x='Metric', y='Average Score (Overall Run)', color='system_config',\n",
    "                   barmode='group', title='Group 1: Overall Average Performance Scores',\n",
    "                   category_orders={\"system_config\": group1_all_configs}, color_discrete_map=color_mapping, height=500)\n",
    "fig_perf1.update_layout(xaxis_tickangle=0)\n",
    "try:\n",
    "    fig_perf1.show()\n",
    "    fig_perf1.write_image(os.path.join(FT_RESULTS_SAVE_PATH, f'{plot_name_perf1}.png'), scale=SAVE_SCALE)\n",
    "    #print(f\"Saved Group 1 overall performance bar chart to: {FT_RESULTS_SAVE_PATH}/{plot_name_perf1}.png\")\n",
    "except Exception as e: print(f\"ERROR saving/showing {plot_name_perf1}: {e}\")\n",
    "print(f\"Group 1: Overall Average Performance Scores:\")\n",
    "print(group1_perf_melted)\n",
    "\n",
    "# Plot 1.2: Bar Charts for PER-CLAIM BINARY Metrics (pc_strict_score, pc_label_accuracy)\n",
    "for metric in pc_binary_metrics:\n",
    "    plot_name_bar1_pc = f'group1_{metric}_proportion_bar'\n",
    "    # Calculate proportion of 1s (successes)\n",
    "    prop_df = group1_df.groupby('system_config')[metric].agg(ProportionSuccess='mean', Count='size').reset_index()\n",
    "    fig_bar1_pc = px.bar(prop_df, x='system_config', y='ProportionSuccess', color='system_config',\n",
    "                         title=f'Group 1: Proportion of Claims with {metric.replace(\"pc_\", \"\").replace(\"_\", \" \").title()} = 1',\n",
    "                         labels={'ProportionSuccess': f'Proportion of Claims ({metric})', 'system_config': 'System Configuration'},\n",
    "                         category_orders={\"system_config\": group1_all_configs}, color_discrete_map=color_mapping, height=500)\n",
    "    fig_bar1_pc.update_yaxes(range=[0, 1]) # Ensure y-axis is 0 to 1 for proportions\n",
    "    try:\n",
    "        fig_bar1_pc.show()\n",
    "        fig_bar1_pc.write_image(os.path.join(FT_RESULTS_SAVE_PATH, f'{plot_name_bar1_pc}.png'), scale=SAVE_SCALE)\n",
    "        #print(f\"Saved {plot_name_bar1_pc} to: {FT_RESULTS_SAVE_PATH}/{plot_name_bar1_pc}.png\")\n",
    "    except Exception as e: print(f\"ERROR saving/showing {plot_name_bar1_pc}: {e}\")\n",
    "\n",
    "# Plot 1.3: Box Plots for PER-CLAIM CONTINUOUS & OTHER Metrics\n",
    "metrics_for_box_group1 = pc_continuous_metrics + other_operational_metrics\n",
    "for metric in metrics_for_box_group1:\n",
    "    plot_name_box1 = f'group1_{metric}_boxplot'\n",
    "    try:\n",
    "        fig_box1 = px.box(group1_df, x='system_config', y=metric,\n",
    "                         title=f'Group 1: Distribution of {metric.replace(\"pc_\", \"\").replace(\"_\", \" \").title()} Per-Claim ',\n",
    "                         points=\"all\", category_orders={\"system_config\": group1_all_configs},\n",
    "                         color=\"system_config\", color_discrete_map=color_mapping)\n",
    "        fig_box1.update_layout(showlegend=False)\n",
    "        #fig_box1.show()\n",
    "        fig_box1.write_image(os.path.join(FT_RESULTS_SAVE_PATH, f'{plot_name_box1}.png'), scale=SAVE_SCALE)\n",
    "        #print(f\"Saved {plot_name_box1} to: {FT_RESULTS_SAVE_PATH}/{plot_name_box1}.png\")\n",
    "    except Exception as e: print(f\"Could not generate/save box plot for {metric} (Group 1): {e}\")\n",
    "    # Print the dataframe description\n",
    "    print(f\"\\nGroup 1: Description of {metric} per system configuration:\")\n",
    "    print(group1_df.groupby('system_config')[metric].describe())\n",
    "\n",
    "# --- Group 2 Comparison (Claim Rephrasing - Multi-Rep) ---\n",
    "group2_baseline = 'tuned_GPT-sBERTn1024-sentEx-noRephrs'\n",
    "group2_configs_tuned = [\n",
    "    'tuned_GPT-sBERTn1024-sentEx-naiveRephrs', 'tuned_GPT-sBERTn1024-sentEx-rephsHist'\n",
    "]\n",
    "group2_all_configs = sorted(group2_configs_tuned + [group2_baseline])\n",
    "group2_df = all_results_df[all_results_df['system_config'].isin(group2_all_configs)].copy()\n",
    "\n",
    "print(\"\\n--- Group 2 Analysis: Claim Rephrasing (Quad-Rep) ---\")\n",
    "\n",
    "# Plot 2.1: Grouped Bar Chart for OVERALL Performance Metrics (avg_ columns)\n",
    "group2_agg_df = group2_df.groupby('system_config')[overall_perf_metrics].mean().reset_index()\n",
    "group2_perf_melted = pd.melt(group2_agg_df, id_vars='system_config', value_vars=overall_perf_metrics,\n",
    "                             var_name='Metric', value_name='Average Score (Overall Run)')\n",
    "plot_name_perf2 = 'group2_overall_avg_performance_bar'\n",
    "fig_perf2 = px.bar(group2_perf_melted, x='Metric', y='Average Score (Overall Run)', color='system_config',\n",
    "                   barmode='group', title='Group 2: Overall Avg Performance by Rephrasing',\n",
    "                   category_orders={\"system_config\": group2_all_configs}, color_discrete_map=color_mapping, height=500)\n",
    "fig_perf2.update_layout(xaxis_tickangle=0)\n",
    "try:\n",
    "    fig_perf2.show()\n",
    "    fig_perf2.write_image(os.path.join(CR_RESULTS_SAVE_PATH, f'{plot_name_perf2}.png'), scale=SAVE_SCALE)\n",
    "    #print(f\"Saved Group 2 overall performance bar chart to: {CR_RESULTS_SAVE_PATH}/{plot_name_perf2}.png\")\n",
    "except Exception as e: print(f\"ERROR saving/showing {plot_name_perf2}: {e}\")\n",
    "print(f\"Group 2: Overall Average Performance Scores:\")\n",
    "print(group2_perf_melted)\n",
    "\n",
    "# Plot 2.2: Bar Charts for PER-CLAIM BINARY Metrics\n",
    "for metric in pc_binary_metrics:\n",
    "    plot_name_bar2_pc = f'group2_{metric}_proportion_bar'\n",
    "    prop_df = group2_df.groupby('system_config')[metric].agg(ProportionSuccess='mean', Count='size').reset_index()\n",
    "    fig_bar2_pc = px.bar(prop_df, x='system_config', y='ProportionSuccess', color='system_config',\n",
    "                         title=f'Group 2: Proportion of Claims with {metric.replace(\"pc_\", \"\").replace(\"_\", \" \").title()} = 1',\n",
    "                         labels={'ProportionSuccess': f'Proportion of Claims ({metric})', 'system_config': 'System Configuration'},\n",
    "                         category_orders={\"system_config\": group2_all_configs}, color_discrete_map=color_mapping, height=500)\n",
    "    fig_bar2_pc.update_yaxes(range=[0, 1])\n",
    "    try:\n",
    "        fig_bar2_pc.show()\n",
    "        fig_bar2_pc.write_image(os.path.join(CR_RESULTS_SAVE_PATH, f'{plot_name_bar2_pc}.png'), scale=SAVE_SCALE)\n",
    "        #print(f\"Saved {plot_name_bar2_pc} to: {CR_RESULTS_SAVE_PATH}/{plot_name_bar2_pc}.png\")\n",
    "    except Exception as e: print(f\"ERROR saving/showing {plot_name_bar2_pc}: {e}\")\n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "for metric in metrics_to_test_group2_pc:\n",
    "    print(f\"\\n--- Analyzing Per-Claim Metric: {metric.replace('_', ' ').title()} (Group 2) ---\")\n",
    "    data_groups = [group2_df[group2_df['system_config'] == config][metric].dropna() for config in group2_all_configs]\n",
    "    group_names = group2_all_configs\n",
    "\n",
    "    if metric in pc_binary_metrics:\n",
    "        print(f\"Statistical Test for Binary Metric: {metric}\")\n",
    "        # --- Chi-Squared Test ---\n",
    "        # Create a contingency table: counts of 0s and 1s for each system_config\n",
    "        contingency_table_data = []\n",
    "        for config_name, data_series in zip(group_names, data_groups):\n",
    "            successes = data_series.sum() # Count of 1s\n",
    "            failures = len(data_series) - successes # Count of 0s\n",
    "            contingency_table_data.append([successes, failures])\n",
    "            print(f\"  {config_name}: Successes={successes}, Failures={failures} (Total={len(data_series)})\")\n",
    "\n",
    "        # Ensure all groups have data\n",
    "        if all(len(data_series) > 0 for data_series in data_groups) and len(contingency_table_data) > 1:\n",
    "            try:\n",
    "                # Convert to numpy array for chi2_contingency\n",
    "                observed = np.array(contingency_table_data)\n",
    "                chi2, p_val, dof, expected = stats.chi2_contingency(observed)\n",
    "                print(f\"  Chi-Squared Test: chi2 = {chi2:.4f}, p = {p_val:.4f}, df = {dof}\")\n",
    "                if p_val < alpha:\n",
    "                    print(f\"  Significant difference in proportions found between groups (p < {alpha}).\")\n",
    "                    # Optional: Post-hoc pairwise comparisons (e.g., multiple 2x2 Chi-squared tests with Bonferroni)\n",
    "                else:\n",
    "                    print(f\"  No significant difference in proportions found between groups (p >= {alpha}).\")\n",
    "            except ValueError as ve: # Catches errors like \"The internally computed table of expected frequencies has a zero element at...\"\n",
    "                 print(f\"  Chi-Squared Test could not be performed: {ve}. Likely due to low counts in some cells.\")\n",
    "                 print(f\"  Consider Fisher's Exact Test for small N or if expected frequencies are low.\")\n",
    "            except Exception as e_chi2:\n",
    "                print(f\"  Error during Chi-Squared test for {metric}: {e_chi2}\")\n",
    "        else:\n",
    "            print(\"  Skipping Chi-Squared test (insufficient data in one or more groups).\")\n",
    "\n",
    "    else: # Continuous or other operational metrics\n",
    "        # --- Visualizations (Box Plot & Q-Q) ---\n",
    "        plot_name_box2_pc = f'group2_{metric}_boxplot'\n",
    "        try:\n",
    "            fig_box2_pc = px.box(group2_df, x='system_config', y=metric, points=\"all\",\n",
    "                              title=f'Group 2: Distribution of Per-Claim {metric.replace(\"pc_\",\"\").replace(\"_\",\" \").title()}',\n",
    "                              category_orders={\"system_config\": group2_all_configs},\n",
    "                              color=\"system_config\", color_discrete_map=color_mapping)\n",
    "            fig_box2_pc.update_layout(showlegend=False)\n",
    "            fig_box2_pc.show()\n",
    "            fig_box2_pc.write_image(os.path.join(CR_RESULTS_SAVE_PATH, f'{plot_name_box2_pc}.png'), scale=SAVE_SCALE)\n",
    "            #print(f\"Saved {plot_name_box2_pc} to: {CR_RESULTS_SAVE_PATH}/{plot_name_box2_pc}.png\")\n",
    "        except Exception as e: print(f\"Could not generate/save box plot for {metric} (Group 2): {e}\")\n",
    "\n",
    "        print(f\"Normality Check (Q-Q Plots & Shapiro-Wilk) for {metric}:\")\n",
    "        normality_passed = True\n",
    "        plot_name_qq = f'group2_{metric}_qqplots'\n",
    "        fig_qq_plt, axes = plt.subplots(1, len(data_groups), figsize=(5 * len(data_groups), 4), squeeze=False)\n",
    "        fig_qq_plt.suptitle(f'Group 2: Q-Q Plots for Per-Claim {metric.replace(\"pc_\",\"\").replace(\"_\",\" \").title()}', fontsize=14)\n",
    "        shapiro_results = {}\n",
    "        for i, (data, name) in enumerate(zip(data_groups, group_names)):\n",
    "            if len(data) >= 3:\n",
    "                try:\n",
    "                    qqplot(data, line='s', ax=axes[0, i])\n",
    "                    axes[0, i].set_title(f'{name}\\n(N={len(data)})')\n",
    "                    shapiro_test = stats.shapiro(data)\n",
    "                    shapiro_results[name] = shapiro_test.pvalue\n",
    "                    print(f\"  Shapiro-Wilk p-value for {name}: {shapiro_test.pvalue:.4f}\", end=\"\")\n",
    "                    if shapiro_test.pvalue < alpha:\n",
    "                        print(f\" -> Potential non-normality (p < {alpha})\"); normality_passed = False\n",
    "                    else:\n",
    "                        print(f\" -> Normality plausible (p >= {alpha})\")\n",
    "                except Exception as e_qq:\n",
    "                     print(f\"  Error generating Q-Q plot or Shapiro for {name}: {e_qq}\"); axes[0,i].set_title(f'{name}\\n(Error)'); shapiro_results[name] = np.nan\n",
    "            else:\n",
    "                print(f\"  Skipping normality check for {name} (N={len(data)} < 3)\"); shapiro_results[name] = np.nan; axes[0,i].set_title(f'{name}\\n(N={len(data)} - Too Small)')\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        try:\n",
    "            plt.savefig(os.path.join(CR_RESULTS_SAVE_PATH, f'{plot_name_qq}.png'), dpi=300)\n",
    "            #print(f\"Saved {plot_name_qq} to: {CR_RESULTS_SAVE_PATH}/{plot_name_qq}.png\")\n",
    "            plt.show()\n",
    "        except Exception as e_save_qq: print(f\"ERROR saving Q-Q plot {plot_name_qq}: {e_save_qq}\"); plt.show()\n",
    "        plt.close(fig_qq_plt)\n",
    "\n",
    "        print(f\"\\nVariance Check (Levene's Test) for {metric}:\")\n",
    "        if all(len(data) > 1 for data in data_groups):\n",
    "            try:\n",
    "                levene_test = stats.levene(*data_groups, center='median')\n",
    "                print(f\"  Levene's test p-value: {levene_test.pvalue:.4f}\", end=\"\")\n",
    "                equal_variance = levene_test.pvalue >= alpha\n",
    "                if equal_variance: print(f\" -> Equal variances plausible (p >= {alpha})\")\n",
    "                else: print(f\" -> Equal variances unlikely (p < {alpha})\")\n",
    "            except Exception as e_levene:\n",
    "                print(f\" Levene's test failed: {e_levene}\"); equal_variance = None\n",
    "        else:\n",
    "            print(\"  Skipping Levene's test (insufficient data)\"); equal_variance = None\n",
    "\n",
    "        print(f\"\\nGroup Comparison for {metric}:\")\n",
    "        total_valid_samples = sum(len(d) for d in data_groups)\n",
    "        if total_valid_samples < len(data_groups) * 2 or len(data_groups) < 2:\n",
    "             print(\"  Skipping comparison test (insufficient data)\"); continue\n",
    "        try:\n",
    "            if normality_passed and equal_variance:\n",
    "                print(\"  Using ANOVA (assumptions met).\")\n",
    "                f_val, p_val = stats.f_oneway(*data_groups)\n",
    "                print(f\"  ANOVA Result: F = {f_val:.4f}, p = {p_val:.4f}\")\n",
    "                if p_val < alpha:\n",
    "                    print(f\"  Significant difference found between groups (p < {alpha}).\")\n",
    "                    print(\"  Pairwise t-tests (uncorrected):\")\n",
    "                    baseline_data = data_groups[group_names.index(group2_baseline)]\n",
    "                    for i, config_name in enumerate(group2_configs_tuned): # Iterate over tuned configs for comparison\n",
    "                        comp_data = data_groups[group_names.index(config_name)]\n",
    "                        if len(baseline_data) > 1 and len(comp_data) > 1:\n",
    "                             t_stat, p_t_pair = stats.ttest_ind(baseline_data, comp_data, equal_var=True)\n",
    "                             print(f\"    {group2_baseline} vs {config_name}: t={t_stat:.3f}, p={p_t_pair:.4f}{' *' if p_t_pair < alpha else ''}\")\n",
    "                else:\n",
    "                    print(f\"  No significant difference found between groups (p >= {alpha}).\")\n",
    "            else:\n",
    "                print(\"  Using Kruskal-Wallis (non-parametric test due to potential assumption violation).\")\n",
    "                h_val, p_val = stats.kruskal(*data_groups)\n",
    "                print(f\"  Kruskal-Wallis Result: H = {h_val:.4f}, p = {p_val:.4f}\")\n",
    "                if p_val < alpha:\n",
    "                    print(f\"  Significant difference found between groups (p < {alpha}).\")\n",
    "                else:\n",
    "                    print(f\"  No significant difference found between groups (p >= {alpha}).\")\n",
    "        except Exception as e_test: print(f\"  Statistical test failed for {metric}: {e_test}\")\n",
    "\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "print(\"\\n--- Analysis Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1747020850939,
     "user": {
      "displayName": "Henry Zelenak",
      "userId": "01809909909045225068"
     },
     "user_tz": 240
    },
    "id": "8qYp_udo7As3",
    "outputId": "2f6940d7-e478-4c8b-d643-bc6be4049fbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figures available for Group 1:\n",
      "group1_pc_strict_score_proportion_bar.png\n",
      "group1_pc_precision_boxplot.png\n",
      "group1_overall_avg_performance_bar.png\n",
      "group1_pc_recall_boxplot.png\n",
      "group1_number_of_pages_retrieved_boxplot.png\n",
      "group1_pc_label_accuracy_proportion_bar.png\n",
      "group1_gpt_total_tokens_boxplot.png\n",
      "group1_iterations_run_boxplot.png\n",
      "group1_pc_f1_boxplot.png\n",
      "group1_number_of_evidence_sentences_boxplot.png\n",
      "group1_time_to_check_boxplot.png\n",
      "\n",
      "\n",
      "Figures available for Group 2:\n",
      "group2_pc_precision_boxplot.png\n",
      "group2_pc_strict_score_proportion_bar.png\n",
      "group2_overall_avg_performance_bar.png\n",
      "group2_pc_precision_qqplots.png\n",
      "group2_pc_label_accuracy_proportion_bar.png\n",
      "group2_pc_recall_boxplot.png\n",
      "group2_pc_f1_boxplot.png\n",
      "group2_pc_recall_qqplots.png\n",
      "group2_pc_f1_qqplots.png\n",
      "group2_number_of_pages_retrieved_boxplot.png\n",
      "group2_number_of_pages_retrieved_qqplots.png\n",
      "group2_gpt_total_tokens_boxplot.png\n",
      "group2_number_of_evidence_sentences_qqplots.png\n",
      "group2_number_of_evidence_sentences_boxplot.png\n",
      "group2_gpt_total_tokens_qqplots.png\n",
      "group2_iterations_run_boxplot.png\n",
      "group2_iterations_run_qqplots.png\n",
      "group2_time_to_check_boxplot.png\n",
      "group2_time_to_check_qqplots.png\n"
     ]
    }
   ],
   "source": [
    "# List all the files in the figure save dirs\n",
    "\n",
    "print(f\"Figures available for Group 1:\")\n",
    "for file in os.listdir(FT_RESULTS_SAVE_PATH):\n",
    "    print(file)\n",
    "print('\\n')\n",
    "print(f\"Figures available for Group 2:\")\n",
    "for file in os.listdir(CR_RESULTS_SAVE_PATH):\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "\n",
    "Sheffieldnlp. (2021). FEVER-scorer. SHEFFIELDNLP/Fever-scorer at release-v2.0. https://github.com/sheffieldnlp/fever-scorer/tree/release-v2.0 \n",
    "\n",
    "Thorne, J., Vlachos, A., Christodoulopoulos, C., & Mittal, A. (2018, April). Fever dataset. Fact Extraction and VERification. https://fever.ai/dataset/fever.html \n",
    "\n",
    "Thorne, J., Vlachos, A., Christodoulopoulos, C., & Mittal, A. (2018, June). FEVER: A large-scale dataset for fact extraction and VERification. In M. Walker, H. Ji, & A. Stent (Eds.), Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) (pp. 809–819). Association for Computational Linguistics. https://doi.org/10.18653/v1/N18-1074"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
