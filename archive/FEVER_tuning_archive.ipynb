{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fever_label_mapping = {\n",
    "    \"SUPPORTS\": \"1\",\n",
    "    \"REFUTES\": \"0\",\n",
    "    \"NOT ENOUGH INFO\": \"-1\"\n",
    "}\n",
    "\n",
    "def convert_fever_to_jsonl_(fever_jsonl_filepath, output_jsonl_filepath, wiki_page_dict, mode, data_segmentation=False):\n",
    "    \"\"\"\n",
    "    Converts a FEVER JSONL file to OpenAI JSONL format, including evidence text.\n",
    "\n",
    "    Args:\n",
    "        fever_jsonl_filepath (str): Path to the input FEVER JSONL file.\n",
    "        output_jsonl_filepath (str): Path to the output OpenAI JSONL file.\n",
    "        wiki_page_dict (dict): Dictionary of loaded Wikipedia pages.\n",
    "        mode (str): Mode for conversion (i.e., model type; 'evidence_extraction' or 'fact_checking').\n",
    "        data_segmentation (bool): Whether to segment the data into two halves to ensure no overlap (default: False).\n",
    "\n",
    "    \"\"\"\n",
    "    fever_data_ = load_jsonl(fever_jsonl_filepath)\n",
    "    # Sort the data\n",
    "    fever_top_half = fever_data_[:((len(fever_data_)-1)//2)]\n",
    "    fever_bottom_half = fever_data_[((len(fever_data_)-1)//2):]\n",
    "    train_jsonl = []\n",
    "    valid_jsonl = []\n",
    "\n",
    "    if mode == 'evidence_extraction':\n",
    "      print(f\"Running conversion for evidence extraction mode...\")\n",
    "      '''\n",
    "      In this mode, we create the user message stub from the full text, and the completion from the evidence sentences.\n",
    "      '''\n",
    "      run_count = len(os.listdir(f'{fever_path}/GPT_sentEx_paper_dev_train'))\n",
    "      print(f\"Number of files in GPT_sentEx_paper_dev_train: {run_count}\")\n",
    "      run_count_str = str(run_count).zfill(3)\n",
    "      skipped_count = 0\n",
    "\n",
    "      if data_segmentation:\n",
    "         fever_data = fever_top_half\n",
    "         train_true = 0\n",
    "         train_total = 200 # TESTING VARIABLE\n",
    "         valid_true = 0\n",
    "         valid_total = 60 # TESTING VARIABLE\n",
    "         print(f\"Segmentation enabled. Using top half of the dataset.\")\n",
    "      else:\n",
    "        fever_data = fever_data_\n",
    "\n",
    "      for fever_item in fever_data:\n",
    "          claim = fever_item['claim']\n",
    "          fever_label = fever_item['label']\n",
    "          mapped_label = fever_label_mapping.get(fever_label)\n",
    "          evidence_sentences, full_text = extract_evidence_text_debug(fever_item, wiki_page_dict) # Use the corrected evidence extraction\n",
    "          # Drop duplicate sentences\n",
    "          evidence_sentences = list(set(evidence_sentences))\n",
    "\n",
    "          # If data_segmentation is enabled, select exactly 200 examples\n",
    "          if data_segmentation:\n",
    "            if len(train_jsonl) >= train_total and len(valid_jsonl) >= valid_total:\n",
    "              #print(f\"Warning: Data segmentation enabled, but more than 200 examples found. Skipping...\")\n",
    "              continue\n",
    "\n",
    "          if mapped_label: # Only process if we have a valid mapping\n",
    "            if len(evidence_sentences) > 0:\n",
    "              evidence_text_combined = \",\".join(evidence_sentences) # Combine evidence sentences into a single string\n",
    "              #print(f\"Claim: {claim}\\nEvidence:\\n{evidence_text_combined}\\nFull Text:\\n{full_text}\\nMapped Label: {mapped_label}\")\n",
    "\n",
    "              ########## CRUCIAL VARIABLEs — SUBJECTIVE PROMPTS ##########\n",
    "              sys_prompt = \"\" # VARIABLE\n",
    "              user_prompt = f\"Extract sentences from the source text that are relevant (either supporting or refuting) to the preceding claim. Return a comma separated list of sentences.\\n\\nClaim: {claim}\\n\\nSource Text: {full_text}\" # VARIABLE\n",
    "              completion = evidence_text_combined # VARIABLE\n",
    "              #########################################################\n",
    "              if fever_label == 'SUPPORTS':\n",
    "                if train_true < train_total // 2:\n",
    "                  # Add to training data\n",
    "                  train_jsonl.append({\n",
    "                      \"messages\": [\n",
    "                          {\"role\": \"system\", \"content\": sys_prompt},\n",
    "                          {\"role\": \"user\", \"content\": user_prompt},\n",
    "                          {\"role\": \"assistant\", \"content\": completion}\n",
    "                      ],\n",
    "                  })\n",
    "                  train_true  += 1\n",
    "                elif valid_true < valid_total // 2:\n",
    "                  # Add to validation data\n",
    "                  valid_jsonl.append({\n",
    "                      \"messages\": [\n",
    "                          {\"role\": \"system\", \"content\": sys_prompt},\n",
    "                          {\"role\": \"user\", \"content\": user_prompt},\n",
    "                          {\"role\": \"assistant\", \"content\": completion}\n",
    "                      ],\n",
    "                  })\n",
    "                  valid_true  += 1\n",
    "                else:\n",
    "                  print(f\"Warning: No space left for SUPPORTS in either train or validation data.\")\n",
    "                  print(f\"current train_jsonl length: {len(train_jsonl)}\")\n",
    "                  print(f\"current valid_jsonl length: {len(valid_jsonl)}\")\n",
    "                  continue\n",
    "              elif fever_label == 'REFUTES':\n",
    "                if len(train_jsonl) < train_total:\n",
    "                  # Add to training data\n",
    "                  train_jsonl.append({\n",
    "                      \"messages\": [\n",
    "                          {\"role\": \"system\", \"content\": sys_prompt},\n",
    "                          {\"role\": \"user\", \"content\": user_prompt},\n",
    "                          {\"role\": \"assistant\", \"content\": completion}\n",
    "                      ],\n",
    "                  })\n",
    "                elif len(valid_jsonl) < valid_total:\n",
    "                  # Add to validation data\n",
    "                  valid_jsonl.append({\n",
    "                      \"messages\": [\n",
    "                          {\"role\": \"system\", \"content\": sys_prompt},\n",
    "                          {\"role\": \"user\", \"content\": user_prompt},\n",
    "                          {\"role\": \"assistant\", \"content\": completion}\n",
    "                      ],\n",
    "                  })\n",
    "                else:\n",
    "                  print(f\"Warning: No space left for REFUTES in either train or validation data.\")\n",
    "                  print(f\"current train_jsonl length: {len(train_jsonl)}\")\n",
    "                  print(f\"current valid_jsonl length: {len(valid_jsonl)}\")\n",
    "                  continue\n",
    "              else:\n",
    "                print(f\"Warning: Unexpected label: {fever_label}\")\n",
    "            else:\n",
    "              # We don't want to include empty evidence in the sentence extraction data. If the semantic analysis module finds no sources above the threshold this model would be skipped.\n",
    "              #print(f\"Warning: No evidence found for claim: {claim}, skipping\")\n",
    "              #print('-------------------------------------------------------------------------------------------')\n",
    "              skipped_count += 1\n",
    "              continue\n",
    "          else:\n",
    "            print(f\"Warning: No mapping found for label: {fever_label}\")\n",
    "            continue\n",
    "      print(f\"Length of the evidence extraction training data: {len(train_jsonl)}\")\n",
    "      print(f\"Length of the evidence extraction validation data: {len(valid_jsonl)}\")\n",
    "\n",
    "      with open(f\"{fever_path}GPT_sentEx_paper_dev_train/{output_jsonl_filepath}_{run_count_str}_{date_fmt}.jsonl\", 'w') as f:\n",
    "         for data_item in train_jsonl:\n",
    "               json.dump(data_item, f)\n",
    "               f.write('\\n')\n",
    "\n",
    "      # Print the number of skipped, true, and false claims\n",
    "      print(f\"Number of claims skipped: {skipped_count}\")\n",
    "      true_count = sum(1 for item in fever_data if item['label'] == 'SUPPORTS')\n",
    "      false_count = sum(1 for item in fever_data if item['label'] == 'REFUTES')\n",
    "      print(f\"Number of true claims: {true_count}\")\n",
    "      print(f\"Number of false claims: {false_count}\")\n",
    "      # Print the number of claims in the original dataset\n",
    "      print(f\"Number of claims in the original dataset: {len(fever_data)}\")\n",
    "      print(f\"FEVER data conversion to OpenAI JSONL completed. Output file: {fever_path}GPT_sentEx_paper_dev_train/{output_jsonl_filepath}_{run_count_str}_{date_fmt}.jsonl\")\n",
    "    ############### END OF EVIDENCE EXTRACTION MODE ################\n",
    "\n",
    "    ############### START OF FACT CHECKING MODE ###################\n",
    "    elif mode == 'fact_checking':\n",
    "      print(f\"Running conversion for fact checking mode...\")\n",
    "      ''''\n",
    "      In this mode, we create the user message stub from the claim, and the completion from the label.\n",
    "      '''\n",
    "      run_count = len(os.listdir(f'{fever_path}/GPT_clf_paper_dev_train'))\n",
    "      print(f\"Number of files in GPT_clf_paper_dev_train: {run_count}\")\n",
    "      run_count_str = str(run_count).zfill(3)\n",
    "      skipped_count = 0\n",
    "\n",
    "      if data_segmentation:\n",
    "          fever_data = fever_bottom_half\n",
    "          train_true = 0\n",
    "          train_total = 200 # TESTING VARIABLE\n",
    "          valid_true = 0\n",
    "          valid_total = 60 # TESTING VARIABLE\n",
    "          print(f\"Segmentation enabled. Using bottom half of the dataset.\")\n",
    "      else:\n",
    "        fever_data = fever_data_\n",
    "\n",
    "      for fever_item in fever_data:\n",
    "          claim = fever_item['claim']\n",
    "          fever_label = fever_item['label']\n",
    "          mapped_label = fever_label_mapping.get(fever_label)\n",
    "          evidence_sentences, full_text = extract_evidence_text_debug(fever_item, wiki_page_dict)\n",
    "          # Drop duplicate sentences\n",
    "          evidence_sentences = list(set(evidence_sentences))\n",
    "\n",
    "          # If data_segmentation is enabled, select exactly 100 examples for each label\n",
    "          if data_segmentation:\n",
    "            if len(train_jsonl) >= train_total and len(valid_jsonl) >= valid_total:\n",
    "              #print(f\"Warning: Data segmentation enabled, but more than 200 examples found. Skipping...\")\n",
    "              continue\n",
    "          #print(f\"Count true: {count_true}, Count false: {count_false}\")\n",
    "\n",
    "          if mapped_label:\n",
    "            if len(evidence_sentences) > 0:\n",
    "                #print(f\"Claim: {claim}\\nEvidence:\\n{evidence_sentences}\\nFull Text:\\n{full_text}\\nMapped Label: {mapped_label}\")\n",
    "\n",
    "                ########## CRUCIAL ELEMENT — SUBJECTIVE PROMPTS ##########\n",
    "                sys_prompt = \"\"\n",
    "                user_prompt = f\"Classify the truthfulness of the text out of the following categories: '1' (if the claim is true), '0' (if the claim is false), given the evidence. Do not use any other labels.\\n\\nClaim:\\n{claim}\\n\\nEvidence:\\n{evidence_sentences}\"\n",
    "                completion = mapped_label\n",
    "                #########################################################\n",
    "\n",
    "                if fever_label == 'SUPPORTS':\n",
    "                  if train_true < train_total // 2:\n",
    "                    # Add to training data\n",
    "                    train_jsonl.append({\n",
    "                        \"messages\": [\n",
    "                            {\"role\": \"system\", \"content\": sys_prompt},\n",
    "                            {\"role\": \"user\", \"content\": user_prompt},\n",
    "                            {\"role\": \"assistant\", \"content\": completion}\n",
    "                        ],\n",
    "                    })\n",
    "                    train_true += 1\n",
    "                  elif valid_true < valid_total // 2:\n",
    "                    # Add to validation data\n",
    "                    valid_jsonl.append({\n",
    "                        \"messages\": [\n",
    "                            {\"role\": \"system\", \"content\": sys_prompt},\n",
    "                            {\"role\": \"user\", \"content\": user_prompt},\n",
    "                            {\"role\": \"assistant\", \"content\": completion}\n",
    "                        ],\n",
    "                    })\n",
    "                    valid_true += 1\n",
    "                elif fever_label == 'REFUTES':\n",
    "                  if len(train_jsonl) < train_total:\n",
    "                    # Add to training data\n",
    "                    train_jsonl.append({\n",
    "                        \"messages\": [\n",
    "                            {\"role\": \"system\", \"content\": sys_prompt},\n",
    "                            {\"role\": \"user\", \"content\": user_prompt},\n",
    "                            {\"role\": \"assistant\", \"content\": completion}\n",
    "                        ],\n",
    "                    })\n",
    "                  elif len(valid_jsonl) < valid_total:\n",
    "                    # Add to validation data\n",
    "                    valid_jsonl.append({\n",
    "                        \"messages\": [\n",
    "                            {\"role\": \"system\", \"content\": sys_prompt},\n",
    "                            {\"role\": \"user\", \"content\": user_prompt},\n",
    "                            {\"role\": \"assistant\", \"content\": completion}\n",
    "                        ],\n",
    "                    })\n",
    "                else:\n",
    "                  print(f\"Warning: Unexpected label: {fever_label}\")\n",
    "            else:\n",
    "              # We don't want to include empty evidence in the sentence extraction data. If the semantic analysis module finds no sources above the threshold this model would be skipped.\n",
    "              #print(f\"Warning: No evidence found for claim: {claim}, skipping\")\n",
    "              #print('-------------------------------------------------------------------------------------------')\n",
    "              skipped_count += 1\n",
    "              continue\n",
    "          else:\n",
    "            print(f\"Warning: No mapping found for label: {fever_label}\")\n",
    "            continue\n",
    "      print(f\"Length of the fact checking training data: {len(train_jsonl)}\")\n",
    "      print(f\"Length of the fact checking validation data: {len(valid_jsonl)}\")\n",
    "      '''\n",
    "      with open(f\"{fever_path}GPT_clf_paper_dev_train/{output_jsonl_filepath}_{run_count_str}_{date_fmt}.jsonl\", 'w') as f:\n",
    "         for data_item in train_jsonl:\n",
    "               json.dump(data_item, f)\n",
    "               f.write('\\n')\n",
    "      '''\n",
    "      # Print the number of skipped, true, and false claims\n",
    "      print(f\"Number of claims skipped: {skipped_count}\")\n",
    "      true_count = sum(1 for item in fever_data if item['label'] == 'SUPPORTS')\n",
    "      false_count = sum(1 for item in fever_data if item['label'] == 'REFUTES')\n",
    "      print(f\"Number of true claims: {true_count}\")\n",
    "      print(f\"Number of false claims: {false_count}\")\n",
    "      # Print the number of claims in the original dataset\n",
    "      print(f\"Number of claims in the original dataset: {len(fever_data_)}\")\n",
    "      print(f\"FEVER data conversion to OpenAI JSONL completed. Output file: {fever_path}GPT_clf_paper_dev_train/{output_jsonl_filepath}_{run_count_str}_{date_fmt}.jsonl\")\n",
    "\n",
    "# Example usage (assuming you have fever_train.jsonl and wiki_page_dict loaded):\n",
    "fever_train_filepath = fever_path + 'paper_dev.jsonl'\n",
    "output_filepath = 'prompt_v1_n200_segmented'\n",
    "# 'evidence_extraction' for relevant sentence extraction, 'fact_checking' for claim:source relationship classification\n",
    "mode = 'evidence_extraction'\n",
    "convert_fever_to_jsonl(fever_train_filepath, output_filepath, wiki_page_list_dicts, mode, data_segmentation=True)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
